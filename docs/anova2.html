<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Comparing several groups (factorial ANOVA) | Learning Statistics with CogStat</title>
  <meta name="description" content="Chapter 13 Comparing several groups (factorial ANOVA) | Learning Statistics with CogStat covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students based on Danielle Navarro’s original Learning Statistics with R with a focus on automatic statistical analysis performed in CogStat." />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Comparing several groups (factorial ANOVA) | Learning Statistics with CogStat" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://learningstatisticswithcogstat.com/cover.jpg" />
  <meta property="og:description" content="Chapter 13 Comparing several groups (factorial ANOVA) | Learning Statistics with CogStat covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students based on Danielle Navarro’s original Learning Statistics with R with a focus on automatic statistical analysis performed in CogStat." />
  <meta name="github-repo" content="https://github.com/robertfodor/lsc" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Comparing several groups (factorial ANOVA) | Learning Statistics with CogStat" />
  
  <meta name="twitter:description" content="Chapter 13 Comparing several groups (factorial ANOVA) | Learning Statistics with CogStat covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students based on Danielle Navarro’s original Learning Statistics with R with a focus on automatic statistical analysis performed in CogStat." />
  <meta name="twitter:image" content="https://learningstatisticswithcogstat.com/cover.jpg" />

<meta name="author" content="Danielle Navarro" />
<meta name="author" content="Róbert Fodor" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="anova.html"/>
<link rel="next" href="regression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<meta name="twitter:card" content="summary"/>
<meta property="og:type" content="book"/>
<meta property="og:locale" content="en_US"/>
<meta property="article:author" content="Danielle Navarro"/>
<meta property="article:author" content="Róbert Fodor"/>
<meta name="citation_title" content="Chapter 13 Comparing several groups (factorial ANOVA) | Learning Statistics with CogStat"/>
<meta name="citation_author" content="Danielle Navarro"/>
<meta name="citation_author" content="Róbert Fodor"/>
<meta name="citation_publication_date" content="2022/09/27"/>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Learning Statistics with CogStat</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this book</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#versions"><i class="fa fa-check"></i>Versions</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#authors-note"><i class="fa fa-check"></i>Author’s note</a></li>
</ul></li>
<li class="part"><span><b>INTRODUCTIONS</b></span></li>
<li class="chapter" data-level="1" data-path="whywhywhy.html"><a href="whywhywhy.html"><i class="fa fa-check"></i><b>1</b> Why do we learn statistics?</a></li>
<li class="chapter" data-level="2" data-path="autostat.html"><a href="autostat.html"><i class="fa fa-check"></i><b>2</b> An introduction to automatic statistical analysis</a></li>
<li class="chapter" data-level="3" data-path="cogstat_intro.html"><a href="cogstat_intro.html"><i class="fa fa-check"></i><b>3</b> An Introduction to CogStat</a></li>
<li class="chapter" data-level="4" data-path="researchdesign.html"><a href="researchdesign.html"><i class="fa fa-check"></i><b>4</b> A brief introduction to research design</a>
<ul>
<li class="chapter" data-level="4.1" data-path="researchdesign.html"><a href="researchdesign.html#measurement"><i class="fa fa-check"></i><b>4.1</b> Introduction to psychological measurement</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="researchdesign.html"><a href="researchdesign.html#some-thoughts-about-psychological-measurement"><i class="fa fa-check"></i><b>4.1.1</b> Some thoughts about psychological measurement</a></li>
<li class="chapter" data-level="4.1.2" data-path="researchdesign.html"><a href="researchdesign.html#operationalisation-defining-your-measurement"><i class="fa fa-check"></i><b>4.1.2</b> Operationalisation: defining your measurement</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="researchdesign.html"><a href="researchdesign.html#scales"><i class="fa fa-check"></i><b>4.2</b> Scales of measurement</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="researchdesign.html"><a href="researchdesign.html#nominalscale"><i class="fa fa-check"></i><b>4.2.1</b> Nominal scale</a></li>
<li class="chapter" data-level="4.2.2" data-path="researchdesign.html"><a href="researchdesign.html#ordinalscale"><i class="fa fa-check"></i><b>4.2.2</b> Ordinal scale</a></li>
<li class="chapter" data-level="4.2.3" data-path="researchdesign.html"><a href="researchdesign.html#intervalscale"><i class="fa fa-check"></i><b>4.2.3</b> Interval scale</a></li>
<li class="chapter" data-level="4.2.4" data-path="researchdesign.html"><a href="researchdesign.html#ratioscale"><i class="fa fa-check"></i><b>4.2.4</b> Ratio scale</a></li>
<li class="chapter" data-level="4.2.5" data-path="researchdesign.html"><a href="researchdesign.html#continuousdiscrete"><i class="fa fa-check"></i><b>4.2.5</b> Continuous versus discrete variables</a></li>
<li class="chapter" data-level="4.2.6" data-path="researchdesign.html"><a href="researchdesign.html#likertscale"><i class="fa fa-check"></i><b>4.2.6</b> Some complexities: the Likert scale</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="researchdesign.html"><a href="researchdesign.html#reliability"><i class="fa fa-check"></i><b>4.3</b> Assessing the reliability of a measurement</a></li>
<li class="chapter" data-level="4.4" data-path="researchdesign.html"><a href="researchdesign.html#ivdv"><i class="fa fa-check"></i><b>4.4</b> The “role” of variables: predictors and outcomes</a></li>
<li class="chapter" data-level="4.5" data-path="researchdesign.html"><a href="researchdesign.html#researchdesigns"><i class="fa fa-check"></i><b>4.5</b> Experimental and non-experimental research</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="researchdesign.html"><a href="researchdesign.html#experimental-research"><i class="fa fa-check"></i><b>4.5.1</b> Experimental research</a></li>
<li class="chapter" data-level="4.5.2" data-path="researchdesign.html"><a href="researchdesign.html#non-experimental-research"><i class="fa fa-check"></i><b>4.5.2</b> Non-experimental research</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="researchdesign.html"><a href="researchdesign.html#validity"><i class="fa fa-check"></i><b>4.6</b> Assessing the validity of a study</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="researchdesign.html"><a href="researchdesign.html#internal-validity"><i class="fa fa-check"></i><b>4.6.1</b> Internal validity</a></li>
<li class="chapter" data-level="4.6.2" data-path="researchdesign.html"><a href="researchdesign.html#external-validity"><i class="fa fa-check"></i><b>4.6.2</b> External validity</a></li>
<li class="chapter" data-level="4.6.3" data-path="researchdesign.html"><a href="researchdesign.html#construct-validity"><i class="fa fa-check"></i><b>4.6.3</b> Construct validity</a></li>
<li class="chapter" data-level="4.6.4" data-path="researchdesign.html"><a href="researchdesign.html#face-validity"><i class="fa fa-check"></i><b>4.6.4</b> Face validity</a></li>
<li class="chapter" data-level="4.6.5" data-path="researchdesign.html"><a href="researchdesign.html#ecological-validity"><i class="fa fa-check"></i><b>4.6.5</b> Ecological validity</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="researchdesign.html"><a href="researchdesign.html#confounds-artefacts-and-other-threats-to-validity"><i class="fa fa-check"></i><b>4.7</b> Confounds, artefacts and other threats to validity</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="researchdesign.html"><a href="researchdesign.html#history-effects"><i class="fa fa-check"></i><b>4.7.1</b> History effects</a></li>
<li class="chapter" data-level="4.7.2" data-path="researchdesign.html"><a href="researchdesign.html#maturation-effects"><i class="fa fa-check"></i><b>4.7.2</b> Maturation effects</a></li>
<li class="chapter" data-level="4.7.3" data-path="researchdesign.html"><a href="researchdesign.html#repeated-testing-effects"><i class="fa fa-check"></i><b>4.7.3</b> Repeated testing effects</a></li>
<li class="chapter" data-level="4.7.4" data-path="researchdesign.html"><a href="researchdesign.html#selection-bias"><i class="fa fa-check"></i><b>4.7.4</b> Selection bias</a></li>
<li class="chapter" data-level="4.7.5" data-path="researchdesign.html"><a href="researchdesign.html#differentialattrition"><i class="fa fa-check"></i><b>4.7.5</b> Differential attrition</a></li>
<li class="chapter" data-level="4.7.6" data-path="researchdesign.html"><a href="researchdesign.html#non-response-bias"><i class="fa fa-check"></i><b>4.7.6</b> Non-response bias</a></li>
<li class="chapter" data-level="4.7.7" data-path="researchdesign.html"><a href="researchdesign.html#regression-to-the-mean"><i class="fa fa-check"></i><b>4.7.7</b> Regression to the mean</a></li>
<li class="chapter" data-level="4.7.8" data-path="researchdesign.html"><a href="researchdesign.html#experimenter-bias"><i class="fa fa-check"></i><b>4.7.8</b> Experimenter bias</a></li>
<li class="chapter" data-level="4.7.9" data-path="researchdesign.html"><a href="researchdesign.html#demand-effects-and-reactivity"><i class="fa fa-check"></i><b>4.7.9</b> Demand effects and reactivity</a></li>
<li class="chapter" data-level="4.7.10" data-path="researchdesign.html"><a href="researchdesign.html#placebo-effects"><i class="fa fa-check"></i><b>4.7.10</b> Placebo effects</a></li>
<li class="chapter" data-level="4.7.11" data-path="researchdesign.html"><a href="researchdesign.html#situation-measurement-and-subpopulation-effects"><i class="fa fa-check"></i><b>4.7.11</b> Situation, measurement and subpopulation effects</a></li>
<li class="chapter" data-level="4.7.12" data-path="researchdesign.html"><a href="researchdesign.html#fraud-deception-and-self-deception"><i class="fa fa-check"></i><b>4.7.12</b> Fraud, deception and self-deception</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="researchdesign.html"><a href="researchdesign.html#summary"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>DESCRIPTIVE STATISTICS</b></span></li>
<li class="chapter" data-level="5" data-path="exploringavariable.html"><a href="exploringavariable.html"><i class="fa fa-check"></i><b>5</b> Exploring a single variable</a>
<ul>
<li class="chapter" data-level="5.1" data-path="exploringavariable.html"><a href="exploringavariable.html#centraltendency"><i class="fa fa-check"></i><b>5.1</b> Measures of central tendency</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="exploringavariable.html"><a href="exploringavariable.html#mean"><i class="fa fa-check"></i><b>5.1.1</b> The mean</a></li>
<li class="chapter" data-level="5.1.2" data-path="exploringavariable.html"><a href="exploringavariable.html#median"><i class="fa fa-check"></i><b>5.1.2</b> The median</a></li>
<li class="chapter" data-level="5.1.3" data-path="exploringavariable.html"><a href="exploringavariable.html#mean-or-median-whats-the-difference"><i class="fa fa-check"></i><b>5.1.3</b> Mean or median? What’s the difference?</a></li>
<li class="chapter" data-level="5.1.4" data-path="exploringavariable.html"><a href="exploringavariable.html#trimmedmean"><i class="fa fa-check"></i><b>5.1.4</b> Trimmed mean</a></li>
<li class="chapter" data-level="5.1.5" data-path="exploringavariable.html"><a href="exploringavariable.html#mode"><i class="fa fa-check"></i><b>5.1.5</b> Mode</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="exploringavariable.html"><a href="exploringavariable.html#var"><i class="fa fa-check"></i><b>5.2</b> Measures of variability</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="exploringavariable.html"><a href="exploringavariable.html#range"><i class="fa fa-check"></i><b>5.2.1</b> Range</a></li>
<li class="chapter" data-level="5.2.2" data-path="exploringavariable.html"><a href="exploringavariable.html#IQR"><i class="fa fa-check"></i><b>5.2.2</b> Interquartile range</a></li>
<li class="chapter" data-level="5.2.3" data-path="exploringavariable.html"><a href="exploringavariable.html#aad"><i class="fa fa-check"></i><b>5.2.3</b> Mean absolute deviation</a></li>
<li class="chapter" data-level="5.2.4" data-path="exploringavariable.html"><a href="exploringavariable.html#variance"><i class="fa fa-check"></i><b>5.2.4</b> Variance</a></li>
<li class="chapter" data-level="5.2.5" data-path="exploringavariable.html"><a href="exploringavariable.html#sd"><i class="fa fa-check"></i><b>5.2.5</b> Standard deviation</a></li>
<li class="chapter" data-level="5.2.6" data-path="exploringavariable.html"><a href="exploringavariable.html#mad"><i class="fa fa-check"></i><b>5.2.6</b> Median absolute deviation</a></li>
<li class="chapter" data-level="5.2.7" data-path="exploringavariable.html"><a href="exploringavariable.html#which-measure-to-use"><i class="fa fa-check"></i><b>5.2.7</b> Which measure to use?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="exploringavariable.html"><a href="exploringavariable.html#skewnesskurtosis"><i class="fa fa-check"></i><b>5.3</b> Skewness and kurtosis</a></li>
<li class="chapter" data-level="5.4" data-path="exploringavariable.html"><a href="exploringavariable.html#zscore"><i class="fa fa-check"></i><b>5.4</b> Standard scores (<span class="math inline">\(z\)</span>-score)</a></li>
<li class="chapter" data-level="5.5" data-path="exploringavariable.html"><a href="exploringavariable.html#summary-descriptives"><i class="fa fa-check"></i><b>5.5</b> Summary: descriptives</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="correl.html"><a href="correl.html"><i class="fa fa-check"></i><b>6</b> Exploring a variable pair</a>
<ul>
<li class="chapter" data-level="6.1" data-path="correl.html"><a href="correl.html#the-strength-and-direction-of-a-relationship"><i class="fa fa-check"></i><b>6.1</b> The strength and direction of a relationship</a></li>
<li class="chapter" data-level="6.2" data-path="correl.html"><a href="correl.html#pearson"><i class="fa fa-check"></i><b>6.2</b> The correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="correl.html"><a href="correl.html#interpretingcorrelations"><i class="fa fa-check"></i><b>6.3</b> Interpreting a correlation</a></li>
<li class="chapter" data-level="6.4" data-path="correl.html"><a href="correl.html#spearman"><i class="fa fa-check"></i><b>6.4</b> Spearman’s rank correlations</a></li>
<li class="chapter" data-level="6.5" data-path="correl.html"><a href="correl.html#missingvaluespair"><i class="fa fa-check"></i><b>6.5</b> Missing values in pairwise calculations</a></li>
<li class="chapter" data-level="6.6" data-path="correl.html"><a href="correl.html#summary-1"><i class="fa fa-check"></i><b>6.6</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>INFERENTIAL STATISTICS</b></span></li>
<li class="chapter" data-level="7" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>7</b> Probability and distributions</a>
<ul>
<li class="chapter" data-level="7.1" data-path="probability.html"><a href="probability.html#probabilitystats"><i class="fa fa-check"></i><b>7.1</b> How are probability and statistics different?</a></li>
<li class="chapter" data-level="7.2" data-path="probability.html"><a href="probability.html#probabilitymeaning"><i class="fa fa-check"></i><b>7.2</b> What does probability mean?</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="probability.html"><a href="probability.html#the-frequentist-view"><i class="fa fa-check"></i><b>7.2.1</b> The frequentist view</a></li>
<li class="chapter" data-level="7.2.2" data-path="probability.html"><a href="probability.html#the-bayesian-view"><i class="fa fa-check"></i><b>7.2.2</b> The Bayesian view</a></li>
<li class="chapter" data-level="7.2.3" data-path="probability.html"><a href="probability.html#whats-the-difference-and-who-is-right"><i class="fa fa-check"></i><b>7.2.3</b> What’s the difference? And who is right?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="probability.html"><a href="probability.html#basicprobability"><i class="fa fa-check"></i><b>7.3</b> Basic probability theory</a></li>
<li class="chapter" data-level="7.4" data-path="probability.html"><a href="probability.html#distributions"><i class="fa fa-check"></i><b>7.4</b> Distributions</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="probability.html"><a href="probability.html#binomial"><i class="fa fa-check"></i><b>7.4.1</b> The binomial distribution</a></li>
<li class="chapter" data-level="7.4.2" data-path="probability.html"><a href="probability.html#normal"><i class="fa fa-check"></i><b>7.4.2</b> The normal distribution</a></li>
<li class="chapter" data-level="7.4.3" data-path="probability.html"><a href="probability.html#density"><i class="fa fa-check"></i><b>7.4.3</b> Probability density</a></li>
<li class="chapter" data-level="7.4.4" data-path="probability.html"><a href="probability.html#otherdists"><i class="fa fa-check"></i><b>7.4.4</b> Other useful distributions</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="probability.html"><a href="probability.html#summary-2"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>8</b> Population, sampling, estimation</a>
<ul>
<li class="chapter" data-level="8.1" data-path="estimation.html"><a href="estimation.html#srs"><i class="fa fa-check"></i><b>8.1</b> Samples, populations and sampling</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="estimation.html"><a href="estimation.html#pop"><i class="fa fa-check"></i><b>8.1.1</b> Defining a population</a></li>
<li class="chapter" data-level="8.1.2" data-path="estimation.html"><a href="estimation.html#simple-random-samples"><i class="fa fa-check"></i><b>8.1.2</b> Simple random samples</a></li>
<li class="chapter" data-level="8.1.3" data-path="estimation.html"><a href="estimation.html#most-samples-are-not-simple-random-samples"><i class="fa fa-check"></i><b>8.1.3</b> Most samples are not simple random samples</a></li>
<li class="chapter" data-level="8.1.4" data-path="estimation.html"><a href="estimation.html#how-much-does-it-matter-if-you-dont-have-a-simple-random-sample"><i class="fa fa-check"></i><b>8.1.4</b> How much does it matter if you don’t have a simple random sample?</a></li>
<li class="chapter" data-level="8.1.5" data-path="estimation.html"><a href="estimation.html#population-parameters-and-sample-statistics"><i class="fa fa-check"></i><b>8.1.5</b> Population parameters and sample statistics</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="estimation.html"><a href="estimation.html#lawlargenumbers"><i class="fa fa-check"></i><b>8.2</b> The law of large numbers</a></li>
<li class="chapter" data-level="8.3" data-path="estimation.html"><a href="estimation.html#samplesandclt"><i class="fa fa-check"></i><b>8.3</b> Sampling distributions and the central limit theorem</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="estimation.html"><a href="estimation.html#samplingdists"><i class="fa fa-check"></i><b>8.3.1</b> Sampling distribution of the mean</a></li>
<li class="chapter" data-level="8.3.2" data-path="estimation.html"><a href="estimation.html#sampling-distributions-exist-for-any-sample-statistic"><i class="fa fa-check"></i><b>8.3.2</b> Sampling distributions exist for any sample statistic!</a></li>
<li class="chapter" data-level="8.3.3" data-path="estimation.html"><a href="estimation.html#clt"><i class="fa fa-check"></i><b>8.3.3</b> The central limit theorem</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="estimation.html"><a href="estimation.html#pointestimates"><i class="fa fa-check"></i><b>8.4</b> Estimating population parameters</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="estimation.html"><a href="estimation.html#estimating-the-population-mean"><i class="fa fa-check"></i><b>8.4.1</b> Estimating the population mean</a></li>
<li class="chapter" data-level="8.4.2" data-path="estimation.html"><a href="estimation.html#estimating-the-population-standard-deviation"><i class="fa fa-check"></i><b>8.4.2</b> Estimating the population standard deviation</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="estimation.html"><a href="estimation.html#ci"><i class="fa fa-check"></i><b>8.5</b> Estimating a confidence interval</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="estimation.html"><a href="estimation.html#a-slight-mistake-in-the-formula"><i class="fa fa-check"></i><b>8.5.1</b> A slight mistake in the formula</a></li>
<li class="chapter" data-level="8.5.2" data-path="estimation.html"><a href="estimation.html#interpreting-a-confidence-interval"><i class="fa fa-check"></i><b>8.5.2</b> Interpreting a confidence interval</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="estimation.html"><a href="estimation.html#population-parameter-estimations-in-cogstat"><i class="fa fa-check"></i><b>8.6</b> Population parameter estimations in CogStat</a></li>
<li class="chapter" data-level="8.7" data-path="estimation.html"><a href="estimation.html#summary-3"><i class="fa fa-check"></i><b>8.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="hypothesistesting.html"><a href="hypothesistesting.html"><i class="fa fa-check"></i><b>9</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="9.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#hypotheses"><i class="fa fa-check"></i><b>9.1</b> A menagerie of hypotheses</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#research-hypotheses-versus-statistical-hypotheses"><i class="fa fa-check"></i><b>9.1.1</b> Research hypotheses versus statistical hypotheses</a></li>
<li class="chapter" data-level="9.1.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#null-hypotheses-and-alternative-hypotheses"><i class="fa fa-check"></i><b>9.1.2</b> Null hypotheses and alternative hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#errortypes"><i class="fa fa-check"></i><b>9.2</b> Two types of errors</a></li>
<li class="chapter" data-level="9.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#teststatistics"><i class="fa fa-check"></i><b>9.3</b> Test statistics and sampling distributions</a></li>
<li class="chapter" data-level="9.4" data-path="hypothesistesting.html"><a href="hypothesistesting.html#decisionmaking"><i class="fa fa-check"></i><b>9.4</b> Making decisions</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#critical-regions-and-critical-values"><i class="fa fa-check"></i><b>9.4.1</b> Critical regions and critical values</a></li>
<li class="chapter" data-level="9.4.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-note-on-statistical-significance"><i class="fa fa-check"></i><b>9.4.2</b> A note on statistical “significance”</a></li>
<li class="chapter" data-level="9.4.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-note-on-the-word-prove"><i class="fa fa-check"></i><b>9.4.3</b> A note on the word “prove”</a></li>
<li class="chapter" data-level="9.4.4" data-path="hypothesistesting.html"><a href="hypothesistesting.html#onesidedtests"><i class="fa fa-check"></i><b>9.4.4</b> The difference between one sided and two sided tests</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="hypothesistesting.html"><a href="hypothesistesting.html#pvalue"><i class="fa fa-check"></i><b>9.5</b> The <span class="math inline">\(p\)</span> value of a test</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-softer-view-of-decision-making"><i class="fa fa-check"></i><b>9.5.1</b> A softer view of decision making</a></li>
<li class="chapter" data-level="9.5.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-probability-of-extreme-data"><i class="fa fa-check"></i><b>9.5.2</b> The probability of extreme data</a></li>
<li class="chapter" data-level="9.5.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-common-mistake"><i class="fa fa-check"></i><b>9.5.3</b> A common mistake</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="hypothesistesting.html"><a href="hypothesistesting.html#writeup"><i class="fa fa-check"></i><b>9.6</b> Reporting the results of a hypothesis test</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-issue"><i class="fa fa-check"></i><b>9.6.1</b> The issue</a></li>
<li class="chapter" data-level="9.6.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#two-proposed-solutions"><i class="fa fa-check"></i><b>9.6.2</b> Two proposed solutions</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effectsize"><i class="fa fa-check"></i><b>9.7</b> Effect size, sample size and power</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-power-function"><i class="fa fa-check"></i><b>9.7.1</b> The power function</a></li>
<li class="chapter" data-level="9.7.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effect-size"><i class="fa fa-check"></i><b>9.7.2</b> Effect size</a></li>
<li class="chapter" data-level="9.7.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#increasing-the-power-of-your-study"><i class="fa fa-check"></i><b>9.7.3</b> Increasing the power of your study</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="hypothesistesting.html"><a href="hypothesistesting.html#nhstmess"><i class="fa fa-check"></i><b>9.8</b> Some issues to consider</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#neyman-versus-fisher"><i class="fa fa-check"></i><b>9.8.1</b> Neyman versus Fisher</a></li>
<li class="chapter" data-level="9.8.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#bayesians-versus-frequentists"><i class="fa fa-check"></i><b>9.8.2</b> Bayesians versus frequentists</a></li>
<li class="chapter" data-level="9.8.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#traps"><i class="fa fa-check"></i><b>9.8.3</b> Traps</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="hypothesistesting.html"><a href="hypothesistesting.html#summary-4"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>STATISTICAL TOOLS</b></span></li>
<li class="chapter" data-level="10" data-path="chisquare.html"><a href="chisquare.html"><i class="fa fa-check"></i><b>10</b> Categorical data analysis</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chisquare.html"><a href="chisquare.html#goftest"><i class="fa fa-check"></i><b>10.1</b> The <span class="math inline">\(\chi^2\)</span> goodness-of-fit test</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="chisquare.html"><a href="chisquare.html#the-null-hypothesis-and-the-alternative-hypothesis"><i class="fa fa-check"></i><b>10.1.1</b> The null hypothesis and the alternative hypothesis</a></li>
<li class="chapter" data-level="10.1.2" data-path="chisquare.html"><a href="chisquare.html#the-goodness-of-fit-test-statistic"><i class="fa fa-check"></i><b>10.1.2</b> The “goodness of fit” test statistic</a></li>
<li class="chapter" data-level="10.1.3" data-path="chisquare.html"><a href="chisquare.html#the-sampling-distribution-of-the-gof-statistic-advanced"><i class="fa fa-check"></i><b>10.1.3</b> The sampling distribution of the GOF statistic (advanced)</a></li>
<li class="chapter" data-level="10.1.4" data-path="chisquare.html"><a href="chisquare.html#degrees-of-freedom"><i class="fa fa-check"></i><b>10.1.4</b> Degrees of freedom</a></li>
<li class="chapter" data-level="10.1.5" data-path="chisquare.html"><a href="chisquare.html#testing-the-null-hypothesis"><i class="fa fa-check"></i><b>10.1.5</b> Testing the null hypothesis</a></li>
<li class="chapter" data-level="10.1.6" data-path="chisquare.html"><a href="chisquare.html#chisqreport"><i class="fa fa-check"></i><b>10.1.6</b> How to report the results of the test</a></li>
<li class="chapter" data-level="10.1.7" data-path="chisquare.html"><a href="chisquare.html#a-comment-on-statistical-notation-advanced"><i class="fa fa-check"></i><b>10.1.7</b> A comment on statistical notation (advanced)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="chisquare.html"><a href="chisquare.html#chisqindependence"><i class="fa fa-check"></i><b>10.2</b> The <span class="math inline">\(\chi^2\)</span> test of independence (or association)</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="chisquare.html"><a href="chisquare.html#constructing-our-hypothesis-test"><i class="fa fa-check"></i><b>10.2.1</b> Constructing our hypothesis test</a></li>
<li class="chapter" data-level="10.2.2" data-path="chisquare.html"><a href="chisquare.html#AssocTestInCogStat"><i class="fa fa-check"></i><b>10.2.2</b> The test results in CogStat</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chisquare.html"><a href="chisquare.html#yates"><i class="fa fa-check"></i><b>10.3</b> Yates correction for 1 degree of freedom</a></li>
<li class="chapter" data-level="10.4" data-path="chisquare.html"><a href="chisquare.html#chisqeffectsize"><i class="fa fa-check"></i><b>10.4</b> Effect size (Cramér’s <span class="math inline">\(V\)</span>)</a></li>
<li class="chapter" data-level="10.5" data-path="chisquare.html"><a href="chisquare.html#chisqassumptions"><i class="fa fa-check"></i><b>10.5</b> Assumptions of the test(s)</a></li>
<li class="chapter" data-level="10.6" data-path="chisquare.html"><a href="chisquare.html#fisherexacttest"><i class="fa fa-check"></i><b>10.6</b> The Fisher exact test</a></li>
<li class="chapter" data-level="10.7" data-path="chisquare.html"><a href="chisquare.html#mcnemar"><i class="fa fa-check"></i><b>10.7</b> The McNemar test</a></li>
<li class="chapter" data-level="10.8" data-path="chisquare.html"><a href="chisquare.html#whats-the-difference-between-mcnemar-and-independence"><i class="fa fa-check"></i><b>10.8</b> What’s the difference between McNemar and independence?</a></li>
<li class="chapter" data-level="10.9" data-path="chisquare.html"><a href="chisquare.html#summary-5"><i class="fa fa-check"></i><b>10.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ttest.html"><a href="ttest.html"><i class="fa fa-check"></i><b>11</b> Comparing two means</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ttest.html"><a href="ttest.html#ztest"><i class="fa fa-check"></i><b>11.1</b> The one-sample <span class="math inline">\(z\)</span>-test</a></li>
<li class="chapter" data-level="11.2" data-path="ttest.html"><a href="ttest.html#onesamplettest"><i class="fa fa-check"></i><b>11.2</b> The one-sample <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="11.3" data-path="ttest.html"><a href="ttest.html#studentttest"><i class="fa fa-check"></i><b>11.3</b> The independent samples <span class="math inline">\(t\)</span>-test (Student test)</a></li>
<li class="chapter" data-level="11.4" data-path="ttest.html"><a href="ttest.html#welchttest"><i class="fa fa-check"></i><b>11.4</b> The independent samples <span class="math inline">\(t\)</span>-test (Welch test)</a></li>
<li class="chapter" data-level="11.5" data-path="ttest.html"><a href="ttest.html#pairedsamplesttest"><i class="fa fa-check"></i><b>11.5</b> The paired-samples <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="11.6" data-path="ttest.html"><a href="ttest.html#cohensd"><i class="fa fa-check"></i><b>11.6</b> Effect size (Cohen’s <span class="math inline">\(d\)</span>)</a></li>
<li class="chapter" data-level="11.7" data-path="ttest.html"><a href="ttest.html#shapiro"><i class="fa fa-check"></i><b>11.7</b> Normality of a sample</a></li>
<li class="chapter" data-level="11.8" data-path="ttest.html"><a href="ttest.html#wilcox"><i class="fa fa-check"></i><b>11.8</b> Testing non-normal data with Wilcoxon tests</a>
<ul>
<li class="chapter" data-level="11.8.1" data-path="ttest.html"><a href="ttest.html#mannwhitney"><i class="fa fa-check"></i><b>11.8.1</b> Two-sample Wilcoxon test (Mann-Whitney test)</a></li>
<li class="chapter" data-level="11.8.2" data-path="ttest.html"><a href="ttest.html#wilcoxon"><i class="fa fa-check"></i><b>11.8.2</b> One-sample and paired samples Wilcoxon tests</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="ttest.html"><a href="ttest.html#summary-6"><i class="fa fa-check"></i><b>11.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>12</b> Comparing several means (one-way ANOVA)</a>
<ul>
<li class="chapter" data-level="12.1" data-path="anova.html"><a href="anova.html#the-data"><i class="fa fa-check"></i><b>12.1</b> The data</a></li>
<li class="chapter" data-level="12.2" data-path="anova.html"><a href="anova.html#anovaintro"><i class="fa fa-check"></i><b>12.2</b> How ANOVA works</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="anova.html"><a href="anova.html#from-variance"><i class="fa fa-check"></i><b>12.2.1</b> From variance…</a></li>
<li class="chapter" data-level="12.2.2" data-path="anova.html"><a href="anova.html#to-total-sum-of-squares"><i class="fa fa-check"></i><b>12.2.2</b> … to total sum of squares</a></li>
<li class="chapter" data-level="12.2.3" data-path="anova.html"><a href="anova.html#from-sums-of-squares-to-the-f-test"><i class="fa fa-check"></i><b>12.2.3</b> From sums of squares to the <span class="math inline">\(F\)</span>-test</a></li>
<li class="chapter" data-level="12.2.4" data-path="anova.html"><a href="anova.html#anovamodel"><i class="fa fa-check"></i><b>12.2.4</b> Further reading: the meaning of <span class="math inline">\(F\)</span> (advanced)</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="anova.html"><a href="anova.html#introduceaov"><i class="fa fa-check"></i><b>12.3</b> Interpreting our results in CogStat</a></li>
<li class="chapter" data-level="12.4" data-path="anova.html"><a href="anova.html#anovaeffect"><i class="fa fa-check"></i><b>12.4</b> Effect size</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="anova.html"><a href="anova.html#eta-squared"><i class="fa fa-check"></i><b>12.4.1</b> Eta-squared</a></li>
<li class="chapter" data-level="12.4.2" data-path="anova.html"><a href="anova.html#omega-squared"><i class="fa fa-check"></i><b>12.4.2</b> Omega-squared</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="anova.html"><a href="anova.html#posthoc"><i class="fa fa-check"></i><b>12.5</b> Post hoc tests</a></li>
<li class="chapter" data-level="12.6" data-path="anova.html"><a href="anova.html#levene"><i class="fa fa-check"></i><b>12.6</b> Checking the homogeneity of variance assumption</a></li>
<li class="chapter" data-level="12.7" data-path="anova.html"><a href="anova.html#kruskalwallis"><i class="fa fa-check"></i><b>12.7</b> Testing for non-normal data with Kruskal-Wallis test</a></li>
<li class="chapter" data-level="12.8" data-path="anova.html"><a href="anova.html#anovaandt"><i class="fa fa-check"></i><b>12.8</b> On the relationship between ANOVA and the Student <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="12.9" data-path="anova.html"><a href="anova.html#summary-7"><i class="fa fa-check"></i><b>12.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="anova2.html"><a href="anova2.html"><i class="fa fa-check"></i><b>13</b> Comparing several groups (factorial ANOVA)</a>
<ul>
<li class="chapter" data-level="13.1" data-path="anova2.html"><a href="anova2.html#factorialanovasimple"><i class="fa fa-check"></i><b>13.1</b> Balanced designs</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="anova2.html"><a href="anova2.html#factanovahyp"><i class="fa fa-check"></i><b>13.1.1</b> What hypotheses are we testing?</a></li>
<li class="chapter" data-level="13.1.2" data-path="anova2.html"><a href="anova2.html#means-sums-of-squares-and-degrees-of-freedom"><i class="fa fa-check"></i><b>13.1.2</b> Means, sums of squares, and degrees of freedom</a></li>
<li class="chapter" data-level="13.1.3" data-path="anova2.html"><a href="anova2.html#the-interaction"><i class="fa fa-check"></i><b>13.1.3</b> The <em>interaction</em></a></li>
<li class="chapter" data-level="13.1.4" data-path="anova2.html"><a href="anova2.html#how-to-interpret-the-results"><i class="fa fa-check"></i><b>13.1.4</b> How to interpret the results</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="anova2.html"><a href="anova2.html#effectsizefactorialanova"><i class="fa fa-check"></i><b>13.2</b> Effect size</a></li>
<li class="chapter" data-level="13.3" data-path="anova2.html"><a href="anova2.html#meansfactorialanova"><i class="fa fa-check"></i><b>13.3</b> Estimated group means and confidence intervals</a></li>
<li class="chapter" data-level="13.4" data-path="anova2.html"><a href="anova2.html#posthoc2"><i class="fa fa-check"></i><b>13.4</b> Post hoc tests</a></li>
<li class="chapter" data-level="13.5" data-path="anova2.html"><a href="anova2.html#unbalancedanova"><i class="fa fa-check"></i><b>13.5</b> Unbalanced designs and types of sums of squares</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="anova2.html"><a href="anova2.html#type-i-sum-of-squares"><i class="fa fa-check"></i><b>13.5.1</b> Type I sum of squares</a></li>
<li class="chapter" data-level="13.5.2" data-path="anova2.html"><a href="anova2.html#type-iii-sum-of-squares"><i class="fa fa-check"></i><b>13.5.2</b> Type III sum of squares</a></li>
<li class="chapter" data-level="13.5.3" data-path="anova2.html"><a href="anova2.html#type-ii-sum-of-squares"><i class="fa fa-check"></i><b>13.5.3</b> Type II sum of squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>14</b> Linear regression</a>
<ul>
<li class="chapter" data-level="14.1" data-path="regression.html"><a href="regression.html#introregression"><i class="fa fa-check"></i><b>14.1</b> What is a linear regression model?</a></li>
<li class="chapter" data-level="14.2" data-path="regression.html"><a href="regression.html#regressionestimation"><i class="fa fa-check"></i><b>14.2</b> Estimating a linear regression model</a></li>
<li class="chapter" data-level="14.3" data-path="regression.html"><a href="regression.html#regressioninterpretation"><i class="fa fa-check"></i><b>14.3</b> Interpreting the results of a linear regression</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="regression.html"><a href="regression.html#confidence-intervals-for-the-coefficients"><i class="fa fa-check"></i><b>14.3.1</b> Confidence intervals for the coefficients</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="regression.html"><a href="regression.html#r2"><i class="fa fa-check"></i><b>14.4</b> Quantifying the fit of the regression model</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="regression.html"><a href="regression.html#the-adjusted-r2-value"><i class="fa fa-check"></i><b>14.4.1</b> The adjusted <span class="math inline">\(R^2\)</span> value</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="regression.html"><a href="regression.html#regressiontests"><i class="fa fa-check"></i><b>14.5</b> Hypothesis tests for regression models</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="regression.html"><a href="regression.html#testing-the-model-as-a-whole"><i class="fa fa-check"></i><b>14.5.1</b> Testing the model as a whole</a></li>
<li class="chapter" data-level="14.5.2" data-path="regression.html"><a href="regression.html#tests-for-individual-coefficients"><i class="fa fa-check"></i><b>14.5.2</b> Tests for individual coefficients</a></li>
<li class="chapter" data-level="14.5.3" data-path="regression.html"><a href="regression.html#stdcoef"><i class="fa fa-check"></i><b>14.5.3</b> Calculating standardised regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="regression.html"><a href="regression.html#regressiondiagnostics"><i class="fa fa-check"></i><b>14.6</b> Model checking</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="regression.html"><a href="regression.html#three-kinds-of-residuals"><i class="fa fa-check"></i><b>14.6.1</b> Three kinds of residuals</a></li>
<li class="chapter" data-level="14.6.2" data-path="regression.html"><a href="regression.html#regressionoutliers"><i class="fa fa-check"></i><b>14.6.2</b> Three kinds of anomalous data</a></li>
<li class="chapter" data-level="14.6.3" data-path="regression.html"><a href="regression.html#regressionnormality"><i class="fa fa-check"></i><b>14.6.3</b> Checking the normality of the residuals</a></li>
<li class="chapter" data-level="14.6.4" data-path="regression.html"><a href="regression.html#regressionhomogeneity"><i class="fa fa-check"></i><b>14.6.4</b> Checking the homoscedasticity of the residuals</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="regression.html"><a href="regression.html#regressionassumptions"><i class="fa fa-check"></i><b>14.7</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>BAYESIAN STATISTICS</b></span></li>
<li class="chapter" data-level="15" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>15</b> Introduction to Bayesian statistics</a>
<ul>
<li class="chapter" data-level="15.1" data-path="bayes.html"><a href="bayes.html#priors-what-you-believed-before"><i class="fa fa-check"></i><b>15.1</b> Priors: what you believed before</a></li>
<li class="chapter" data-level="15.2" data-path="bayes.html"><a href="bayes.html#likelihoods-theories-about-the-data"><i class="fa fa-check"></i><b>15.2</b> Likelihoods: theories about the data</a></li>
<li class="chapter" data-level="15.3" data-path="bayes.html"><a href="bayes.html#the-joint-probability-of-data-and-hypothesis"><i class="fa fa-check"></i><b>15.3</b> The joint probability of data and hypothesis</a></li>
<li class="chapter" data-level="15.4" data-path="bayes.html"><a href="bayes.html#updating-beliefs-using-bayes-rule"><i class="fa fa-check"></i><b>15.4</b> Updating beliefs using Bayes’ rule</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html"><i class="fa fa-check"></i><b>16</b> Bayesian hypothesis tests</a>
<ul>
<li class="chapter" data-level="16.1" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#the-bayes-factor"><i class="fa fa-check"></i><b>16.1</b> The Bayes factor</a></li>
<li class="chapter" data-level="16.2" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#interpreting-bayes-factors"><i class="fa fa-check"></i><b>16.2</b> Interpreting Bayes factors</a></li>
<li class="chapter" data-level="16.3" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#bayesian-statistics-in-cogstat"><i class="fa fa-check"></i><b>16.3</b> Bayesian statistics in CogStat</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#one-sample-t-test"><i class="fa fa-check"></i><b>16.3.1</b> One-sample <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="16.3.2" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#independent-samples-t-test"><i class="fa fa-check"></i><b>16.3.2</b> Independent samples <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="16.3.3" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#anova-1"><i class="fa fa-check"></i><b>16.3.3</b> ANOVA</a></li>
<li class="chapter" data-level="16.3.4" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#linear-regression"><i class="fa fa-check"></i><b>16.3.4</b> Linear regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="whybayes.html"><a href="whybayes.html"><i class="fa fa-check"></i><b>17</b> Why be a Bayesian?</a>
<ul>
<li class="chapter" data-level="17.1" data-path="whybayes.html"><a href="whybayes.html#evidentiary-standards-you-can-believe"><i class="fa fa-check"></i><b>17.1</b> Evidentiary standards you can believe</a></li>
<li class="chapter" data-level="17.2" data-path="whybayes.html"><a href="whybayes.html#the-p-value-is-a-lie."><i class="fa fa-check"></i><b>17.2</b> The <span class="math inline">\(p\)</span>-value is a lie.</a></li>
<li class="chapter" data-level="17.3" data-path="whybayes.html"><a href="whybayes.html#is-it-really-this-bad"><i class="fa fa-check"></i><b>17.3</b> Is it really this bad?</a></li>
</ul></li>
<li class="part"><span><b>APPENDICES</b></span></li>
<li class="chapter" data-level="18" data-path="summaryguide.html"><a href="summaryguide.html"><i class="fa fa-check"></i><b>18</b> Summary guide</a>
<ul>
<li class="chapter" data-level="" data-path="summaryguide.html"><a href="summaryguide.html#descriptive-statistics"><i class="fa fa-check"></i>Descriptive statistics</a></li>
<li class="chapter" data-level="" data-path="summaryguide.html"><a href="summaryguide.html#analysing-differences-with-hyptothesis-testing"><i class="fa fa-check"></i>Analysing differences with hyptothesis testing</a></li>
<li class="chapter" data-level="" data-path="summaryguide.html"><a href="summaryguide.html#analysing-relationship-with-hyptothesis-testing"><i class="fa fa-check"></i>Analysing relationship with hyptothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>19</b> Epilogue</a>
<ul>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#the-undiscovered-statistics"><i class="fa fa-check"></i>The undiscovered statistics</a>
<ul>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#omissions-within-the-topics-covered"><i class="fa fa-check"></i>Omissions within the topics covered</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#statistical-models-missing-from-the-book"><i class="fa fa-check"></i>Statistical models missing from the book</a></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#last-words"><i class="fa fa-check"></i>Last words</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Learning Statistics with CogStat</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="anova2" class="section level1 hasAnchor" number="13">
<h1><span class="header-section-number">Chapter 13</span> Comparing several groups (factorial ANOVA)<a href="anova2.html#anova2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In past chapters, we compared two groups for the same variable (Chapter <a href="anova.html#anova">12</a>), or we compared two variables (Chapter <a href="ttest.html#ttest">11</a>). In this chapter, we are going to look at more than one grouping variables, which we sometimes refer to as <strong>factorial ANOVA</strong>.</p>
<div id="factorialanovasimple" class="section level2 hasAnchor" number="13.1">
<h2><span class="header-section-number">13.1</span> Balanced designs<a href="anova2.html#factorialanovasimple" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we discussed the analysis of variance in Chapter <a href="anova.html#anova">12</a>, we assumed a fairly simple experimental design: each person falls into one of several groups, and we want to know whether these groups have different means on some outcome variable. In this section, we will look at a broader class of experimental designs, known as <strong>factorial designs</strong>, where we have more than one grouping variable.</p>
<p>Let’s take the example that appears in Chapter <a href="anova.html#anova">12</a>, in which we were looking at the effect of different drugs on the <code>mood_gain</code> experienced by each person. In that chapter, we did find a significant effect of <code>drug</code> (Chapter <a href="anova.html#introduceaov">12.3</a>), but at the end of the chapter we also ran an analysis to see if there was an effect of <code>therapy</code> (Chapter <a href="anova.html#anovaandt">12.8</a>). We didn’t find one, but there’s something a bit worrying about trying to run two <em>separate</em> analyses trying to predict the same outcome. Maybe there actually <em>is</em> an effect of therapy on mood gain, but we couldn’t find it because it was being “hidden” by the effect of drug? In other words, we’re going to want to run a <em>single</em> analysis that includes <em>both</em> <code>drug</code> and <code>therapy</code> as predictors.</p>
<p>For this analysis, each person is cross-classified by the drug they were given (a factor with 3 levels) and what therapy they received (a factor with 2 levels). We refer to this as a <span class="math inline">\(3 \times 2\)</span> factorial design. Let’s load the <a href="resources/data/clinicaltrial.csv"><code>clinicaltrial.csv</code></a> data set again to CogStat, and run the <code>Compare groups</code> function with both <code>drug</code> and <code>therapy</code> as grouping variables.</p>
<p><img src="resources/image/cogstatcompareclinanova2.png" width="100%" style="display: block; margin: auto;" /></p>
<p>We get the following table:
<img src="resources/image/cogstatanova2clinload.png" width="100%" style="display: block; margin: auto;" /></p>
<p>As you can see, not only do we have participants corresponding to all possible combinations of the two factors, indicating that our design is <strong>completely crossed</strong>, it turns out that there are an equal number of people in each group. In other words, we have a <strong>balanced</strong> design.</p>
<div id="factanovahyp" class="section level3 hasAnchor" number="13.1.1">
<h3><span class="header-section-number">13.1.1</span> What hypotheses are we testing?<a href="anova2.html#factanovahyp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Like one-way ANOVA, factorial ANOVA is a tool for testing certain types of hypotheses about population means. So a sensible place to start would be to be explicit about what our hypotheses actually are.</p>
<p>However, before we can even get to that point, it’s really useful to have some clean and simple ways to describe the means. Because of the fact that observations are cross-classified in terms of two different factors, we’ll need a cross-tabulation of sorts.</p>
<p>Now, this output by CogStat shows a cross-tabulation of the group means and other descriptive statistics for all possible combinations of the two factors (e.g. people who received the placebo and no therapy, people who received the placebo while getting CBT etc.), and it also shows a boxplot comparing these combinations.</p>
<p><img src="resources/image/cogstatanova2boxplotclinical.png" width="100%" style="display: block; margin: auto;" /></p>
<p>But sometimes, we want to dissect our data in a different output format. To do that, we have the <code>Pivot table</code> function in CogStat. Let’s use it to create a cross-tabulation of the means of <code>mood_gain</code> for each combination of <code>drug</code> and <code>therapy</code>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:anova2clinicalpivot"></span>
<img src="resources/image/cogstatpivotclinicaldialog.png" alt="Cross-tabulation of the means of `mood_gain` for each combination of `drug` and `therapy` in `Pivot table` function. You can select different functions to tabulate: *N* (count), *Sum*, *Mean*, *Median*, *Lower* and *Upper quartile*, *Standard deviation*, and *Variance*." width="100%" /><img src="resources/image/cogstatpivotclinicalpivot.png" alt="Cross-tabulation of the means of `mood_gain` for each combination of `drug` and `therapy` in `Pivot table` function. You can select different functions to tabulate: *N* (count), *Sum*, *Mean*, *Median*, *Lower* and *Upper quartile*, *Standard deviation*, and *Variance*." width="100%" />
<p class="caption">
Figure 13.1: Cross-tabulation of the means of <code>mood_gain</code> for each combination of <code>drug</code> and <code>therapy</code> in <code>Pivot table</code> function. You can select different functions to tabulate: <em>N</em> (count), <em>Sum</em>, <em>Mean</em>, <em>Median</em>, <em>Lower</em> and <em>Upper quartile</em>, <em>Standard deviation</em>, and <em>Variance</em>.
</p>
</div>
<p>Let’s use some mathematical notation and the symbol <span class="math inline">\(\mu\)</span> to denote a population mean. However, because there are lots of different means, we’ll need to use subscripts to distinguish between them. Here’s how the notation works. Our table is defined in terms of two factors: each row corresponds to a different level of Factor A (in this case, <code>drug</code>), and each column corresponds to a different level of Factor B (in this case, <code>therapy</code>). If we let <span class="math inline">\(R\)</span> denote the number of rows in the table, and <span class="math inline">\(C\)</span> denote the number of columns, we can refer to this as an <span class="math inline">\(R \times C\)</span> factorial ANOVA. In this case <span class="math inline">\(R=3\)</span> and <span class="math inline">\(C=2\)</span>.</p>
<p>We’ll use lowercase letters to refer to specific rows and columns, so <span class="math inline">\(\mu_{rc}\)</span> refers to the population mean associated with the <span class="math inline">\(r\)</span>th level of Factor A (i.e. row number <span class="math inline">\(r\)</span>) and the <span class="math inline">\(c\)</span>th level of Factor B (column number <span class="math inline">\(c\)</span>).<a href="#fn60" class="footnote-ref" id="fnref60"><sup>60</sup></a> We use the “dot” notation to express averages across rows and columns. In the case of Joyzepam, notice that we’re talking about the mean associated with the third row in the table. That is, we’re averaging across two cell means (i.e., <span class="math inline">\(\mu_{31}\)</span> and <span class="math inline">\(\mu_{32}\)</span>). The result of this averaging is referred to as a <strong>marginal mean</strong>, and would be denoted <span class="math inline">\(\mu_{3.}\)</span> in this case. The marginal mean for CBT corresponds to the population mean associated with the second column in the table, so we use the notation <span class="math inline">\(\mu_{.2}\)</span> to describe it. The grand mean is denoted <span class="math inline">\(\mu_{..}\)</span> because it is the mean obtained by averaging (marginalising<a href="#fn61" class="footnote-ref" id="fnref61"><sup>61</sup></a>) over both. So our full table of population means can be written down like this:</p>
<table>
<caption>
<span id="tab:unnamed-chunk-35">Table 13.1: </span>Population means in factorial ANOVA.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
No therapy
</th>
<th style="text-align:center;">
CBT
</th>
<th style="text-align:center;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Placebo
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{11}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{12}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{1.}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Anxifree
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{21}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{21}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{2.}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Joyzepam
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{31}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{32}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{3.}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{.1}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{.2}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\mu_{..}\)</span>
</td>
</tr>
</tbody>
</table>
<p>Now that we have this notation, it is straightforward to formulate and express some hypotheses. Let’s suppose that the goal is to find out two things: firstly, does the drug choice have any effect on mood, and secondly, does CBT have any effect on mood? These aren’t the only hypotheses that we could formulate of course, but these are the two simplest hypotheses to test, and so we’ll start there.</p>
<p>Consider the first test. If the drug has no effect, then we would expect all of the row means to be identical, right? So that’s our null hypothesis. On the other hand, if the drug does matter, then we should expect these row means to be different. Formally, we write down our null and alternative hypotheses in terms of the <em>equality of marginal means</em>:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Null hypothesis <span class="math inline">\(H_0\)</span>:
</td>
<td style="text-align:left;">
row means are the same i.e. <span class="math inline">\(\mu_{1.} = \mu_{2.} = \mu_{3.}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Alternative hypothesis <span class="math inline">\(H_1\)</span>:
</td>
<td style="text-align:left;">
at least one row mean is different.
</td>
</tr>
</tbody>
</table>
<p>It’s worth noting that these are <em>exactly</em> the same statistical hypotheses that we formed when we ran a one-way ANOVA on these data back in Chapter <a href="anova.html#anova">12</a>. Back then, we used the notation <span class="math inline">\(\mu_P\)</span> to refer to the mean mood gain for the placebo group, with <span class="math inline">\(\mu_A\)</span> and <span class="math inline">\(\mu_J\)</span> corresponding to the group means for the two drugs, and the null hypothesis was <span class="math inline">\(\mu_P = \mu_A = \mu_J\)</span>. So we’re actually talking about the same hypothesis: it’s just that the more complicated ANOVA requires more careful notation due to the presence of multiple grouping variables, so we’re now referring to this hypothesis as <span class="math inline">\(\mu_{1.} = \mu_{2.} = \mu_{3.}\)</span>. However, as we’ll see shortly, although the hypothesis is identical, the test of that hypothesis is subtly different due to the fact that we’re now acknowledging the existence of the second grouping variable.</p>
<p>Speaking of the other grouping variable, you won’t be surprised to discover that our second hypothesis test is formulated the same way. However, since we’re talking about the psychological therapy rather than drugs, our null hypothesis now corresponds to the equality of the column means:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Null hypothesis <span class="math inline">\(H_0\)</span>:
</td>
<td style="text-align:left;">
column means are the same, i.e., <span class="math inline">\(\mu_{.1} = \mu_{.2}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Alternative hypothesis <span class="math inline">\(H_1\)</span>:
</td>
<td style="text-align:left;">
column means are different, i.e., <span class="math inline">\(\mu_{.1} \neq \mu_{.2}\)</span>
</td>
</tr>
</tbody>
</table>
</div>
<div id="means-sums-of-squares-and-degrees-of-freedom" class="section level3 hasAnchor" number="13.1.2">
<h3><span class="header-section-number">13.1.2</span> Means, sums of squares, and degrees of freedom<a href="anova2.html#means-sums-of-squares-and-degrees-of-freedom" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The null and alternative hypotheses might seem awfully familiar: they’re basically the same as the hypotheses that we were testing in our simpler one-way ANOVAs in Chapter <a href="anova.html#anova">12</a>. So you’re probably expecting that the hypothesis <em>tests</em> that are used in factorial ANOVA will be essentially the same as the <span class="math inline">\(F\)</span>-test from Chapter <a href="anova.html#anova">12</a>. You’re expecting to see references to sums of squares (SS), mean squares (MS), degrees of freedom (df), and finally an <span class="math inline">\(F\)</span>-statistic that we can convert into a <span class="math inline">\(p\)</span>-value, right? Well, you’re absolutely and completely right.</p>
<p>We used <span class="math inline">\(\mu\)</span> for population means, but we’ll use <span class="math inline">\(\bar{Y}\)</span> to refer to a sample mean. For the rest of the formulas, we can use the same notation as before to refer to group means, marginal means and grand means: that is, <span class="math inline">\(\bar{Y}_{rc}\)</span> is the sample mean associated with the <span class="math inline">\(r\)</span>th level of Factor A and the <span class="math inline">\(c\)</span>th level of Factor B, <span class="math inline">\(\bar{Y}_{r.}\)</span> would be the marginal mean for the <span class="math inline">\(r\)</span>th level of Factor A, <span class="math inline">\(\bar{Y}_{.c}\)</span> would be the marginal mean for the <span class="math inline">\(c\)</span>th level of Factor B, and <span class="math inline">\(\bar{Y}_{..}\)</span> is the grand mean. In other words, our sample means can be organised into the same table as the population means. For our clinical trial data, that table looks like this:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
No therapy
</th>
<th style="text-align:center;">
CBT
</th>
<th style="text-align:center;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Placebo
</td>
<td style="text-align:center;">
<span class="math inline">\(\bar{Y}_{11}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\bar{Y}_{12}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\bar{Y}_{1.}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Anxifree
</td>
<td style="text-align:center;">
<span class="math inline">\(\bar{Y}_{21}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\bar{Y}_{21}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\bar{Y}_{2.}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Joyzepam
</td>
<td style="text-align:center;">
<span class="math inline">\(\bar{Y}_{31}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\bar{Y}_{32}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\bar{Y}_{3.}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:center;">
<span class="math inline">\(\bar{Y}_{.1}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\bar{Y}_{.2}\)</span>
</td>
<td style="text-align:center;">
<span class="math inline">\(\bar{Y}_{..}\)</span>
</td>
</tr>
</tbody>
</table>
<p>If we look at the sample means from earlier (Figure <a href="anova2.html#fig:anova2clinicalpivot">13.1</a>), we have <span class="math inline">\(\bar{Y}_{11} = 0.30\)</span>, <span class="math inline">\(\bar{Y}_{12} = 0.60\)</span> etc. In our clinical trial example, the <code>drugs</code> factor has 3 levels and the <code>therapy</code> factor has 2 levels, and so what we’re trying to run is a <span class="math inline">\(3 \times 2\)</span> factorial ANOVA.</p>
<p>To be a little more general, we can say that Factor A (the row factor) has <span class="math inline">\(R\)</span> levels and Factor B (the column factor) has <span class="math inline">\(C\)</span> levels, and so what we’re running here is an <span class="math inline">\(R \times C\)</span> factorial ANOVA. The formula for the sum of squares values for each of the two factors is relatively familiar. For Factor A, our between group sum of squares is calculated by assessing the extent to which the (row) marginal means <span class="math inline">\(\bar{Y}_{1.}\)</span>, <span class="math inline">\(\bar{Y}_{2.}\)</span> etc, are different from the grand mean <span class="math inline">\(\bar{Y}_{..}\)</span>:
<span class="math display">\[
\mbox{SS}_{A} = (N \times C)  \sum_{r=1}^R  \left( \bar{Y}_{r.} - \bar{Y}_{..} \right)^2
\]</span></p>
<p>The formula for factor B is, of course, the same thing, just with some subscripts shuffled around:
<span class="math display">\[
\mbox{SS}_{B} = (N \times R) \sum_{c=1}^C \left( \bar{Y}_{.c} - \bar{Y}_{..} \right)^2
\]</span></p>
<p>Okay, now let’s calculate the sum of squares associated with the main effect of <code>drug</code>. There are a total of <span class="math inline">\(N=3\)</span> people in each group, and <span class="math inline">\(C=2\)</span> different types of therapy. Or, to put it another way, there are <span class="math inline">\(3 \times 2 = 6\)</span> people who received any particular drug. So our calculations are:
<span class="math display">\[
\begin{array}{rcl}
\mbox{SS}_{drug} &amp;=&amp; (N \times C)  \sum_{r=1}^R  \left( \bar{Y}_{r.} - \bar{Y}_{..} \right)^2
    \\
    &amp;=&amp; 3.453333
\end{array}
\]</span></p>
<p>We can repeat the same kind of calculation for the effect of therapy. Again there are <span class="math inline">\(N=3\)</span> people in each group, but since there are <span class="math inline">\(R=3\)</span> different drugs, this time around we note that there are <span class="math inline">\(3 \times 3 = 9\)</span> people who received CBT, and an additional 9 people who received the placebo. So our calculation is now:
<span class="math display">\[
\begin{array}{rcl}
\mbox{SS}_{therapy} &amp;=&amp; (N \times C)  \sum_{r=1}^R  \left( \bar{Y}_{r.} - \bar{Y}_{..} \right)^2
    \\
    &amp;=&amp; 0.467222
\end{array}
\]</span></p>
<p>So that’s how you calculate the SS values for the two main effects. These SS values are analogous to the between-group sum of squares values that we calculated when doing one-way ANOVA in Chapter <a href="anova.html#anova">12</a>. However, it’s not a good idea to think of them as between-groups SS values anymore, just because we have two different grouping variables and it’s easy to get confused. In order to construct an <span class="math inline">\(F\)</span> test, however, we also need to calculate the within-groups sum of squares. We will refer to the within-groups SS value as the <em>residual</em><a href="#fn62" class="footnote-ref" id="fnref62"><sup>62</sup></a> sum of squares SS<span class="math inline">\(_R\)</span>.</p>
<p>The easiest way to think about the residual SS values in this context, is to think of it as the leftover variation in the outcome variable after you take into account the differences in the marginal means (i.e. after you remove SS<span class="math inline">\(_{drug}\)</span> and SS<span class="math inline">\(_{therapy}\)</span>). What I mean by that is we can start by calculating the total sum of squares (SS<span class="math inline">\(_T\)</span>). We take the difference between each observation <span class="math inline">\(Y_{rci}\)</span> and the grand mean <span class="math inline">\(\bar{Y}_{..}\)</span>, square the differences, and add them all up
<span class="math display">\[
\begin{array}{rcl}
\mbox{SS}_T &amp;=&amp; \sum_{r=1}^R \sum_{c=1}^C \sum_{i=1}^N \left( Y_{rci} - \bar{Y}_{..}\right)^2
    \\
    &amp;=&amp; 4.845
\end{array}
\]</span></p>
<p>The “triple summation” here looks more complicated than it is. In the first two summations, we’re summing across all levels of Factor A (i.e., over all possible rows <span class="math inline">\(r\)</span> in our table), across all levels of Factor B (i.e. all possible columns <span class="math inline">\(c\)</span>). Each <span class="math inline">\(rc\)</span> combination corresponds to a single group, and each group contains <span class="math inline">\(N\)</span> people: so we have to sum across all those people (i.e. all <span class="math inline">\(i\)</span> values) too. In other words, all we’re doing here is summing across all observations in the data set (i.e. all possible <span class="math inline">\(rci\)</span> combinations).</p>
<p>The residual sum of squares is thus defined to be the variability in <span class="math inline">\(Y\)</span> that <em>can’t</em> be attributed to either of our two factors. In other words:
<span class="math display">\[
\begin{array}{rcl}
\mbox{SS}_R &amp;=&amp; \mbox{SS}_T - (\mbox{SS}_A + \mbox{SS}_B)
    \\
    &amp;=&amp; 4.845 - (3.45333 + 0.46722)
    \\
    &amp;=&amp; 0.92445
\end{array}
\]</span></p>
<p>It is commonplace to refer to <span class="math inline">\(\mbox{SS}_A + \mbox{SS}_B\)</span> as the variance attributable to the “ANOVA model”, denoted SS<span class="math inline">\(_M\)</span>, and so we often say that the total sum of squares is equal to the model sum of squares plus the residual sum of squares.</p>
<p>The degrees of freedom are calculated in much the same way as for one-way ANOVA. The degrees of freedom equals the number of quantities that are observed, minus the number of constraints. So, for the <code>drugs</code> factor, we observe 3 separate group means, but these are constrained by 1 grand mean; and therefore the degrees of freedom is <span class="math inline">\(df = 2\)</span>. For the <code>therapy</code> factor we obtain <span class="math inline">\(df=1\)</span>.</p>
<p>For the residuals, the logic is similar, but not quite the same. The total number of observations in our experiment is 18. The constraints correspond to the 1 grand mean, the 2 additional group means that the <code>drug</code> factor introduces, and the 1 additional group mean that the the <code>therapy</code> factor introduces, and so our degrees of freedom is 14. As a formula, this is <span class="math inline">\(N-1 -(R-1)-(C-1)\)</span>, which simplifies to <span class="math inline">\(N-R-C+1\)</span>.</p>
<p>Just like we saw with the original one-way ANOVA, note that the mean square value is calculated by dividing SS by the corresponding <span class="math inline">\(df\)</span>. That is, it’s still true that
<span class="math display">\[
\mbox{MS} = \frac{\mbox{SS}}{df}
\]</span>
regardless of whether we’re talking about <code>drug</code>, <code>therapy</code> or the residuals. To see this, let’s not worry about how the sums of squares values are calculated. So for the <code>drug</code> factor, we divide <span class="math inline">\(3.45333\)</span> by <span class="math inline">\(2\)</span>, and end up with a mean square value of <span class="math inline">\(1.73\)</span>. For the <code>therapy</code> factor, there’s only 1 degree of freedom, so our calculations are even simpler: dividing <span class="math inline">\(0.46722\)</span> (the SS value) by 1 gives us an answer of <span class="math inline">\(0.47\)</span> (the MS value).</p>
<p>Turning to the <span class="math inline">\(F\)</span> statistics and the <span class="math inline">\(p\)</span> values, notice that we have two of each: one corresponding to the <code>drug</code> factor and the other corresponding to the <code>therapy</code> factor. Regardless of which one we’re talking about, the <span class="math inline">\(F\)</span> statistic is calculated by dividing the mean square value associated with the factor by the mean square value associated with the residuals:
<span class="math display">\[
F_{A} = \frac{\mbox{MS}_{A}}{\mbox{MS}_{R}}
\]</span>
and an equivalent formula exists for factor B (i.e. <code>therapy</code>).</p>
<p>So for the <code>drug</code> factor, we take the mean square of <span class="math inline">\(1.73\)</span> and divide it by the residual mean square value of <span class="math inline">\(0.07\)</span>, which gives us an <span class="math inline">\(F\)</span>-statistic of <span class="math inline">\(26.15\)</span>. The corresponding calculation for the <code>therapy</code> variable would be to divide <span class="math inline">\(0.47\)</span> by <span class="math inline">\(0.07\)</span> which gives <span class="math inline">\(7.08\)</span> as the <span class="math inline">\(F\)</span>-statistic.</p>
<p>Regarding the <span class="math inline">\(p\)</span>-value, what we’re trying to do is test the null hypothesis that there is no relationship between the factor and the outcome variable. To that end, we’ve followed a similar strategy that we did in the one-way ANOVA, and have calculated an <span class="math inline">\(F\)</span>-statistic for each of these hypotheses. To convert these to <span class="math inline">\(p\)</span> values, all we need to do is note that the sampling distribution for the <span class="math inline">\(F\)</span> <em>statistic</em> under the null hypothesis is an <span class="math inline">\(F\)</span> <em>distribution</em>, and that two degrees of freedom values are those corresponding to the factor, and those corresponding to the residuals. For the <code>drug</code> factor we’re talking about an <span class="math inline">\(F\)</span> distribution with 2 and 14 degrees of freedom. In contrast, for the <code>therapy</code> factor sampling distribution is <span class="math inline">\(F\)</span> with 1 and 14 degrees of freedom.</p>
<p>So, for the <code>drug</code> factor, we have an <span class="math inline">\(F\)</span>-statistic of <span class="math inline">\(26.15\)</span> and an <span class="math inline">\(F\)</span>-distribution with 2 and 14 degrees of freedom. The corresponding <span class="math inline">\(p\)</span>-value is <span class="math inline">\(&lt;0.001\)</span>. For the <code>therapy</code> factor, we have an <span class="math inline">\(F\)</span>-statistic of <span class="math inline">\(7.08\)</span> and an <span class="math inline">\(F\)</span>-distribution with 1 and 14 degrees of freedom. The corresponding <span class="math inline">\(p\)</span>-value is <span class="math inline">\(0.02\)</span>.</p>
<p>But hang on! You’ve run the analysis in CogStat and you see something vastly different. That’s okay. Bear with us for a moment.</p>
</div>
<div id="the-interaction" class="section level3 hasAnchor" number="13.1.3">
<h3><span class="header-section-number">13.1.3</span> The <em>interaction</em><a href="anova2.html#the-interaction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The ANOVA model that we’ve been talking about so far covers a range of different patterns that we might observe in our data. For instance, in a two-way ANOVA design, there are four possibilities: (a) only Factor A matters, (b) only Factor B matters, (c) both A and B matter, and (d) neither A nor B matters. An example of each of these four possibilities is plotted in Figure <a href="anova2.html#fig:maineffects">13.2</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:maineffects"></span>
<img src="resources/image/maineffectA.png" alt="Factor main effects" width="50%" /><img src="resources/image/maineffectB.png" alt="Factor main effects" width="50%" /><img src="resources/image/maineffectAB.png" alt="Factor main effects" width="50%" /><img src="resources/image/maineffectO.png" alt="Factor main effects" width="50%" />
<p class="caption">
Figure 13.2: Factor main effects
</p>
</div>
<p>The four patterns of data shown in Figure <a href="anova2.html#fig:maineffects">13.2</a> are all quite realistic: there are a great many data sets that produce exactly those patterns. However, they are not the whole story, and the ANOVA model that we have been talking about up to this point is not sufficient to fully account for a table of group means. Why not? Well, so far we have the ability to talk about the idea that drugs can influence mood, and therapy can influence mood, but no way of talking about the possibility of an <strong>interaction</strong> between the two. An interaction between A and B is said to occur whenever the effect of Factor A is <em>different</em>, depending on which level of Factor B we’re talking about. Several examples of an interaction effect with the context of a 2 x 2 ANOVA are shown in Figure <a href="anova2.html#fig:interaction">13.3</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:interaction"></span>
<img src="resources/image/interaction1.png" alt="Qualitatively different interactions for a $2 \times 2$ ANOVA" width="50%" /><img src="resources/image/interaction2.png" alt="Qualitatively different interactions for a $2 \times 2$ ANOVA" width="50%" /><img src="resources/image/interaction3.png" alt="Qualitatively different interactions for a $2 \times 2$ ANOVA" width="50%" /><img src="resources/image/interaction4.png" alt="Qualitatively different interactions for a $2 \times 2$ ANOVA" width="50%" />
<p class="caption">
Figure 13.3: Qualitatively different interactions for a <span class="math inline">\(2 \times 2\)</span> ANOVA
</p>
</div>
<p>To give a more concrete example, suppose that the operation of Anxifree and Joyzepam is governed quite different physiological mechanisms, and one consequence of this is that while Joyzepam has more or less the same effect on mood regardless of whether one is in therapy, Anxifree is actually much more effective when administered in conjunction with CBT. The ANOVA that we developed manually, does not capture this idea. To get some idea of whether an interaction is actually happening here, it helps to plot the various group means.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:interactionplot"></span>
<img src="resources/image/interactionplot.png" alt="Interaction plot for our clinical trial data" width="100%" />
<p class="caption">
Figure 13.4: Interaction plot for our clinical trial data
</p>
</div>
<p>Our main concern relates to the fact that the two lines aren’t parallel. The effect of CBT (difference between solid line and dotted line) when the drug is Joyzepam (right side) appears to be near zero, even smaller than the effect of CBT when a placebo is used (left side). However, when Anxifree is administered, the effect of CBT is larger than the placebo (middle). Is this effect real, or is this just random variation due to chance? Our original ANOVA cannot answer this question, because we make no allowances for the idea that interactions even exist! In this section, we’ll fix this problem.</p>
<p>Although there are only two <em>factors</em> involved in our model (i.e. <code>drug</code> and <code>therapy</code>), there are actually three distinct <strong>terms</strong> (i.e. <code>drug</code>, <code>therapy</code> and <code>drug × therapy</code>). That is, in addition to the main effects of <code>drug</code> and <code>therapy</code>, we have a new component to the model, which is our interaction term <code>drug × therapy</code>.</p>
<p>Intuitively, the idea behind an interaction effect is fairly simple: it means that the effect of Factor A is different, depending on which level of Factor B we’re talking about. But what does that actually mean in terms of our data? Figure <a href="anova2.html#fig:interactionplot">13.4</a> depicts several different patterns that, although quite different to each other, would all count as an interaction effect. So it’s not entirely straightforward to translate this qualitative idea into something mathematical that a statistician can work with. As a consequence, the way that the idea of an interaction effect is formalised in terms of null and alternative hypotheses is slightly difficult.</p>
<p>To start with, we need to be a little more explicit about our main effects. Consider the main effect of Factor A (<code>drug</code> in our running example). We originally formulated this in terms of the null hypothesis that the two marginal means <span class="math inline">\(\mu_{r.}\)</span> are all equal to each other. Obviously, if all of these are equal to each other, then they must also be equal to the grand mean <span class="math inline">\(\mu_{..}\)</span> as well, right? So what we can do is define the <em>effect</em> of Factor A at level <span class="math inline">\(r\)</span> to be equal to the difference between the marginal mean <span class="math inline">\(\mu_{r.}\)</span> and the grand mean <span class="math inline">\(\mu_{..}\)</span>.<br />
Let’s denote this effect by <span class="math inline">\(\alpha_r\)</span>, and note that
<span class="math display">\[
\alpha_r  = \mu_{r.} - \mu_{..}
\]</span></p>
<p>Now, by definition all of the <span class="math inline">\(\alpha_r\)</span> values must sum to zero, for the same reason that the average of the marginal means <span class="math inline">\(\mu_{r.}\)</span> must be the grand mean <span class="math inline">\(\mu_{..}\)</span>. We can similarly define the effect of Factor B at level <span class="math inline">\(i\)</span> to be the difference between the column marginal mean <span class="math inline">\(\mu_{.c}\)</span> and the grand mean <span class="math inline">\(\mu_{..}\)</span>
<span class="math display">\[
\beta_c = \mu_{.c} - \mu_{..}
\]</span>
and once again, these <span class="math inline">\(\beta_c\)</span> values must sum to zero. The reason that statisticians sometimes like to talk about the main effects in terms of these <span class="math inline">\(\alpha_r\)</span> and <span class="math inline">\(\beta_c\)</span> values is that it allows them to be precise about what it means to say that there is no interaction effect. If there is no interaction at all, then these <span class="math inline">\(\alpha_r\)</span> and <span class="math inline">\(\beta_c\)</span> values will perfectly describe the group means <span class="math inline">\(\mu_{rc}\)</span>. Specifically, it means that
<span class="math display">\[
\mu_{rc} = \mu_{..} + \alpha_r + \beta_c
\]</span></p>
<p>That is, there’s nothing <em>special</em> about the group means that you couldn’t predict perfectly by knowing all the marginal means. And that’s our null hypothesis, right there. The alternative hypothesis is that
<span class="math display">\[
\mu_{rc} \neq \mu_{..} + \alpha_r + \beta_c
\]</span>
for at least one group <span class="math inline">\(rc\)</span> in our table. However, statisticians often like to write this slightly differently. They’ll usually define the specific interaction associated with group <span class="math inline">\(rc\)</span> to be some number, awkwardly referred to as <span class="math inline">\((\alpha\beta)_{rc}\)</span>, and then they will say that the alternative hypothesis is that
<span class="math display">\[\mu_{rc} = \mu_{..} + \alpha_r + \beta_c + (\alpha\beta)_{rc}\]</span>
where <span class="math inline">\((\alpha\beta)_{rc}\)</span> is non-zero for at least one group. This notation is kind of ugly to look at, but it is handy to calculate the sum of squares.</p>
<p>How should we calculate the sum of squares for the interaction terms, SS<span class="math inline">\(_{A:B}\)</span>? For Factor A, a good way to estimate the main effect at level <span class="math inline">\(r\)</span> as the difference between the <em>sample</em> marginal mean <span class="math inline">\(\bar{Y}_{rc}\)</span> and the sample grand mean <span class="math inline">\(\bar{Y}_{..}\)</span>. That is, we would use this as our estimate of the effect<a href="#fn63" class="footnote-ref" id="fnref63"><sup>63</sup></a>:
<span class="math display">\[
\hat{\alpha}_r = \bar{Y}_{r.} - \bar{Y}_{..}
\]</span>
Similarly, our estimate of the main effect of Factor B at level <span class="math inline">\(c\)</span> can be defined as follows:
<span class="math display">\[
\hat{\beta}_c = \bar{Y}_{.c} - \bar{Y}_{..}
\]</span></p>
<p>Now, if you go back to the formulas of the SS values for the two main effects, you’ll notice that these effect terms are exactly the quantities that we were squaring and summing! So what’s the analogue of this for interaction terms? The answer to this can be found by first rearranging the formula for the group means <span class="math inline">\(\mu_{rc}\)</span> under the alternative hypothesis, so that we get this:
<span class="math display">\[\begin{eqnarray*}
(\alpha \beta)_{rc} &amp;=&amp; \mu_{rc} - \mu_{..} - \alpha_r - \beta_c \\
&amp;=&amp; \mu_{rc} - \mu_{..} - (\mu_{r.} - \mu_{..}) - (\mu_{.c} - \mu_{..}) \\
&amp;=&amp; \mu_{rc} - \mu_{r.} - \mu_{.c} + \mu_{..}
\end{eqnarray*}\]</span>
So, once again, if we substitute our sample statistics in place of the population means, we get the following as our estimate of the interaction effect for group <span class="math inline">\(rc\)</span>, which is
<span class="math display">\[
\hat{(\alpha\beta)}_{rc} = \bar{Y}_{rc} - \bar{Y}_{r.} - \bar{Y}_{.c} + \bar{Y}_{..}
\]</span></p>
<p>Now all we have to do is sum all of these estimates across all <span class="math inline">\(R\)</span> levels of Factor A and all <span class="math inline">\(C\)</span> levels of Factor B, and we obtain the following formula for the sum of squares associated with the interaction as a whole:
<span class="math display">\[
\mbox{SS}_{A:B} = N \sum_{r=1}^R \sum_{c=1}^C \left( \bar{Y}_{rc} - \bar{Y}_{r.} - \bar{Y}_{.c} + \bar{Y}_{..} \right)^2
\]</span>
where, we multiply by <span class="math inline">\(N\)</span> because there are <span class="math inline">\(N\)</span> observations in each of the groups, and we want our SS values to reflect the variation among <em>observations</em> accounted for by the interaction, not the variation among groups.</p>
<p>Now that we have a formula for calculating SS<span class="math inline">\(_{A:B}\)</span>, it’s important to recognise that the interaction term is part of the model (of course), so the total sum of squares associated with the model, SS<span class="math inline">\(_M\)</span> is now equal to the sum of the three relevant SS values, <span class="math inline">\(\mbox{SS}_A + \mbox{SS}_B + \mbox{SS}_{A:B}\)</span>. The residual sum of squares <span class="math inline">\(\mbox{SS}_R\)</span> is still defined as the leftover variation, namely <span class="math inline">\(\mbox{SS}_T - \mbox{SS}_M\)</span>, but now that we have the interaction term this becomes
<span class="math display">\[
\mbox{SS}_R = \mbox{SS}_T - (\mbox{SS}_A + \mbox{SS}_B + \mbox{SS}_{A:B})
\]</span>
As a consequence, the residual sum of squares SS<span class="math inline">\(_R\)</span> will be smaller than in our original ANOVA that didn’t include interactions.</p>
<p>Calculating the degrees of freedom for the interaction is, once again, slightly trickier than the corresponding calculation for the main effects. To start with, let’s think about the ANOVA model as a whole. Once we include interaction effects in the model, we’re allowing every single group has a unique mean, <span class="math inline">\(\mu_{rc}\)</span>. For an <span class="math inline">\(R \times C\)</span> factorial ANOVA, this means that there are <span class="math inline">\(R \times C\)</span> quantities of interest in the model, and only the one constraint: all of the group means need to average out to the grand mean. So the model as a whole needs to have <span class="math inline">\((R\times C) - 1\)</span> degrees of freedom. But the main effect of Factor A has <span class="math inline">\(R-1\)</span> degrees of freedom, and the main effect of Factor B has <span class="math inline">\(C-1\)</span> degrees of freedom. Which means that the degrees of freedom associated with the interaction is:
<span class="math display">\[
\begin{eqnarray*}
df_{A:B} &amp;=&amp; (R\times C - 1) - (R - 1) - (C -1 ) \\
&amp;=&amp; RC - R - C + 1 \\
&amp;=&amp; (R-1)(C-1)
\end{eqnarray*}
\]</span>
which is just the product of the degrees of freedom associated with the row factor and the column factor.</p>
<p>What about the residual degrees of freedom? Because we’ve added interaction terms, which absorb some degrees of freedom, there are fewer residual degrees of freedom left over. Specifically, note that if the model with interaction has a total of <span class="math inline">\((R \times C) - 1\)</span>, and there are <span class="math inline">\(N\)</span> observations in your data set that are constrained to satisfy 1 grand mean, your residual degrees of freedom now become <span class="math inline">\(N-(R \times C)-1+1\)</span>, or just <span class="math inline">\(N-(R \times C)\)</span>.</p>
<p>This means that our residual degrees of freedom are for the model with interaction are:</p>
<ul>
<li><span class="math inline">\(N-(R \times C)\)</span> for the residual degrees of freedom: 18 - (3 ) = 12</li>
<li><span class="math inline">\((R-1)(C-1)\)</span> for the interaction degrees of freedom: (3-1)(2-1) = 2</li>
<li><span class="math inline">\(R-1\)</span> for the main effect of Factor A (<code>drug</code>): 3 levels - 1 = 2</li>
<li><span class="math inline">\(C-1\)</span> for the main effect of Factor B (<code>therapy</code>): 2 levels - 1 = 1</li>
</ul>
<p>This changes the <span class="math inline">\(F\)</span>-statistic for all of the effects in the model. Let’s look at the CogStat results now.</p>
<div style="margin: 0 0 1.275em; border: 1px solid #ddd; padding:.85em 1em;">
     <p style="font-size:medium; font-weight:600; color:#3960a5;">Hypothesis tests</p>
     <p style="color:#008000;">Testing if the means are the same.<br />
     At least two grouping variables. Interval variable. &gt;&gt; Choosing factorial ANOVA.</p>
     <p style="color:black;">Result of multi-way ANOVA:</p>
     <p style="color:black">Main effect of drug: <span style="font-style:italic;">F</span>(2, 12) = 31.71, <span style="font-style:italic;">p</span> &lt; .001</p>
    <p style="color:black">Main effect of therapy: <span style="font-style:italic;">F</span>(1, 12) = 8.58, <span style="font-style:italic;">p</span> = .013</p>
    <p style="color:black">Interaction of drug and therapy: <span style="font-style:italic;">F</span>(2, 12) = 2.49, <span style="font-style:italic;">p</span> = .125</p>
</div>
</div>
<div id="how-to-interpret-the-results" class="section level3 hasAnchor" number="13.1.4">
<h3><span class="header-section-number">13.1.4</span> How to interpret the results<a href="anova2.html#how-to-interpret-the-results" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are a couple of very important things to consider when interpreting the results of factorial ANOVA. Firstly, there’s the same issue that we had with one-way ANOVA, which is that if you obtain a significant main effect of <code>drug</code>, it doesn’t tell you anything about which drugs are different to one another. Knowing that there’s a significant interaction doesn’t tell you anything about what kind of interaction exists. Again, you’ll need to run additional analyses.</p>
<p>Secondly, there’s a very peculiar interpretation issue that arises when you obtain a significant interaction effect but no corresponding main effect. This happens sometimes. For instance, in the crossover interaction shown in Figure <a href="anova2.html#fig:interaction">13.3</a>, this is exactly what you’d find: in this case, neither of the main effects would be significant, but the interaction effect would be. This is a difficult situation to interpret, and people often get a bit confused about it. The general advice that statisticians like to give in this situation is that you shouldn’t pay much attention to the main effects when an interaction is present. The reason they say this is that, although the tests of the main effects are perfectly valid from a mathematical point of view, when there is a significant interaction effect the main effects rarely test interesting hypotheses. Recall that the null hypothesis for a main effect is that the <em>marginal means</em> are equal to each other, and that a marginal mean is formed by averaging across several different groups. But if you have a significant interaction effect, then you <em>know</em> that the groups that comprise the marginal mean aren’t homogeneous, so it’s not really obvious why you would even care about those marginal means.</p>
<p>Here’s what that means. Again, let’s stick with a clinical example. Suppose that we had a <span class="math inline">\(2 \times 2\)</span> design comparing two different treatments for phobias (e.g., systematic desensitisation vs flooding), and two different anxiety-reducing drugs (e.g., Anxifree vs Joyzepam). Now suppose what we found was that Anxifree had no effect when desensitisation was the treatment, and Joyzepam had no effect when flooding was the treatment. But both were pretty effective for the other treatment. This is a classic crossover interaction, and what we’d find when running the ANOVA is that there is no main effect of drug, but a significant interaction. Now, what does it actually <em>mean</em> to say that there’s no main effect? Well, it means that, if we average over the two different psychological treatments, then the <em>average</em> effect of Anxifree and Joyzepam is the same. But why would anyone care about that? When treating someone for phobias, it is never the case that a person can be treated using an “average” of flooding and desensitisation: that doesn’t make a lot of sense. You either get one or the other. For one treatment, one drug is effective; and for the other treatment, the other drug is effective. The interaction is the important thing; the main effect is kind of irrelevant.</p>
<p>This sort of thing happens a lot: the main effect are tests of marginal means, and when an interaction is present we often find ourselves not being terribly interested in marginal means, because they imply averaging over things that the interaction tells us shouldn’t be averaged! Of course, it’s not always the case that a main effect is meaningless when an interaction is present. Often you can get a big main effect and a very small interaction, in which case you can still say things like “drug A is generally more effective than drug B” (because there was a big effect of drug), but you’d need to modify it a bit by adding that “the difference in effectiveness was different for different psychological treatments”. In any case, the main point here is that whenever you get a significant interaction you should stop and <em>think</em> about what the main effect actually means in this context. Don’t automatically assume that the main effect is interesting.</p>
</div>
</div>
<div id="effectsizefactorialanova" class="section level2 hasAnchor" number="13.2">
<h2><span class="header-section-number">13.2</span> Effect size<a href="anova2.html#effectsizefactorialanova" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The effect size calculations for a factorial ANOVA should be similar to those used in one-way ANOVA (<span class="math inline">\(\eta^2\)</span> and <span class="math inline">\(\omega^2\)</span>). However, when doing a factorial ANOVA, there is a second measure of effect size that people like to report, known as partial <span class="math inline">\(\eta^2\)</span>. The idea behind partial <span class="math inline">\(\eta^2\)</span> (which is sometimes denoted <span class="math inline">\(_p\eta^2\)</span> or <span class="math inline">\(\eta^2_p\)</span>) is that, when measuring the effect size for a particular term (e.g. the main effect of Factor A), you want to deliberately ignore the other effects in the model (e.g. the main effect of Factor B). That is, you would pretend that the effect of all these other terms is zero, and then calculate what the <span class="math inline">\(\eta^2\)</span> value would have been. This is actually pretty easy to calculate. All you have to do is remove the sum of squares associated with the other terms from the denominator. In other words, if you want the partial <span class="math inline">\(\eta^2\)</span> for the main effect of Factor A, the denominator is just the sum of the SS values for Factor A and the residuals:
<span class="math display">\[
\mbox{partial } \eta^2_A = \frac{\mbox{SS}_{A}}{\mbox{SS}_{A} + \mbox{SS}_{R}}
\]</span></p>
<p>This will always give you a larger number than <span class="math inline">\(\eta^2\)</span>, which the cynic in me suspects accounts for the popularity of partial <span class="math inline">\(\eta^2\)</span>. And once again you get a number between 0 and 1, where 0 represents no effect. However, it’s slightly trickier to interpret what a large partial <span class="math inline">\(\eta^2\)</span> value means. In particular, you can’t actually compare the partial <span class="math inline">\(\eta^2\)</span> values across terms! Suppose, for instance, there is no within-groups variability at all: if so, SS<span class="math inline">\(_R = 0\)</span>. What that means is that <em>every</em> term has a partial <span class="math inline">\(\eta^2\)</span> value of 1. But that doesn’t mean that all terms in your model are equally important, or indeed that they are equally large. All it means is that all terms in your model have effect sizes that are large <em>relative to the residual variation</em>. It is not comparable across terms.</p>
<p>CogStat currently doesn’t provide effect sizes for multi-way ANOVAs. But if you’re interested in <span class="math inline">\(\eta^2\)</span> and partial <span class="math inline">\(\eta^2\)</span>, read on.</p>
<p>First, let’s have a look at the effect sizes for the original ANOVA without the interaction term:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
<span class="math inline">\(\eta^2\)</span>
</th>
<th style="text-align:center;">
Partial <span class="math inline">\(\eta^2\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Drug
</td>
<td style="text-align:center;">
0.713
</td>
<td style="text-align:center;">
0.789
</td>
</tr>
<tr>
<td style="text-align:left;">
Therapy
</td>
<td style="text-align:center;">
0.096
</td>
<td style="text-align:center;">
0.336
</td>
</tr>
</tbody>
</table>
<p>Looking at the <span class="math inline">\(\eta^2\)</span> values first, we see that <code>drug</code> accounts for 71.3% of the variance (i.e. <span class="math inline">\(\eta^2 = 0.713\)</span>) in <code>mood_gain</code>, whereas <code>therapy</code> only accounts for 9.6%. This leaves a total of 19.1% of the variation unaccounted for (i.e. the residuals constitute 19.1% of the variation in the outcome). Overall, this implies that we have a very large effect<a href="#fn64" class="footnote-ref" id="fnref64"><sup>64</sup></a> of <code>drug</code> and a modest effect of <code>therapy</code>.</p>
<p>Now let’s look at the partial <span class="math inline">\(\eta^2\)</span> values. Because the effect of <code>therapy</code> isn’t all that large, controlling for it doesn’t make much of a difference, so the partial <span class="math inline">\(\eta^2\)</span> for <code>drug</code> doesn’t increase very much, and we obtain a value of <span class="math inline">\(_p\eta^2 = 0.789\)</span>). In contrast, because the effect of <code>drug</code> was very large, controlling for it makes a big difference, and so when we calculate the partial <span class="math inline">\(\eta^2\)</span> for <code>therapy</code> you can see that it rises to <span class="math inline">\(_p\eta^2 = 0.336\)</span>. The question that we have to ask ourselves is, what does these partial <span class="math inline">\(\eta^2\)</span> values actually <em>mean</em>?</p>
<p>The partial <span class="math inline">\(\eta^2\)</span> for the main effect of Factor A is a statement about a hypothetical experiment in which <em>only</em> Factor A was being varied. So, even though in <em>this</em> experiment we varied both A and B, we can easily imagine an experiment in which only Factor A was varied: the partial <span class="math inline">\(\eta^2\)</span> statistic tells you how much of the variance in the outcome variable you would expect to see accounted for in that experiment. However, it should be noted that this interpretation – like many things associated with main effects – doesn’t make a lot of sense when there is a large and significant interaction effect.</p>
<p>Speaking of interaction effects, here’s what we get when we calculate the effect sizes for the model that includes the interaction term. As you can see, the <span class="math inline">\(\eta^2\)</span> values for the main effects don’t change, but the partial <span class="math inline">\(\eta^2\)</span> values do:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
<span class="math inline">\(\eta^2\)</span>
</th>
<th style="text-align:center;">
Partial <span class="math inline">\(\eta^2\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Drug
</td>
<td style="text-align:center;">
0.713
</td>
<td style="text-align:center;">
0.841
</td>
</tr>
<tr>
<td style="text-align:left;">
Therapy
</td>
<td style="text-align:center;">
0.096
</td>
<td style="text-align:center;">
0.417
</td>
</tr>
<tr>
<td style="text-align:left;">
Drug <span class="math inline">\(\times\)</span> Therapy
</td>
<td style="text-align:center;">
0.056
</td>
<td style="text-align:center;">
0.293
</td>
</tr>
</tbody>
</table>
</div>
<div id="meansfactorialanova" class="section level2 hasAnchor" number="13.3">
<h2><span class="header-section-number">13.3</span> Estimated group means and confidence intervals<a href="anova2.html#meansfactorialanova" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You will find yourself wanting to report estimates of all the group means based on the results of your ANOVA, as well as confidence intervals associated with them. If the ANOVA that you have run is a <strong>saturated model</strong> (i.e. contains all possible main effects and all possible interaction effects), then the estimates of the group means are actually identical to the sample means, though the confidence intervals will use a pooled estimate of the standard errors, rather than use a separate one for each group.</p>
<p>If you look at the <code>Population parameter estimations</code> output from CogStat, you see that there are confidence intervals given for each combination of our variables (Figure <a href="anova2.html#fig:cogstatafci">13.5</a>). Estimated mean mood gain for the placebo group with no therapy was <span class="math inline">\(0.30\)</span>, with a 95% confidence interval from <span class="math inline">\(-0.20\)</span> to <span class="math inline">\(0.80\)</span>. However, that is when you calculate the confidence intervals separately for each group.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cogstatafci"></span>
<img src="resources/image/cogstatafci.png" alt="Population parameter estimations: estimated group means -- reduced model" width="100%" />
<p class="caption">
Figure 13.5: Population parameter estimations: estimated group means – reduced model
</p>
</div>
<p>In a saturated model, the estimated mean mood gain for the placebo group with no therapy should still be <span class="math inline">\(0.30\)</span>, but with a 95% confidence interval from <span class="math inline">\(0.006\)</span> to <span class="math inline">\(0.594\)</span>.</p>
<table>
<caption colspan="6">Estimated group means and confidence intervals</caption>
<thead>
    <tr><th class="cornerLabels" rowspan="2">drug</th><th class="cornerLabels" rowspan="2">therapy</th><th rowspan="2">Mean</th><th rowspan="2">Std. Error</th><th colspan="2">95% Confidence Interval</th></tr>
    <tr><th>Lower Bound</th><th>Upper Bound</th></tr>
</thead>
<tbody data-rect="0,5,0,3">
    <tr><th rowspan="2" style="min-width:71px">Placebo</th><th style="min-width:78px">No therapy</th><td class=" har" style="min-width:68px">.300</td><td class=" har" style="min-width:69px">.135</td><td class=" har" style="min-width:88px">.006</td><td class=" har" style="min-width:90px">.594</td></tr>
    <tr><th>CBT</th><td class=" har">.600</td><td class=" har">.135</td><td class=" har">.306</td><td class=" har">.894</td></tr>
    <tr><th rowspan="2">Anxifree</th><th>No therapy</th><td class=" har">.400</td><td class=" har">.135</td><td class=" har">.106</td><td class=" har">.694</td></tr>
    <tr><th>CBT</th><td class=" har">1.033</td><td class=" har">.135</td><td class=" har">.740</td><td class=" har">1.327</td></tr>
    <tr><th rowspan="2">Joyzepam</th><th>No therapy</th><td class=" har">1.467</td><td class=" har">.135</td><td class=" har">1.173</td><td class=" har">1.760</td></tr>
    <tr><th>CBT</th><td class=" har">1.500</td><td class=" har">.135</td><td class=" har">1.206</td><td class=" har">1.794</td></tr>
</tbody>
</table>
</div>
<div id="posthoc2" class="section level2 hasAnchor" number="13.4">
<h2><span class="header-section-number">13.4</span> Post hoc tests<a href="anova2.html#posthoc2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s suppose you’ve done your ANOVA, and it turns out that you obtained some significant effects. Because of the fact that the <span class="math inline">\(F\)</span>-tests are “omnibus” tests that only really test the null hypothesis that there are no differences among groups, obtaining a significant effect doesn’t tell you which groups are different to which other ones</p>
<p>We discussed this issue back in Chapter <a href="anova.html#anova">12</a>, and in that chapter we mentioned an easy solution to run <span class="math inline">\(t\)</span>-tests for all possible pairs of groups, but CogStat opts for Tukey’s HSD, which is the right call in terms of ANOVA.</p>
<p>We would be interested in the following four comparisons:</p>
<ul>
<li>The difference in mood gain for people given Anxifree versus people given the placebo.</li>
<li>The difference in mood gain for people given Joyzepam versus people given the placebo.</li>
<li>The difference in mood gain for people given Anxifree versus people given Joyzepam.</li>
<li>The difference in mood gain for people treated with CBT and people given no therapy.</li>
</ul>
<p>For any one of these comparisons, we’re interested in the true difference between (population) group means. Tukey’s HSD constructs <strong>simultaneous confidence intervals</strong> for all four of these comparisons. What we mean by 95% “simultaneous” confidence interval is that there is a 95% probability that <em>all</em> of these confidence intervals contain the relevant true value. Moreover, we can use these confidence intervals to calculate an adjusted <span class="math inline">\(p\)</span> value for any specific comparison.</p>
<p>Currently, CogStat does not do a stacked Tukey’s HSD for multi-way ANOVA.</p>
<p>We can run the <code>Compare groups</code> function in CogStat but with only <code>drug</code> as grouping variable, as we did in Chapter <a href="anova.html#posthoc">12.5</a>. The confidence intervals would be slightly different, though. For the second variable, namely <code>therapy</code>, we don’t get a Tukey’s HSD, as it’s a two-level factor.</p>
<p>But what would be quite interesting, is the situation where your model includes interaction terms. The number of pairwise comparisons that we would need to consider starts to increase. As before, we need to consider the three comparisons that are relevant to the main effect of <code>drug</code> and the one comparison that is relevant to the main effect of <code>therapy</code>. But, if we want to consider the possibility of a significant interaction (and try to find the group differences that underpin that significant interaction), we need to include comparisons such as the following:</p>
<ul>
<li>The difference in mood gain for people given Anxifree and treated with CBT, versus people given the placebo and treated with CBT</li>
<li>The difference in mood gain for people given Anxifree and given no therapy, versus people given the placebo and given no therapy.</li>
<li>etc</li>
</ul>
<p>There are quite a lot of these comparisons that you need to consider. So, the ideal output of Tukey’s HSD would make a <em>lot</em> of pairwise comparisons (19 in total). This is not yet possible in CogStat, but let’s look at what the result would look like if it were.</p>
<table>
<caption>
<span id="tab:unnamed-chunk-41">Table 13.2: </span>Tukey’s HSD for a multi-way ANOVA
</caption>
<thead>
<tr>
<th style="text-align:left;">
Pair
</th>
<th style="text-align:center;">
Difference
</th>
<th style="text-align:center;">
CI 95% (Lower)
</th>
<th style="text-align:center;">
CI 95% (Upper)
</th>
<th style="text-align:center;">
<span class="math inline">\(p\)</span>
</th>
</tr>
</thead>
<tbody>
<tr grouplength="5">
<td colspan="5" style="border-bottom: 1px solid;">
<strong>… - placebo-no_therapy</strong>
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
anxifree-no_therapy
</td>
<td style="text-align:center;">
0.100
</td>
<td style="text-align:center;">
-0.540
</td>
<td style="text-align:center;">
0.740
</td>
<td style="text-align:center;">
0.994
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
joyzepam-no_therapy
</td>
<td style="text-align:center;">
1.167
</td>
<td style="text-align:center;">
0.527
</td>
<td style="text-align:center;">
1.807
</td>
<td style="text-align:center;">
0.001
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
placebo-CBT
</td>
<td style="text-align:center;">
0.300
</td>
<td style="text-align:center;">
-0.340
</td>
<td style="text-align:center;">
0.940
</td>
<td style="text-align:center;">
0.628
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
anxifree-CBT
</td>
<td style="text-align:center;">
0.733
</td>
<td style="text-align:center;">
0.093
</td>
<td style="text-align:center;">
1.373
</td>
<td style="text-align:center;">
0.022
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
joyzepam-CBT
</td>
<td style="text-align:center;">
1.200
</td>
<td style="text-align:center;">
0.560
</td>
<td style="text-align:center;">
1.840
</td>
<td style="text-align:center;">
0.000
</td>
</tr>
<tr grouplength="4">
<td colspan="5" style="border-bottom: 1px solid;">
<strong>… - anxifree-no_therapy</strong>
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
joyzepam-no_therapy
</td>
<td style="text-align:center;">
1.067
</td>
<td style="text-align:center;">
0.427
</td>
<td style="text-align:center;">
1.707
</td>
<td style="text-align:center;">
0.001
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
placebo-CBT
</td>
<td style="text-align:center;">
0.200
</td>
<td style="text-align:center;">
-0.440
</td>
<td style="text-align:center;">
0.840
</td>
<td style="text-align:center;">
0.892
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
anxifree-CBT
</td>
<td style="text-align:center;">
0.633
</td>
<td style="text-align:center;">
-0.007
</td>
<td style="text-align:center;">
1.273
</td>
<td style="text-align:center;">
0.053
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
joyzepam-CBT
</td>
<td style="text-align:center;">
1.100
</td>
<td style="text-align:center;">
0.460
</td>
<td style="text-align:center;">
1.740
</td>
<td style="text-align:center;">
0.001
</td>
</tr>
<tr grouplength="3">
<td colspan="5" style="border-bottom: 1px solid;">
<strong>… - joyzepam-no_therapy</strong>
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
placebo-CBT
</td>
<td style="text-align:center;">
-0.867
</td>
<td style="text-align:center;">
-1.507
</td>
<td style="text-align:center;">
-0.227
</td>
<td style="text-align:center;">
0.007
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
anxifree-CBT
</td>
<td style="text-align:center;">
-0.433
</td>
<td style="text-align:center;">
-1.073
</td>
<td style="text-align:center;">
0.207
</td>
<td style="text-align:center;">
0.275
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
joyzepam-CBT
</td>
<td style="text-align:center;">
0.033
</td>
<td style="text-align:center;">
-0.607
</td>
<td style="text-align:center;">
0.673
</td>
<td style="text-align:center;">
1.000
</td>
</tr>
<tr grouplength="2">
<td colspan="5" style="border-bottom: 1px solid;">
<strong>… - placebo-CBT</strong>
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
anxifree-CBT
</td>
<td style="text-align:center;">
0.433
</td>
<td style="text-align:center;">
-0.207
</td>
<td style="text-align:center;">
1.073
</td>
<td style="text-align:center;">
0.275
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
joyzepam-CBT
</td>
<td style="text-align:center;">
0.900
</td>
<td style="text-align:center;">
0.260
</td>
<td style="text-align:center;">
1.540
</td>
<td style="text-align:center;">
0.005
</td>
</tr>
<tr grouplength="1">
<td colspan="5" style="border-bottom: 1px solid;">
<strong>… - anxifree-CBT</strong>
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
joyzepam-CBT
</td>
<td style="text-align:center;">
0.467
</td>
<td style="text-align:center;">
-0.173
</td>
<td style="text-align:center;">
1.107
</td>
<td style="text-align:center;">
0.214
</td>
</tr>
</tbody>
</table>
</div>
<div id="unbalancedanova" class="section level2 hasAnchor" number="13.5">
<h2><span class="header-section-number">13.5</span> Unbalanced designs and types of sums of squares<a href="anova2.html#unbalancedanova" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In real life, we’re rarely lucky enough to have perfectly balanced designs. For one reason or another, it’s typical to end up with more observations in some cells than in others. Or, to put it another way, we have an <strong>unbalanced design</strong>.</p>
<p>Unbalanced designs need to be treated with a lot more care than balanced designs, and the statistical theory that underpins them is a lot messier. But we must not ignore this issue, like a lot of stats textbooks (or psychology students) tend to do. The net result of this is that a lot of active researchers in the field don’t actually know that there’s several different “types” of unbalanced ANOVAs, and they produce quite different answers. In fact, reading the psychological literature, it seems that most people don’t even realise that their statistical software package is making a whole lot of substantive data analysis decisions on their behalf. CogStat will also make all decisions for you, but when interpreting the results, it’s important to know what the software is doing and what is the underlying statistical theory.</p>
<p>As usual, it will help us to work with some data. The <a href="resources/data/coffee.csv"><code>coffee.csv</code></a> file contains a hypothetical data set that produces an unbalanced <span class="math inline">\(3 \times 2\)</span> ANOVA. Suppose we were interested in finding out whether or not the tendency of people to <code>babble</code> when they have too much coffee is purely an effect of the coffee itself, or whether there’s some effect of the <code>milk</code> and <code>sugar</code> that people add to the coffee. Suppose we took 18 people, and gave them some coffee to drink. The amount of coffee / caffeine was held constant, and we varied whether or not milk was added: so <code>milk</code> is a binary factor with two levels, <code>"yes"</code> and <code>"no"</code>. We also varied the kind of sugar involved. The coffee might contain <code>"real"</code> sugar, or it might contain <code>"fake"</code> sugar (i.e., artificial sweetener), or it might contain <code>"none"</code> at all, so the <code>sugar</code> variable is a three-level factor. Our outcome variable is a continuous variable that presumably refers to some psychologically sensible measure of the extent to which someone is “babbling”. The details don’t really matter for our purpose. To get a sense of what the data look like, let’s load it into CogStat and run the <code>Compare groups</code> function. Let’s look at the sample properties:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-42"></span>
<img src="resources/image/cogstatcoffeedescr.png" alt="The coffee data set" width="100%" />
<p class="caption">
Figure 13.6: The coffee data set
</p>
</div>
<p>Across groups, the standard deviation varies from <span class="math inline">\(.10\)</span> to <span class="math inline">\(.62\)</span>, which is fairly small relative to the differences in group means. So far, it’s looking like a straightforward factorial ANOVA, just like we did earlier.</p>
<p>Unbalanced designs lead us to the somewhat unsettling discovery that there isn’t really any one thing that we might refer to as a standard ANOVA. In fact, it turns out that there are <em>three</em> fundamentally different ways<a href="#fn65" class="footnote-ref" id="fnref65"><sup>65</sup></a> in which you might want to run an ANOVA in an unbalanced design. If you have a balanced design, all three versions produce identical results, with the sums of squares, <span class="math inline">\(F\)</span>-values and so on. All conforming to the formulas from the start of this chapter. However, when your design is unbalanced they don’t give the same answers. Furthermore, they are not all equally appropriate to every situation: some methods will be more appropriate to your situation than others. Given all this, it’s important to understand what the different types of ANOVA are and how they differ from one another.</p>
<p>The first kind of ANOVA is conventionally referred to as <em>Type I sum of squares</em>. You can guess what the other two are called. The “sum of squares” part of the name was introduced by the SAS statistical software package, and has become standard nomenclature, but it’s a bit misleading in some ways. The logic for referring to them as different types of sum of squares is that, when you look at the ANOVA tables that they produce, the key difference in the numbers is the SS values. The degrees of freedom don’t change, the MS values are still defined as SS divided by df, etc. However, what the terminology gets wrong is that it hides the reason <em>why</em> the SS values are different from one another.</p>
<p>To that end, it’s a lot more helpful to think of the three different kinds of ANOVA as three different <em>hypothesis testing strategies</em>. These different strategies lead to different SS values, to be sure, but it’s the strategy that is the important thing here, not the SS values themselves. Any particular <span class="math inline">\(F\)</span>-test is best thought of as a comparison between two linear models. So when you’re looking at an ANOVA result, it helps to remember that each of those <span class="math inline">\(F\)</span>-tests corresponds to a <em>pair</em> of models that are being compared. Of course, this leads naturally to the question of <em>which</em> pair of models is being compared. This is the fundamental difference between ANOVA Types I, II and III: each one corresponds to a different way of choosing the model pairs for the tests.</p>
<div id="type-i-sum-of-squares" class="section level3 hasAnchor" number="13.5.1">
<h3><span class="header-section-number">13.5.1</span> Type I sum of squares<a href="anova2.html#type-i-sum-of-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Type I method is sometimes referred to as the “sequential” sum of squares, because it involves a process of adding terms to the model one at a time. Consider the coffee data, for instance. Suppose we want to run the full <span class="math inline">\(3 \times 2\)</span> factorial ANOVA, including interaction terms.</p>
<p>The simplest possible model for the data would be one in which neither milk nor sugar is assumed to have any effect on babbling. The only term that would be included in such a model is the intercept. This is our initial null hypothesis.</p>
<p>The next simplest model for the data would be one in which only one of the two main effects is included. In the coffee data, there are two different possible choices here, because we could choose to add milk first or to add sugar first. The order actually matters, but for now, let’s just make a choice arbitrarily, and pick sugar. So the second model in our sequence of models is <code>babble ~ sugar</code>. This comparison forms our hypothesis test of the main effect of <code>sugar</code>. The next step in our model-building exercise it to add the other main effect term, so the next model in our sequence is <code>babble ~ sugar + milk</code>. This comparison forms our hypothesis test of the main effect of <code>milk</code>. In one sense, this approach is very elegant: the alternative hypothesis from the first test forms the null hypothesis for the second one. It is in this sense that the Type I method is strictly sequential. Every test builds directly on the results of the last one. However, in another sense, it’s very inelegant because there’s a strong asymmetry between the two tests. The test of the main effect of <code>sugar</code> (the first test) completely ignores <code>milk</code>, whereas the test of the main effect of <code>milk</code> (the second test) does take <code>sugar</code> into account. In any case, the fourth model in our sequence is now the full model, <code>babble ~ sugar + milk + sugar:milk</code>.</p>
<table>
<caption>
<span id="tab:unnamed-chunk-43">Table 13.3: </span>Hypotheses for the Type I Sum of Squares for the <code>coffee</code> dataset (sugar first).
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model
</th>
<th style="text-align:left;">
<span class="math inline">\(H_0\)</span> Null hypothesis
</th>
<th style="text-align:left;">
<span class="math inline">\(H_1\)</span> Alternative hypothesis
</th>
<th style="text-align:left;">
Main effect
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Model 1
</td>
<td style="text-align:left;">
babble ~ 1
</td>
<td style="text-align:left;">
babble ~ sugar
</td>
<td style="text-align:left;">
sugar
</td>
</tr>
<tr>
<td style="text-align:left;">
Model 2
</td>
<td style="text-align:left;">
babble ~ sugar
</td>
<td style="text-align:left;">
babble ~ sugar + milk
</td>
<td style="text-align:left;">
milk
</td>
</tr>
<tr>
<td style="text-align:left;">
Model 3
</td>
<td style="text-align:left;">
babble ~ sugar + milk
</td>
<td style="text-align:left;">
babble ~ sugar + milk + sugar×milk
</td>
<td style="text-align:left;">
sugar×milk
</td>
</tr>
</tbody>
</table>
<p>When run (in another statistical software package that allows for selection of Type I, II or III), the results of the analysis are the following:</p>
<table>
<caption>
<span id="tab:unnamed-chunk-44">Table 13.4: </span>Test of between-subject effects with Type I Sum of Squares for the <code>coffee</code> dataset (sugar first).
</caption>
<thead>
<tr>
<th style="text-align:left;">
Source
</th>
<th style="text-align:left;">
Type I SS
</th>
<th style="text-align:left;">
df
</th>
<th style="text-align:left;">
MS
</th>
<th style="text-align:left;">
<span class="math inline">\(F\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(p\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
sugar
</td>
<td style="text-align:left;">
3.558
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
1.779
</td>
<td style="text-align:left;">
6.749
</td>
<td style="text-align:left;">
0.011
</td>
</tr>
<tr>
<td style="text-align:left;">
milk
</td>
<td style="text-align:left;">
0.956
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.956
</td>
<td style="text-align:left;">
3.628
</td>
<td style="text-align:left;">
0.081
</td>
</tr>
<tr>
<td style="text-align:left;">
sugar×milk
</td>
<td style="text-align:left;">
5.944
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
2.972
</td>
<td style="text-align:left;">
11.277
</td>
<td style="text-align:left;">
0.002
</td>
</tr>
</tbody>
</table>
<p>The big problem with using Type I sum of squares is the fact that it really does depend on the order in which you enter the variables. Yet, in many situations, the researcher has no reason to prefer one ordering over another. This is presumably the case for our milk and sugar problem. Should we add milk first, or sugar first? It feels exactly as arbitrary as a data analysis question as it does as a coffee-making question. There may in fact be some people with firm opinions about ordering, but it’s hard to imagine a principled answer to the question. Yet, look what happens when we change the ordering:</p>
<table>
<caption>
<span id="tab:unnamed-chunk-45">Table 13.5: </span>Hypotheses for the Type I Sum of Squares for the <code>coffee</code> dataset (milk first).
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model
</th>
<th style="text-align:left;">
<span class="math inline">\(H_0\)</span> Null hypothesis
</th>
<th style="text-align:left;">
<span class="math inline">\(H_1\)</span> Alternative hypothesis
</th>
<th style="text-align:left;">
Main effect
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Model 1
</td>
<td style="text-align:left;">
babble ~ 1
</td>
<td style="text-align:left;">
babble ~ milk
</td>
<td style="text-align:left;">
milk
</td>
</tr>
<tr>
<td style="text-align:left;">
Model 2
</td>
<td style="text-align:left;">
babble ~ milk
</td>
<td style="text-align:left;">
babble ~ milk + sugar
</td>
<td style="text-align:left;">
sugar
</td>
</tr>
<tr>
<td style="text-align:left;">
Model 3
</td>
<td style="text-align:left;">
babble ~ milk + sugar
</td>
<td style="text-align:left;">
babble ~ milk + sugar + milk×sugar
</td>
<td style="text-align:left;">
milk×sugar
</td>
</tr>
</tbody>
</table>
<table>
<caption>
<span id="tab:unnamed-chunk-46">Table 13.6: </span>Test of between-subject effects with Type I Sum of Squares for the <code>coffee</code> dataset (milk first)
</caption>
<thead>
<tr>
<th style="text-align:left;">
Source
</th>
<th style="text-align:left;">
Type I SS
</th>
<th style="text-align:left;">
df
</th>
<th style="text-align:left;">
MS
</th>
<th style="text-align:left;">
<span class="math inline">\(F\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(p\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
milk
</td>
<td style="text-align:left;">
1.444
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
1.444
</td>
<td style="text-align:left;">
5.479
</td>
<td style="text-align:left;">
0.037
</td>
</tr>
<tr>
<td style="text-align:left;">
sugar
</td>
<td style="text-align:left;">
3.070
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
1.535
</td>
<td style="text-align:left;">
5.824
</td>
<td style="text-align:left;">
0.017
</td>
</tr>
<tr>
<td style="text-align:left;">
milk×sugar
</td>
<td style="text-align:left;">
5.944
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
2.972
</td>
<td style="text-align:left;">
11.277
</td>
<td style="text-align:left;">
0.002
</td>
</tr>
</tbody>
</table>
<p>The <span class="math inline">\(p\)</span>-values for both main effect terms have changed, and fairly dramatically. Among other things, the effect of <code>milk</code> has become significant (though one should avoid drawing any strong conclusions about this, as I’ve mentioned previously). Which of these two ANOVAs should one report? It’s not immediately obvious.</p>
<p>When you look at the hypothesis tests that are used to define the “first” main effect and the “second” one, it’s clear that they’re qualitatively different from one another. In our initial example, we saw that the test for the main effect of <code>sugar</code> completely ignores <code>milk</code>, whereas the test of the main effect of <code>milk</code> does take <code>sugar</code> into account. As such, the Type I testing strategy really does treat the first main effect as if it had a kind of theoretical primacy over the second one. There is very rarely if ever any theoretically primacy of this kind that would justify treating any two main effects asymmetrically.</p>
<p>The consequence of all this is that Type I tests are very rarely of much interest.</p>
</div>
<div id="type-iii-sum-of-squares" class="section level3 hasAnchor" number="13.5.2">
<h3><span class="header-section-number">13.5.2</span> Type III sum of squares<a href="anova2.html#type-iii-sum-of-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Having just finished talking about Type I tests, you might think that the natural thing to do next would be to talk about Type II tests. However, it’s actually a bit more natural to discuss Type III tests (which are simple) before talking about Type II tests (which are trickier). The basic idea behind Type III tests is extremely simple: regardless of which term you’re trying to evaluate, run the <span class="math inline">\(F\)</span>-test in which the alternative hypothesis corresponds to the full ANOVA model as specified by the user, and the null model just deletes that one term that you’re testing. For instance, in the coffee example, the hypotheses would look like this:</p>
<table>
<caption>
<span id="tab:unnamed-chunk-47">Table 13.7: </span>Hypotheses for the Type III Sum of Squares for the <code>coffee</code> dataset.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model
</th>
<th style="text-align:left;">
<span class="math inline">\(H_0\)</span> Null hypothesis
</th>
<th style="text-align:left;">
<span class="math inline">\(H_1\)</span> Alternative hypothesis
</th>
<th style="text-align:left;">
Main effect
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Model 1
</td>
<td style="text-align:left;">
babble ~ milk + sugar×milk
</td>
<td style="text-align:left;">
babble ~ sugar + milk + sugar×milk
</td>
<td style="text-align:left;">
sugar
</td>
</tr>
<tr>
<td style="text-align:left;">
Model 2
</td>
<td style="text-align:left;">
babble ~ sugar + sugar×milk
</td>
<td style="text-align:left;">
babble ~ sugar + milk + sugar×milk
</td>
<td style="text-align:left;">
milk
</td>
</tr>
<tr>
<td style="text-align:left;">
Model 3
</td>
<td style="text-align:left;">
babble ~ sugar + milk
</td>
<td style="text-align:left;">
babble ~ sugar + milk + sugar×milk
</td>
<td style="text-align:left;">
sugar×milk
</td>
</tr>
</tbody>
</table>
<p>At first pass, Type III tests seem like a nice idea. Firstly, we’ve removed the asymmetry that caused us to have problems when running Type I tests. And because we’re now treating all terms the same way, the results of the hypothesis tests do not depend on the order in which we specify them. This is definitely a good thing.</p>
<p>However, there is a big problem when interpreting the results of the tests, especially for main effect terms. Consider the coffee data. Suppose it turns out that the main effect of <code>milk</code> is not significant according to the Type III tests. What this is telling us is that <code>babble ~ sugar + sugar×milk</code> is a better model for the data than the full model. But what does that even <em>mean</em>? If the interaction term <code>sugar×milk</code> was also non-significant, we’d be tempted to conclude that the data are telling us that the only thing that matters is <code>sugar</code>. But suppose we have a significant interaction term, but a non-significant main effect of <code>milk</code>. In this case, are we to assume that there really is an “effect of sugar”, an “interaction between milk and sugar”, but no “effect of milk”? That seems crazy. The right answer simply <em>must</em> be that it’s meaningless<a href="#fn66" class="footnote-ref" id="fnref66"><sup>66</sup></a> to talk about the main effect if the interaction is significant. In general, this seems to be what most statisticians advise us to do. But if it really is meaningless to talk about non-significant main effects in the presence of a significant interaction, then it’s not at all obvious why Type III tests should allow the null hypothesis to rely on a model that includes the interaction but omits one of the main effects that make it up. When characterised in this fashion, the null hypotheses really don’t make much sense at all.</p>
Let’s look at the results:
<table>
<caption>
<span id="tab:unnamed-chunk-48">Table 13.8: </span>Test of between-subject effects with Type III Sum of Squares for the <code>coffee</code> dataset.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Source
</th>
<th style="text-align:left;">
Type III SS
</th>
<th style="text-align:left;">
df
</th>
<th style="text-align:left;">
MS
</th>
<th style="text-align:left;">
<span class="math inline">\(F\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(p\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
milk
</td>
<td style="text-align:left;">
1.004
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
1.004
</td>
<td style="text-align:left;">
3.810
</td>
<td style="text-align:left;">
0.075
</td>
</tr>
<tr>
<td style="text-align:left;">
sugar
</td>
<td style="text-align:left;">
2.132
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
1.066
</td>
<td style="text-align:left;">
4.045
</td>
<td style="text-align:left;">
0.045
</td>
</tr>
<tr>
<td style="text-align:left;">
milk×sugar
</td>
<td style="text-align:left;">
5.944
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
2.972
</td>
<td style="text-align:left;">
11.277
</td>
<td style="text-align:left;">
0.002
</td>
</tr>
</tbody>
</table>
<p>You’ll notice that CogStat will automatically use Type III Sums of Squares. This is exactly the same result set that we get from CogStat whether we put <code>milk</code> or <code>sugar</code> first in the <code>Compare groups</code> dialog’s <code>Group(s)</code> list.</p>
<p>With <code>milk</code> first, we get:</p>
<div style="margin: 0 0 1.275em; border: 1px solid #ddd; padding:.85em 1em;">
     <p style="font-size:medium; font-weight:600; color:#3960a5;">Hypothesis tests</p>
     <p style="color:#008000;">Testing if the means are the same.<br />
     At least two grouping variables. Interval variable. &gt;&gt; Choosing factorial ANOVA.</p>
     <p style="color:black;">Result of multi-way ANOVA:</p>
     <p style="color:black">Main effect of milk: <span style="font-style:italic;">F</span>(1, 12) = 3.81, <span style="font-style:italic;">p</span> = .075</p>
    <p style="color:black">Main effect of sugar: <span style="font-style:italic;">F</span>(2, 12) = 4.04, <span style="font-style:italic;">p</span> = .045</p>
    <p style="color:black">Interaction of milk and sugar: <span style="font-style:italic;">F</span>(2, 12) = 11.28, <span style="font-style:italic;">p</span> = .002</p>
</div>
<p>With <code>sugar</code> first, we get:</p>
<div style="margin: 0 0 1.275em; border: 1px solid #ddd; padding:.85em 1em;">
     <p style="font-size:medium; font-weight:600; color:#3960a5;">Hypothesis tests</p>
     <p style="color:#008000;">Testing if the means are the same.<br />
     At least two grouping variables. Interval variable. &gt;&gt; Choosing factorial ANOVA.</p>
     <p style="color:black;">Result of multi-way ANOVA:</p>
     <p style="color:black">Main effect of sugar: <span style="font-style:italic;">F</span>(2, 12) = 4.04, <span style="font-style:italic;">p</span> = .045</p>
     <p style="color:black">Main effect of milk: <span style="font-style:italic;">F</span>(1, 12) = 3.81, <span style="font-style:italic;">p</span> = .075</p>
     <p style="color:black">Interaction of sugar and milk: <span style="font-style:italic;">F</span>(2, 12) = 11.28, <span style="font-style:italic;">p</span> = .002</p>
</div>
</div>
<div id="type-ii-sum-of-squares" class="section level3 hasAnchor" number="13.5.3">
<h3><span class="header-section-number">13.5.3</span> Type II sum of squares<a href="anova2.html#type-ii-sum-of-squares" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Type II tests are broadly similar to Type III tests: start with a “full” model, and test a particular term by deleting it from that model. However, Type II tests are based on the <strong>marginality principle</strong> which states that you should not omit a lower order term from your model if there are any higher order ones that depend on it. So, for instance, if your model contains the interaction <code>A:B</code> (a 2nd order term), then it really ought to contain the main effects <code>A</code> and <code>B</code> (1st order terms). Similarly, if it contains a three-way interaction term <code>A:B:C</code>, then the model must also include the main effects <code>A</code>, <code>B</code> and <code>C</code> as well as the simpler interactions <code>A:B</code>, <code>A:C</code> and <code>B:C</code>. Type III tests routinely violate the marginality principle. For instance, consider the test of the main effect of <code>A</code> in the context of a three-way ANOVA that includes all possible interaction terms. According to Type III tests, our null and alternative models are:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Null model:
</td>
<td style="text-align:left;">
<code>outcome ~ B + C + A:B + A:C + B:C + A:B:C</code>
</td>
</tr>
<tr>
<td style="text-align:left;">
Alternative model:
</td>
<td style="text-align:left;">
<code>outcome ~ A + B + C + A:B + A:C + B:C + A:B:C</code>
</td>
</tr>
</tbody>
</table>
<p>Notice that the null hypothesis omits <code>A</code>, but includes <code>A:B</code>, <code>A:C</code> and <code>A:B:C</code> as part of the model. This, according to the Type II tests, is not a good choice of null hypothesis. What we should do instead, if we want to test the null hypothesis that <code>A</code> is not relevant to our <code>outcome</code>, is to specify the null hypothesis that is the most complicated model that does not rely on <code>A</code> in any form, even as an interaction. The alternative hypothesis corresponds to this null model plus a main effect term of <code>A</code>. This is a lot closer to what most people would intuitively think of as a “main effect of <code>A</code>”, and it yields the following as our Type II test of the main effect of <code>A</code>.<a href="#fn67" class="footnote-ref" id="fnref67"><sup>67</sup></a></p>
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Null model:
</td>
<td style="text-align:left;">
<code>outcome ~ B + C + B:C</code>
</td>
</tr>
<tr>
<td style="text-align:left;">
Alternative model:
</td>
<td style="text-align:left;">
<code>outcome ~ A + B + C + B:C</code>
</td>
</tr>
</tbody>
</table>
<p>In the context of the two way ANOVA that we’ve been using in the coffee data, the hypothesis tests are even simpler.</p>
<table>
<caption>
<span id="tab:unnamed-chunk-51">Table 13.9: </span>Hypotheses for the Type II Sum of Squares for the <code>coffee</code> dataset.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Model
</th>
<th style="text-align:left;">
<span class="math inline">\(H_0\)</span> Null hypothesis
</th>
<th style="text-align:left;">
<span class="math inline">\(H_1\)</span> Alternative hypothesis
</th>
<th style="text-align:left;">
Main effect
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Model 1
</td>
<td style="text-align:left;">
babble ~ milk
</td>
<td style="text-align:left;">
babble ~ sugar + milk
</td>
<td style="text-align:left;">
sugar
</td>
</tr>
<tr>
<td style="text-align:left;">
Model 2
</td>
<td style="text-align:left;">
babble ~ sugar
</td>
<td style="text-align:left;">
babble ~ sugar + milk
</td>
<td style="text-align:left;">
milk
</td>
</tr>
<tr>
<td style="text-align:left;">
Model 3
</td>
<td style="text-align:left;">
babble ~ sugar + milk
</td>
<td style="text-align:left;">
babble ~ sugar + milk + sugar×milk
</td>
<td style="text-align:left;">
sugar×milk
</td>
</tr>
</tbody>
</table>
<p>The results would be:</p>
<table>
<caption>
<span id="tab:unnamed-chunk-52">Table 13.10: </span>Test of between-subject effects with Type II Sum of Squares for the <code>coffee</code> dataset.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Source
</th>
<th style="text-align:left;">
Type III SS
</th>
<th style="text-align:left;">
df
</th>
<th style="text-align:left;">
MS
</th>
<th style="text-align:left;">
<span class="math inline">\(F\)</span>
</th>
<th style="text-align:left;">
<span class="math inline">\(p\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
milk
</td>
<td style="text-align:left;">
0.956
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.956
</td>
<td style="text-align:left;">
3.628
</td>
<td style="text-align:left;">
0.081
</td>
</tr>
<tr>
<td style="text-align:left;">
sugar
</td>
<td style="text-align:left;">
3.070
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
1.535
</td>
<td style="text-align:left;">
5.824
</td>
<td style="text-align:left;">
0.017
</td>
</tr>
<tr>
<td style="text-align:left;">
milk×sugar
</td>
<td style="text-align:left;">
5.944
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
2.972
</td>
<td style="text-align:left;">
11.277
</td>
<td style="text-align:left;">
0.002
</td>
</tr>
</tbody>
</table>
<p>Type II tests have some clear advantages over Type I and Type III tests: they don’t depend on the order in which you specify factors (unlike Type I), and they don’t depend on the contrasts<a href="#fn68" class="footnote-ref" id="fnref68"><sup>68</sup></a> that you use to specify your factors (unlike Type III).</p>
<p>For the moment, you only have Type III available in CogStat, and it is also the default setting in SPPS. It is important, though, that in psychological literature, researchers seldom bother to report which Type of tests they ran. Often they don’t report what software they used either.</p>
<p>Make sure you indicate what software you used, and if you’re reporting ANOVA results for unbalanced data, then specify what type of tests you ran. Or, even better, do hypotheses tests that correspond to things you really care about, and then report those!</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="60">
<li id="fn60"><p>The nice thing about the subscript notation is that generalises nicely: if our experiment had involved a third factor, then we could just add a third subscript. In principle, the notation extends to as many factors as you might care to include.<a href="anova2.html#fnref60" class="footnote-back">↩︎</a></p></li>
<li id="fn61"><p>Technically, marginalising isn’t quite identical to a regular mean: it’s a weighted average, where you take into account the frequency of the different events that you’re averaging over. However, in a balanced design, all of our cell frequencies are equal by definition, so the two are equivalent.<a href="anova2.html#fnref61" class="footnote-back">↩︎</a></p></li>
<li id="fn62"><p>This will be a term used and explained in Chapter <a href="regression.html#regression">14</a>.<a href="anova2.html#fnref62" class="footnote-back">↩︎</a></p></li>
<li id="fn63"><p>Again, we switched <span class="math inline">\(\mu\)</span> to <span class="math inline">\(\bar{Y}\)</span> to indicate that we’re using the sample mean instead of the population mean.<a href="anova2.html#fnref63" class="footnote-back">↩︎</a></p></li>
<li id="fn64"><p>Implausibly large: the artificiality of this data set is really starting to show!<a href="anova2.html#fnref64" class="footnote-back">↩︎</a></p></li>
<li id="fn65"><p>Actually, this is a bit of a lie. ANOVAs can vary in other ways besides the ones discussed in this book. For instance, we ignored the difference between fixed-effect models, in which the levels of a factor are “fixed” by the experimenter or the world, and random-effect models, in which the levels are random samples from a larger population of possible levels (this book only covers fixed-effect models). Don’t make the mistake of thinking that this book – or any other one – will tell you “everything you need to know” about statistics, any more than a single book could possibly tell you everything you need to know about psychology, physics or philosophy. Life is too complicated for that to <em>ever</em> be true. This isn’t a cause for despair, though. Most researchers get by with a basic working knowledge of ANOVA that doesn’t go any further than this book does. Keep in mind that this book is only the beginning of a very long story, not the whole story.<a href="anova2.html#fnref65" class="footnote-back">↩︎</a></p></li>
<li id="fn66"><p>Or, at the very least, rarely of interest.<a href="anova2.html#fnref66" class="footnote-back">↩︎</a></p></li>
<li id="fn67"><p>Note, of course, that this does depend on the model that the user specified. If original ANOVA model doesn’t contain an interaction term for <code>B:C</code>, then obviously it won’t appear in either the null or the alternative. But that’s true for Types I, II and III. They never include any terms that you <em>didn’t</em> include, but they make different choices about how to construct tests for the ones that you did include. <a href="anova2.html#fnref67" class="footnote-back">↩︎</a></p></li>
<li id="fn68"><p>Which is something we might discuss in a future version of this book.<a href="anova2.html#fnref68" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="anova.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"number_sections": true,
"fig_caption": true
},
"toc_depth": 2,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
