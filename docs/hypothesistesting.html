<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Hypothesis testing | Learning Statistics with CogStat</title>
  <meta name="description" content="Chapter 9 Hypothesis testing | Learning Statistics with CogStat (Second Edition) covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students based on Danielle Navarro’s original Learning Statistics with R with a focus on automatic statistical analysis performed in CogStat." />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Hypothesis testing | Learning Statistics with CogStat" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://learningstatisticswithcogstat.com/resources/bookcover/LSC_small.png" />
  <meta property="og:description" content="Chapter 9 Hypothesis testing | Learning Statistics with CogStat (Second Edition) covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students based on Danielle Navarro’s original Learning Statistics with R with a focus on automatic statistical analysis performed in CogStat." />
  <meta name="github-repo" content="https://github.com/robertfodor/lsc" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Hypothesis testing | Learning Statistics with CogStat" />
  
  <meta name="twitter:description" content="Chapter 9 Hypothesis testing | Learning Statistics with CogStat (Second Edition) covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students based on Danielle Navarro’s original Learning Statistics with R with a focus on automatic statistical analysis performed in CogStat." />
  <meta name="twitter:image" content="https://learningstatisticswithcogstat.com/resources/bookcover/LSC_small.png" />

<meta name="author" content="Danielle Navarro" />
<meta name="author" content="Róbert Fodor" />


<meta name="date" content="2023-09-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="estimation.html"/>
<link rel="next" href="chisquare.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<meta name="twitter:card" content="summary"/>
<meta property="og:type" content="book"/>
<meta property="og:locale" content="en_US"/>
<meta property="article:author" content="Danielle Navarro"/>
<meta property="article:author" content="Róbert Fodor"/>
<meta name="citation_title" content="Chapter 9 Hypothesis testing | Learning Statistics with CogStat (2nd Ed)"/>
<meta name="citation_author" content="Danielle Navarro"/>
<meta name="citation_author" content="Róbert Fodor"/>
<meta name="citation_publication_date" content="2023/09/22"/>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Learning Statistics with CogStat</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this book</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a></li>
<li class="part"><span><b>INTRODUCTIONS</b></span></li>
<li class="chapter" data-level="1" data-path="whywhywhy.html"><a href="whywhywhy.html"><i class="fa fa-check"></i><b>1</b> Why do we learn statistics?</a>
<ul>
<li class="chapter" data-level="1.1" data-path="whywhywhy.html"><a href="whywhywhy.html#the-curse-of-belief-bias"><i class="fa fa-check"></i><b>1.1</b> The curse of belief bias</a></li>
<li class="chapter" data-level="1.2" data-path="whywhywhy.html"><a href="whywhywhy.html#the-simpsons-paradox"><i class="fa fa-check"></i><b>1.2</b> The Simpson’s paradox</a></li>
<li class="chapter" data-level="1.3" data-path="whywhywhy.html"><a href="whywhywhy.html#statistics-in-psychology"><i class="fa fa-check"></i><b>1.3</b> Statistics in psychology</a></li>
<li class="chapter" data-level="1.4" data-path="whywhywhy.html"><a href="whywhywhy.html#theres-more-to-research-methods-than-statistics"><i class="fa fa-check"></i><b>1.4</b> There’s more to research methods than statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="researchdesign.html"><a href="researchdesign.html"><i class="fa fa-check"></i><b>2</b> Basic concepts</a>
<ul>
<li class="chapter" data-level="2.1" data-path="researchdesign.html"><a href="researchdesign.html#measurement"><i class="fa fa-check"></i><b>2.1</b> Introduction to psychological measurement</a></li>
<li class="chapter" data-level="2.2" data-path="researchdesign.html"><a href="researchdesign.html#scales"><i class="fa fa-check"></i><b>2.2</b> Measurement levels</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="researchdesign.html"><a href="researchdesign.html#nominalscale"><i class="fa fa-check"></i><b>2.2.1</b> Nominal categories</a></li>
<li class="chapter" data-level="2.2.2" data-path="researchdesign.html"><a href="researchdesign.html#ordinalscale"><i class="fa fa-check"></i><b>2.2.2</b> Ordinal scale and rank</a></li>
<li class="chapter" data-level="2.2.3" data-path="researchdesign.html"><a href="researchdesign.html#intervalscale"><i class="fa fa-check"></i><b>2.2.3</b> Interval scale</a></li>
<li class="chapter" data-level="2.2.4" data-path="researchdesign.html"><a href="researchdesign.html#ratioscale"><i class="fa fa-check"></i><b>2.2.4</b> Ratio scale</a></li>
<li class="chapter" data-level="2.2.5" data-path="researchdesign.html"><a href="researchdesign.html#likertscale"><i class="fa fa-check"></i><b>2.2.5</b> The special case of the Likert scale</a></li>
<li class="chapter" data-level="2.2.6" data-path="researchdesign.html"><a href="researchdesign.html#continuousdiscrete"><i class="fa fa-check"></i><b>2.2.6</b> Continuous versus discrete variables</a></li>
<li class="chapter" data-level="2.2.7" data-path="researchdesign.html"><a href="researchdesign.html#summaryguidelevels"><i class="fa fa-check"></i><b>2.2.7</b> A summary guide for levels of measurement</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="researchdesign.html"><a href="researchdesign.html#ivdv"><i class="fa fa-check"></i><b>2.3</b> Independent and dependent variables</a></li>
<li class="chapter" data-level="2.4" data-path="researchdesign.html"><a href="researchdesign.html#reliability"><i class="fa fa-check"></i><b>2.4</b> Reliability</a></li>
<li class="chapter" data-level="2.5" data-path="researchdesign.html"><a href="researchdesign.html#validity"><i class="fa fa-check"></i><b>2.5</b> Validity</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="autostat.html"><a href="autostat.html"><i class="fa fa-check"></i><b>3</b> Automatic statistical analysis</a></li>
<li class="chapter" data-level="4" data-path="cogstatintro.html"><a href="cogstatintro.html"><i class="fa fa-check"></i><b>4</b> Introduction to CogStat</a></li>
<li class="part"><span><b>DESCRIPTIVE STATISTICS</b></span></li>
<li class="chapter" data-level="5" data-path="exploringavariable.html"><a href="exploringavariable.html"><i class="fa fa-check"></i><b>5</b> Exploring a single variable</a>
<ul>
<li class="chapter" data-level="5.1" data-path="exploringavariable.html"><a href="exploringavariable.html#centraltendency"><i class="fa fa-check"></i><b>5.1</b> Measures of central tendency</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="exploringavariable.html"><a href="exploringavariable.html#mean"><i class="fa fa-check"></i><b>5.1.1</b> The mean</a></li>
<li class="chapter" data-level="5.1.2" data-path="exploringavariable.html"><a href="exploringavariable.html#median"><i class="fa fa-check"></i><b>5.1.2</b> The median</a></li>
<li class="chapter" data-level="5.1.3" data-path="exploringavariable.html"><a href="exploringavariable.html#mean-or-median-whats-the-difference"><i class="fa fa-check"></i><b>5.1.3</b> Mean or median? What’s the difference?</a></li>
<li class="chapter" data-level="5.1.4" data-path="exploringavariable.html"><a href="exploringavariable.html#trimmedmean"><i class="fa fa-check"></i><b>5.1.4</b> Trimmed mean</a></li>
<li class="chapter" data-level="5.1.5" data-path="exploringavariable.html"><a href="exploringavariable.html#mode"><i class="fa fa-check"></i><b>5.1.5</b> Mode</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="exploringavariable.html"><a href="exploringavariable.html#var"><i class="fa fa-check"></i><b>5.2</b> Measures of variability</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="exploringavariable.html"><a href="exploringavariable.html#range"><i class="fa fa-check"></i><b>5.2.1</b> Range</a></li>
<li class="chapter" data-level="5.2.2" data-path="exploringavariable.html"><a href="exploringavariable.html#IQR"><i class="fa fa-check"></i><b>5.2.2</b> Interquartile range</a></li>
<li class="chapter" data-level="5.2.3" data-path="exploringavariable.html"><a href="exploringavariable.html#aad"><i class="fa fa-check"></i><b>5.2.3</b> Mean absolute deviation (average absolute deviation)</a></li>
<li class="chapter" data-level="5.2.4" data-path="exploringavariable.html"><a href="exploringavariable.html#mad"><i class="fa fa-check"></i><b>5.2.4</b> Median absolute deviation</a></li>
<li class="chapter" data-level="5.2.5" data-path="exploringavariable.html"><a href="exploringavariable.html#variance"><i class="fa fa-check"></i><b>5.2.5</b> Variance</a></li>
<li class="chapter" data-level="5.2.6" data-path="exploringavariable.html"><a href="exploringavariable.html#sd"><i class="fa fa-check"></i><b>5.2.6</b> Standard deviation</a></li>
<li class="chapter" data-level="5.2.7" data-path="exploringavariable.html"><a href="exploringavariable.html#which-measure-to-use"><i class="fa fa-check"></i><b>5.2.7</b> Which measure to use?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="exploringavariable.html"><a href="exploringavariable.html#skewnesskurtosis"><i class="fa fa-check"></i><b>5.3</b> Skewness and kurtosis</a></li>
<li class="chapter" data-level="5.4" data-path="exploringavariable.html"><a href="exploringavariable.html#zscore"><i class="fa fa-check"></i><b>5.4</b> Standard scores (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>-score)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="correl.html"><a href="correl.html"><i class="fa fa-check"></i><b>6</b> Exploring a variable pair</a>
<ul>
<li class="chapter" data-level="6.1" data-path="correl.html"><a href="correl.html#the-strength-and-direction-of-a-relationship"><i class="fa fa-check"></i><b>6.1</b> The strength and direction of a relationship</a></li>
<li class="chapter" data-level="6.2" data-path="correl.html"><a href="correl.html#pearson"><i class="fa fa-check"></i><b>6.2</b> The correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="correl.html"><a href="correl.html#interpretingcorrelations"><i class="fa fa-check"></i><b>6.3</b> Interpreting a correlation</a></li>
<li class="chapter" data-level="6.4" data-path="correl.html"><a href="correl.html#spearman"><i class="fa fa-check"></i><b>6.4</b> Spearman’s rank correlations</a></li>
<li class="chapter" data-level="6.5" data-path="correl.html"><a href="correl.html#missingvaluespair"><i class="fa fa-check"></i><b>6.5</b> Missing values in pairwise calculations</a></li>
</ul></li>
<li class="part"><span><b>INFERENTIAL STATISTICS</b></span></li>
<li class="chapter" data-level="7" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>7</b> Probability and distributions</a>
<ul>
<li class="chapter" data-level="7.1" data-path="probability.html"><a href="probability.html#probabilitystats"><i class="fa fa-check"></i><b>7.1</b> How are probability and statistics different?</a></li>
<li class="chapter" data-level="7.2" data-path="probability.html"><a href="probability.html#probabilitymeaning"><i class="fa fa-check"></i><b>7.2</b> What does probability mean?</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="probability.html"><a href="probability.html#the-frequentist-view"><i class="fa fa-check"></i><b>7.2.1</b> The frequentist view</a></li>
<li class="chapter" data-level="7.2.2" data-path="probability.html"><a href="probability.html#the-bayesian-view"><i class="fa fa-check"></i><b>7.2.2</b> The Bayesian view</a></li>
<li class="chapter" data-level="7.2.3" data-path="probability.html"><a href="probability.html#whats-the-difference-and-who-is-right"><i class="fa fa-check"></i><b>7.2.3</b> What’s the difference? And who is right?</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="probability.html"><a href="probability.html#basicprobability"><i class="fa fa-check"></i><b>7.3</b> Basic probability theory</a></li>
<li class="chapter" data-level="7.4" data-path="probability.html"><a href="probability.html#distributions"><i class="fa fa-check"></i><b>7.4</b> Distributions</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="probability.html"><a href="probability.html#binomial"><i class="fa fa-check"></i><b>7.4.1</b> The binomial distribution</a></li>
<li class="chapter" data-level="7.4.2" data-path="probability.html"><a href="probability.html#normal"><i class="fa fa-check"></i><b>7.4.2</b> The normal distribution</a></li>
<li class="chapter" data-level="7.4.3" data-path="probability.html"><a href="probability.html#density"><i class="fa fa-check"></i><b>7.4.3</b> Probability density</a></li>
<li class="chapter" data-level="7.4.4" data-path="probability.html"><a href="probability.html#otherdists"><i class="fa fa-check"></i><b>7.4.4</b> Other useful distributions</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="probability.html"><a href="probability.html#summary-3"><i class="fa fa-check"></i><b>7.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>8</b> Population, sampling, estimation</a>
<ul>
<li class="chapter" data-level="8.1" data-path="estimation.html"><a href="estimation.html#srs"><i class="fa fa-check"></i><b>8.1</b> Samples, populations and sampling</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="estimation.html"><a href="estimation.html#pop"><i class="fa fa-check"></i><b>8.1.1</b> Defining a population</a></li>
<li class="chapter" data-level="8.1.2" data-path="estimation.html"><a href="estimation.html#simple-random-samples"><i class="fa fa-check"></i><b>8.1.2</b> Simple random samples</a></li>
<li class="chapter" data-level="8.1.3" data-path="estimation.html"><a href="estimation.html#most-samples-are-not-simple-random-samples"><i class="fa fa-check"></i><b>8.1.3</b> Most samples are not simple random samples</a></li>
<li class="chapter" data-level="8.1.4" data-path="estimation.html"><a href="estimation.html#how-much-does-it-matter-if-you-dont-have-a-simple-random-sample"><i class="fa fa-check"></i><b>8.1.4</b> How much does it matter if you don’t have a simple random sample?</a></li>
<li class="chapter" data-level="8.1.5" data-path="estimation.html"><a href="estimation.html#population-parameters-and-sample-statistics"><i class="fa fa-check"></i><b>8.1.5</b> Population parameters and sample statistics</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="estimation.html"><a href="estimation.html#lawlargenumbers"><i class="fa fa-check"></i><b>8.2</b> The law of large numbers</a></li>
<li class="chapter" data-level="8.3" data-path="estimation.html"><a href="estimation.html#samplesandclt"><i class="fa fa-check"></i><b>8.3</b> Sampling distributions and the central limit theorem</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="estimation.html"><a href="estimation.html#samplingdists"><i class="fa fa-check"></i><b>8.3.1</b> Sampling distribution of the mean</a></li>
<li class="chapter" data-level="8.3.2" data-path="estimation.html"><a href="estimation.html#sampling-distributions-exist-for-any-sample-statistic"><i class="fa fa-check"></i><b>8.3.2</b> Sampling distributions exist for any sample statistic!</a></li>
<li class="chapter" data-level="8.3.3" data-path="estimation.html"><a href="estimation.html#clt"><i class="fa fa-check"></i><b>8.3.3</b> The central limit theorem</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="estimation.html"><a href="estimation.html#pointestimates"><i class="fa fa-check"></i><b>8.4</b> Estimating population parameters</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="estimation.html"><a href="estimation.html#estimating-the-population-mean"><i class="fa fa-check"></i><b>8.4.1</b> Estimating the population mean</a></li>
<li class="chapter" data-level="8.4.2" data-path="estimation.html"><a href="estimation.html#estimating-the-population-standard-deviation"><i class="fa fa-check"></i><b>8.4.2</b> Estimating the population standard deviation</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="estimation.html"><a href="estimation.html#ci"><i class="fa fa-check"></i><b>8.5</b> Estimating a confidence interval</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="estimation.html"><a href="estimation.html#a-slight-mistake-in-the-formula"><i class="fa fa-check"></i><b>8.5.1</b> A slight mistake in the formula</a></li>
<li class="chapter" data-level="8.5.2" data-path="estimation.html"><a href="estimation.html#interpreting-a-confidence-interval"><i class="fa fa-check"></i><b>8.5.2</b> Interpreting a confidence interval</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="estimation.html"><a href="estimation.html#population-parameter-estimations-in-cogstat"><i class="fa fa-check"></i><b>8.6</b> Population parameter estimations in CogStat</a></li>
<li class="chapter" data-level="8.7" data-path="estimation.html"><a href="estimation.html#summary-4"><i class="fa fa-check"></i><b>8.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="hypothesistesting.html"><a href="hypothesistesting.html"><i class="fa fa-check"></i><b>9</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="9.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#hypotheses"><i class="fa fa-check"></i><b>9.1</b> A menagerie of hypotheses</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#research-hypotheses-versus-statistical-hypotheses"><i class="fa fa-check"></i><b>9.1.1</b> Research hypotheses versus statistical hypotheses</a></li>
<li class="chapter" data-level="9.1.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#null-hypotheses-and-alternative-hypotheses"><i class="fa fa-check"></i><b>9.1.2</b> Null hypotheses and alternative hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#errortypes"><i class="fa fa-check"></i><b>9.2</b> Two types of errors</a></li>
<li class="chapter" data-level="9.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#teststatistics"><i class="fa fa-check"></i><b>9.3</b> Test statistics and sampling distributions</a></li>
<li class="chapter" data-level="9.4" data-path="hypothesistesting.html"><a href="hypothesistesting.html#decisionmaking"><i class="fa fa-check"></i><b>9.4</b> Making decisions</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#critical-regions-and-critical-values"><i class="fa fa-check"></i><b>9.4.1</b> Critical regions and critical values</a></li>
<li class="chapter" data-level="9.4.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-note-on-statistical-significance"><i class="fa fa-check"></i><b>9.4.2</b> A note on statistical “significance”</a></li>
<li class="chapter" data-level="9.4.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#onesidedtests"><i class="fa fa-check"></i><b>9.4.3</b> The difference between one sided and two sided tests</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="hypothesistesting.html"><a href="hypothesistesting.html#pvalue"><i class="fa fa-check"></i><b>9.5</b> The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value of a test</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-softer-view-of-decision-making"><i class="fa fa-check"></i><b>9.5.1</b> A softer view of decision making</a></li>
<li class="chapter" data-level="9.5.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-probability-of-extreme-data"><i class="fa fa-check"></i><b>9.5.2</b> The probability of extreme data</a></li>
<li class="chapter" data-level="9.5.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#a-common-mistake"><i class="fa fa-check"></i><b>9.5.3</b> A common mistake</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="hypothesistesting.html"><a href="hypothesistesting.html#writeup"><i class="fa fa-check"></i><b>9.6</b> Reporting the results of a hypothesis test</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-issue"><i class="fa fa-check"></i><b>9.6.1</b> The issue</a></li>
<li class="chapter" data-level="9.6.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#two-proposed-solutions"><i class="fa fa-check"></i><b>9.6.2</b> Two proposed solutions</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effectsize"><i class="fa fa-check"></i><b>9.7</b> Effect size, sample size and power</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#the-power-function"><i class="fa fa-check"></i><b>9.7.1</b> The power function</a></li>
<li class="chapter" data-level="9.7.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#effect-size"><i class="fa fa-check"></i><b>9.7.2</b> Effect size</a></li>
<li class="chapter" data-level="9.7.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#increasing-the-power-of-your-study"><i class="fa fa-check"></i><b>9.7.3</b> Increasing the power of your study</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="hypothesistesting.html"><a href="hypothesistesting.html#nhstmess"><i class="fa fa-check"></i><b>9.8</b> Some issues to consider</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="hypothesistesting.html"><a href="hypothesistesting.html#neyman-versus-fisher"><i class="fa fa-check"></i><b>9.8.1</b> Neyman versus Fisher</a></li>
<li class="chapter" data-level="9.8.2" data-path="hypothesistesting.html"><a href="hypothesistesting.html#bayesians-versus-frequentists"><i class="fa fa-check"></i><b>9.8.2</b> Bayesians versus frequentists</a></li>
<li class="chapter" data-level="9.8.3" data-path="hypothesistesting.html"><a href="hypothesistesting.html#traps"><i class="fa fa-check"></i><b>9.8.3</b> Traps</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="hypothesistesting.html"><a href="hypothesistesting.html#summary-5"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>STATISTICAL TOOLS</b></span></li>
<li class="chapter" data-level="10" data-path="chisquare.html"><a href="chisquare.html"><i class="fa fa-check"></i><b>10</b> Categorical data analysis</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chisquare.html"><a href="chisquare.html#goftest"><i class="fa fa-check"></i><b>10.1</b> The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>χ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math> goodness-of-fit test</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="chisquare.html"><a href="chisquare.html#the-null-hypothesis-and-the-alternative-hypothesis"><i class="fa fa-check"></i><b>10.1.1</b> The null hypothesis and the alternative hypothesis</a></li>
<li class="chapter" data-level="10.1.2" data-path="chisquare.html"><a href="chisquare.html#the-goodness-of-fit-test-statistic"><i class="fa fa-check"></i><b>10.1.2</b> The “goodness of fit” test statistic</a></li>
<li class="chapter" data-level="10.1.3" data-path="chisquare.html"><a href="chisquare.html#the-sampling-distribution-of-the-gof-statistic-advanced"><i class="fa fa-check"></i><b>10.1.3</b> The sampling distribution of the GOF statistic (advanced)</a></li>
<li class="chapter" data-level="10.1.4" data-path="chisquare.html"><a href="chisquare.html#degrees-of-freedom"><i class="fa fa-check"></i><b>10.1.4</b> Degrees of freedom</a></li>
<li class="chapter" data-level="10.1.5" data-path="chisquare.html"><a href="chisquare.html#testing-the-null-hypothesis"><i class="fa fa-check"></i><b>10.1.5</b> Testing the null hypothesis</a></li>
<li class="chapter" data-level="10.1.6" data-path="chisquare.html"><a href="chisquare.html#chisqreport"><i class="fa fa-check"></i><b>10.1.6</b> How to report the results of the test</a></li>
<li class="chapter" data-level="10.1.7" data-path="chisquare.html"><a href="chisquare.html#a-comment-on-statistical-notation-advanced"><i class="fa fa-check"></i><b>10.1.7</b> A comment on statistical notation (advanced)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="chisquare.html"><a href="chisquare.html#chisqindependence"><i class="fa fa-check"></i><b>10.2</b> The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>χ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math> test of independence (or association)</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="chisquare.html"><a href="chisquare.html#constructing-our-hypothesis-test"><i class="fa fa-check"></i><b>10.2.1</b> Constructing our hypothesis test</a></li>
<li class="chapter" data-level="10.2.2" data-path="chisquare.html"><a href="chisquare.html#AssocTestInCogStat"><i class="fa fa-check"></i><b>10.2.2</b> The test results in CogStat</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chisquare.html"><a href="chisquare.html#yates"><i class="fa fa-check"></i><b>10.3</b> Yates correction for 1 degree of freedom</a></li>
<li class="chapter" data-level="10.4" data-path="chisquare.html"><a href="chisquare.html#chisqeffectsize"><i class="fa fa-check"></i><b>10.4</b> Effect size (Cramér’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>)</a></li>
<li class="chapter" data-level="10.5" data-path="chisquare.html"><a href="chisquare.html#chisqassumptions"><i class="fa fa-check"></i><b>10.5</b> Assumptions of the test(s)</a></li>
<li class="chapter" data-level="10.6" data-path="chisquare.html"><a href="chisquare.html#fisherexacttest"><i class="fa fa-check"></i><b>10.6</b> The Fisher exact test</a></li>
<li class="chapter" data-level="10.7" data-path="chisquare.html"><a href="chisquare.html#mcnemar"><i class="fa fa-check"></i><b>10.7</b> The McNemar test</a></li>
<li class="chapter" data-level="10.8" data-path="chisquare.html"><a href="chisquare.html#whats-the-difference-between-mcnemar-and-independence"><i class="fa fa-check"></i><b>10.8</b> What’s the difference between McNemar and independence?</a></li>
<li class="chapter" data-level="10.9" data-path="chisquare.html"><a href="chisquare.html#summary-6"><i class="fa fa-check"></i><b>10.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ttest.html"><a href="ttest.html"><i class="fa fa-check"></i><b>11</b> Comparing two means</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ttest.html"><a href="ttest.html#ztest"><i class="fa fa-check"></i><b>11.1</b> The one-sample <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>-test</a></li>
<li class="chapter" data-level="11.2" data-path="ttest.html"><a href="ttest.html#onesamplettest"><i class="fa fa-check"></i><b>11.2</b> The one-sample <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-test</a></li>
<li class="chapter" data-level="11.3" data-path="ttest.html"><a href="ttest.html#studentttest"><i class="fa fa-check"></i><b>11.3</b> The independent samples <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-test (Student test)</a></li>
<li class="chapter" data-level="11.4" data-path="ttest.html"><a href="ttest.html#welchttest"><i class="fa fa-check"></i><b>11.4</b> The independent samples <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-test (Welch test)</a></li>
<li class="chapter" data-level="11.5" data-path="ttest.html"><a href="ttest.html#pairedsamplesttest"><i class="fa fa-check"></i><b>11.5</b> The paired-samples <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-test</a></li>
<li class="chapter" data-level="11.6" data-path="ttest.html"><a href="ttest.html#cohensd"><i class="fa fa-check"></i><b>11.6</b> Effect size (Cohen’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math>)</a></li>
<li class="chapter" data-level="11.7" data-path="ttest.html"><a href="ttest.html#shapiro"><i class="fa fa-check"></i><b>11.7</b> Normality of a sample</a></li>
<li class="chapter" data-level="11.8" data-path="ttest.html"><a href="ttest.html#wilcox"><i class="fa fa-check"></i><b>11.8</b> Testing non-normal data with Wilcoxon tests</a>
<ul>
<li class="chapter" data-level="11.8.1" data-path="ttest.html"><a href="ttest.html#mannwhitney"><i class="fa fa-check"></i><b>11.8.1</b> Two-sample Wilcoxon test (Mann-Whitney test)</a></li>
<li class="chapter" data-level="11.8.2" data-path="ttest.html"><a href="ttest.html#wilcoxon"><i class="fa fa-check"></i><b>11.8.2</b> One-sample and paired samples Wilcoxon tests</a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="ttest.html"><a href="ttest.html#summary-7"><i class="fa fa-check"></i><b>11.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>12</b> Comparing several means (one-way ANOVA)</a>
<ul>
<li class="chapter" data-level="12.1" data-path="anova.html"><a href="anova.html#the-data"><i class="fa fa-check"></i><b>12.1</b> The data</a></li>
<li class="chapter" data-level="12.2" data-path="anova.html"><a href="anova.html#anovaintro"><i class="fa fa-check"></i><b>12.2</b> How ANOVA works</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="anova.html"><a href="anova.html#from-variance"><i class="fa fa-check"></i><b>12.2.1</b> From variance…</a></li>
<li class="chapter" data-level="12.2.2" data-path="anova.html"><a href="anova.html#to-total-sum-of-squares"><i class="fa fa-check"></i><b>12.2.2</b> … to total sum of squares</a></li>
<li class="chapter" data-level="12.2.3" data-path="anova.html"><a href="anova.html#from-sums-of-squares-to-the-f-test"><i class="fa fa-check"></i><b>12.2.3</b> From sums of squares to the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>-test</a></li>
<li class="chapter" data-level="12.2.4" data-path="anova.html"><a href="anova.html#anovamodel"><i class="fa fa-check"></i><b>12.2.4</b> Further reading: the meaning of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math> (advanced)</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="anova.html"><a href="anova.html#introduceaov"><i class="fa fa-check"></i><b>12.3</b> Interpreting our results in CogStat</a></li>
<li class="chapter" data-level="12.4" data-path="anova.html"><a href="anova.html#anovaeffect"><i class="fa fa-check"></i><b>12.4</b> Effect size</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="anova.html"><a href="anova.html#eta-squared"><i class="fa fa-check"></i><b>12.4.1</b> Eta-squared</a></li>
<li class="chapter" data-level="12.4.2" data-path="anova.html"><a href="anova.html#omega-squared"><i class="fa fa-check"></i><b>12.4.2</b> Omega-squared</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="anova.html"><a href="anova.html#posthoc"><i class="fa fa-check"></i><b>12.5</b> Post hoc tests</a></li>
<li class="chapter" data-level="12.6" data-path="anova.html"><a href="anova.html#levene"><i class="fa fa-check"></i><b>12.6</b> Checking the homogeneity of variance assumption</a></li>
<li class="chapter" data-level="12.7" data-path="anova.html"><a href="anova.html#kruskalwallis"><i class="fa fa-check"></i><b>12.7</b> Testing for non-normal data with Kruskal-Wallis test</a></li>
<li class="chapter" data-level="12.8" data-path="anova.html"><a href="anova.html#anovaandt"><i class="fa fa-check"></i><b>12.8</b> On the relationship between ANOVA and the Student <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-test</a></li>
<li class="chapter" data-level="12.9" data-path="anova.html"><a href="anova.html#summary-8"><i class="fa fa-check"></i><b>12.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="anova2.html"><a href="anova2.html"><i class="fa fa-check"></i><b>13</b> Comparing several groups (factorial ANOVA)</a>
<ul>
<li class="chapter" data-level="13.1" data-path="anova2.html"><a href="anova2.html#factorialanovasimple"><i class="fa fa-check"></i><b>13.1</b> Balanced designs</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="anova2.html"><a href="anova2.html#factanovahyp"><i class="fa fa-check"></i><b>13.1.1</b> What hypotheses are we testing?</a></li>
<li class="chapter" data-level="13.1.2" data-path="anova2.html"><a href="anova2.html#means-sums-of-squares-and-degrees-of-freedom"><i class="fa fa-check"></i><b>13.1.2</b> Means, sums of squares, and degrees of freedom</a></li>
<li class="chapter" data-level="13.1.3" data-path="anova2.html"><a href="anova2.html#the-interaction"><i class="fa fa-check"></i><b>13.1.3</b> The <em>interaction</em></a></li>
<li class="chapter" data-level="13.1.4" data-path="anova2.html"><a href="anova2.html#how-to-interpret-the-results"><i class="fa fa-check"></i><b>13.1.4</b> How to interpret the results</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="anova2.html"><a href="anova2.html#effectsizefactorialanova"><i class="fa fa-check"></i><b>13.2</b> Effect size</a></li>
<li class="chapter" data-level="13.3" data-path="anova2.html"><a href="anova2.html#meansfactorialanova"><i class="fa fa-check"></i><b>13.3</b> Estimated group means and confidence intervals</a></li>
<li class="chapter" data-level="13.4" data-path="anova2.html"><a href="anova2.html#posthoc2"><i class="fa fa-check"></i><b>13.4</b> Post hoc tests</a></li>
<li class="chapter" data-level="13.5" data-path="anova2.html"><a href="anova2.html#unbalancedanova"><i class="fa fa-check"></i><b>13.5</b> Unbalanced designs and types of sums of squares</a>
<ul>
<li class="chapter" data-level="13.5.1" data-path="anova2.html"><a href="anova2.html#type-i-sum-of-squares"><i class="fa fa-check"></i><b>13.5.1</b> Type I sum of squares</a></li>
<li class="chapter" data-level="13.5.2" data-path="anova2.html"><a href="anova2.html#type-iii-sum-of-squares"><i class="fa fa-check"></i><b>13.5.2</b> Type III sum of squares</a></li>
<li class="chapter" data-level="13.5.3" data-path="anova2.html"><a href="anova2.html#type-ii-sum-of-squares"><i class="fa fa-check"></i><b>13.5.3</b> Type II sum of squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>14</b> Linear regression</a>
<ul>
<li class="chapter" data-level="14.1" data-path="regression.html"><a href="regression.html#introregression"><i class="fa fa-check"></i><b>14.1</b> What is a linear regression model?</a></li>
<li class="chapter" data-level="14.2" data-path="regression.html"><a href="regression.html#regressionestimation"><i class="fa fa-check"></i><b>14.2</b> Estimating a linear regression model</a></li>
<li class="chapter" data-level="14.3" data-path="regression.html"><a href="regression.html#regressioninterpretation"><i class="fa fa-check"></i><b>14.3</b> Interpreting the results of a linear regression</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="regression.html"><a href="regression.html#confidence-intervals-for-the-coefficients"><i class="fa fa-check"></i><b>14.3.1</b> Confidence intervals for the coefficients</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="regression.html"><a href="regression.html#r2"><i class="fa fa-check"></i><b>14.4</b> Quantifying the fit of the regression model</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="regression.html"><a href="regression.html#the-adjusted-r2-value"><i class="fa fa-check"></i><b>14.4.1</b> The adjusted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding="application/x-tex">R^2</annotation></semantics></math> value</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="regression.html"><a href="regression.html#regressiontests"><i class="fa fa-check"></i><b>14.5</b> Hypothesis tests for regression models</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="regression.html"><a href="regression.html#testing-the-model-as-a-whole"><i class="fa fa-check"></i><b>14.5.1</b> Testing the model as a whole</a></li>
<li class="chapter" data-level="14.5.2" data-path="regression.html"><a href="regression.html#tests-for-individual-coefficients"><i class="fa fa-check"></i><b>14.5.2</b> Tests for individual coefficients</a></li>
<li class="chapter" data-level="14.5.3" data-path="regression.html"><a href="regression.html#stdcoef"><i class="fa fa-check"></i><b>14.5.3</b> Calculating standardised regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="regression.html"><a href="regression.html#regressiondiagnostics"><i class="fa fa-check"></i><b>14.6</b> Model checking</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="regression.html"><a href="regression.html#three-kinds-of-residuals"><i class="fa fa-check"></i><b>14.6.1</b> Three kinds of residuals</a></li>
<li class="chapter" data-level="14.6.2" data-path="regression.html"><a href="regression.html#regressionoutliers"><i class="fa fa-check"></i><b>14.6.2</b> Three kinds of anomalous data</a></li>
<li class="chapter" data-level="14.6.3" data-path="regression.html"><a href="regression.html#regressionnormality"><i class="fa fa-check"></i><b>14.6.3</b> Checking the normality of the residuals</a></li>
<li class="chapter" data-level="14.6.4" data-path="regression.html"><a href="regression.html#regressionhomogeneity"><i class="fa fa-check"></i><b>14.6.4</b> Checking the homoscedasticity of the residuals</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="regression.html"><a href="regression.html#regressionassumptions"><i class="fa fa-check"></i><b>14.7</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>BAYESIAN STATISTICS</b></span></li>
<li class="chapter" data-level="15" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>15</b> Introduction to Bayesian statistics</a>
<ul>
<li class="chapter" data-level="15.1" data-path="bayes.html"><a href="bayes.html#priors-what-you-believed-before"><i class="fa fa-check"></i><b>15.1</b> Priors: what you believed before</a></li>
<li class="chapter" data-level="15.2" data-path="bayes.html"><a href="bayes.html#likelihoods-theories-about-the-data"><i class="fa fa-check"></i><b>15.2</b> Likelihoods: theories about the data</a></li>
<li class="chapter" data-level="15.3" data-path="bayes.html"><a href="bayes.html#the-joint-probability-of-data-and-hypothesis"><i class="fa fa-check"></i><b>15.3</b> The joint probability of data and hypothesis</a></li>
<li class="chapter" data-level="15.4" data-path="bayes.html"><a href="bayes.html#updating-beliefs-using-bayes-rule"><i class="fa fa-check"></i><b>15.4</b> Updating beliefs using Bayes’ rule</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html"><i class="fa fa-check"></i><b>16</b> Bayesian hypothesis tests</a>
<ul>
<li class="chapter" data-level="16.1" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#the-bayes-factor"><i class="fa fa-check"></i><b>16.1</b> The Bayes factor</a></li>
<li class="chapter" data-level="16.2" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#interpreting-bayes-factors"><i class="fa fa-check"></i><b>16.2</b> Interpreting Bayes factors</a></li>
<li class="chapter" data-level="16.3" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#bayesian-statistics-in-cogstat"><i class="fa fa-check"></i><b>16.3</b> Bayesian statistics in CogStat</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#one-sample-t-test"><i class="fa fa-check"></i><b>16.3.1</b> One-sample <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-test</a></li>
<li class="chapter" data-level="16.3.2" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#independent-samples-t-test"><i class="fa fa-check"></i><b>16.3.2</b> Independent samples <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-test</a></li>
<li class="chapter" data-level="16.3.3" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#anova-1"><i class="fa fa-check"></i><b>16.3.3</b> ANOVA</a></li>
<li class="chapter" data-level="16.3.4" data-path="bayesianhypothesistests.html"><a href="bayesianhypothesistests.html#linear-regression"><i class="fa fa-check"></i><b>16.3.4</b> Linear regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="whybayes.html"><a href="whybayes.html"><i class="fa fa-check"></i><b>17</b> Why be a Bayesian?</a>
<ul>
<li class="chapter" data-level="17.1" data-path="whybayes.html"><a href="whybayes.html#evidentiary-standards-you-can-believe"><i class="fa fa-check"></i><b>17.1</b> Evidentiary standards you can believe</a></li>
<li class="chapter" data-level="17.2" data-path="whybayes.html"><a href="whybayes.html#the-p-value-is-a-lie."><i class="fa fa-check"></i><b>17.2</b> The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value is a lie.</a></li>
<li class="chapter" data-level="17.3" data-path="whybayes.html"><a href="whybayes.html#is-it-really-this-bad"><i class="fa fa-check"></i><b>17.3</b> Is it really this bad?</a></li>
</ul></li>
<li class="part"><span><b>APPENDICES</b></span></li>
<li class="chapter" data-level="18" data-path="summaryguide.html"><a href="summaryguide.html"><i class="fa fa-check"></i><b>18</b> Summary guide</a>
<ul>
<li class="chapter" data-level="" data-path="summaryguide.html"><a href="summaryguide.html#descriptive-statistics"><i class="fa fa-check"></i>Descriptive statistics</a></li>
<li class="chapter" data-level="" data-path="summaryguide.html"><a href="summaryguide.html#analysing-differences-with-hyptothesis-testing"><i class="fa fa-check"></i>Analysing differences with hyptothesis testing</a></li>
<li class="chapter" data-level="" data-path="summaryguide.html"><a href="summaryguide.html#analysing-relationship-with-hyptothesis-testing"><i class="fa fa-check"></i>Analysing relationship with hyptothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i>Epilogue</a>
<ul>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#the-undiscovered-statistics"><i class="fa fa-check"></i>The undiscovered statistics</a>
<ul>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#omissions-within-the-topics-covered"><i class="fa fa-check"></i>Omissions within the topics covered</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#statistical-models-missing-from-the-book"><i class="fa fa-check"></i>Statistical models missing from the book</a></li>
<li class="chapter" data-level="" data-path="epilogue.html"><a href="epilogue.html#last-words"><i class="fa fa-check"></i>Last words</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Learning Statistics with CogStat</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hypothesistesting" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Hypothesis testing<a href="hypothesistesting.html#hypothesistesting" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In Chapter <a href="estimation.html#estimation">8</a>, we discussed the ideas behind estimation, which is one of the two “big ideas” in inferential statistics. It’s now time to turn out attention to the other big idea, which is <strong>hypothesis testing</strong>. In its most abstract form, hypothesis testing is really a very simple idea: the researcher has some theory about the world and wants to determine whether or not the data actually support that theory. However, the details are messy, and most people find the theory of hypothesis testing to be the most frustrating part of statistics.</p>
<p>The structure of the chapter is as follows. Firstly, we’ll talk about how hypothesis testing works in a fair amount of detail, using a simple running example to show you how a hypothesis test is “built”. We’ll focus on the underlying logic of the testing procedure rather than being too dogmatic about it. Afterwards, we’ll spend a bit of time talking about the dogmas, rules and heresies surrounding the hypothesis testing theory.</p>
<div id="hypotheses" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> A menagerie of hypotheses<a href="hypothesistesting.html#hypotheses" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Eventually, we all succumb to madness. Let’s suppose that this glorious day has come, and we indulge in a most thoroughly unproductive line of psychological research: the search for extrasensory perception (ESP).<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a> Our first study is a simple one in which we seek to test whether clairvoyance exists. Each participant sits down at a table and is shown a card by an experimenter. The card is black on one side and white on the other. The experimenter takes the card away and places it on a table in an adjacent room. The card is placed black side up or white side up entirely at random, with the randomisation occurring after the experimenter has left the room with the participant. A second experimenter comes in and asks the participant which side of the card is now facing upwards. It’s purely a one-shot experiment. Each person sees only one card and gives only one answer. At no stage is the participant in contact with someone who knows the correct answer.</p>
<p>The data set, therefore, is very simple. We have asked the question of, say, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">N = 100</annotation></semantics></math> people and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>62</mn></mrow><annotation encoding="application/x-tex">X = 62</annotation></semantics></math> of these people have given the correct response. It’s a surprisingly large number, sure, but is it large enough for us to feel safe in claiming we’ve found evidence for ESP? This is the situation where hypothesis testing comes in useful. However, before we talk about how to <em>test</em> hypotheses, we need to be clear about what we mean by hypotheses.</p>
<div id="research-hypotheses-versus-statistical-hypotheses" class="section level3 hasAnchor" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Research hypotheses versus statistical hypotheses<a href="hypothesistesting.html#research-hypotheses-versus-statistical-hypotheses" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first distinction that you need to keep clear in your mind is between research hypotheses and statistical hypotheses. In our ESP study, the overall scientific goal is to demonstrate that clairvoyance exists. In this situation, we have a clear research goal: we are hoping to discover evidence for ESP. In other cases, we might be a lot more neutral than that, so we might say our goal is to determine whether or not clairvoyance exists. Regardless of how we want to portray it, the basic point that we’re trying to convey here is that a research hypothesis involves making a substantive, testable scientific claim. If you are a psychologist, your research hypotheses are fundamentally <em>about</em> psychological constructs. Any of the following would count as <strong>research hypotheses</strong>:</p>
<ul>
<li><em>Listening to music reduces your ability to pay attention to other things.</em> This is a claim about the causal relationship between two psychologically meaningful concepts (listening to music and paying attention to things), so it’s a perfectly reasonable research hypothesis.</li>
<li><em>Intelligence is related to personality</em>. Like the last one, this is a relational claim about two psychological constructs (intelligence and personality), but the claim is weaker: correlational, not causal.</li>
<li><em>Intelligence is the speed of information processing</em>. This hypothesis has quite a different character: it’s not a relational claim at all. It’s an ontological claim about the fundamental character of intelligence. It’s worth expanding on this one actually: It’s usually easier to think about how to construct experiments to test research hypotheses of the form “does X affect Y?” than it is to address claims like “what is X?” And in practice, what usually happens is that you find ways of testing relational claims that follow from your ontological ones. For instance, if we believe that intelligence <em>is</em> the speed of information processing in the brain, our experiments will often involve looking for relationships between measures of intelligence and measures of speed. Consequently, most everyday research questions tend to be relational in nature, but they’re almost always motivated by deeper ontological questions about the state of nature.</li>
</ul>
<p>Notice that in practice, our research hypotheses could overlap a lot. The ultimate goal in the ESP experiment might be to test an ontological claim like “ESP exists”. But we might operationally restrict ourselves to a narrower hypothesis like “Some people can ‘see’ objects in a clairvoyant fashion”. That said, some things really don’t count as proper research hypotheses in any meaningful sense:</p>
<ul>
<li><em>Love is a battlefield</em>. This is too vague to be testable. While it’s okay for a research hypothesis to have a degree of vagueness to it, it has to be possible to operationalise your theoretical ideas. If this cannot be converted into any concrete research design, then this isn’t a scientific research hypothesis: it’s a pop song.</li>
<li><em>The first rule of the tautology club is the first rule of the tautology club</em>. This is not a substantive claim of any kind. It’s true by definition. No conceivable state of nature could possibly be inconsistent with this claim. As such, we say this is an <em>unfalsifiable</em> hypothesis, and as such, it is outside the domain of science. Whatever else you do in science, your claims must have the possibility of being wrong.</li>
<li><em>More people in my experiment will say “yes” than “no”</em>. This one fails as a research hypothesis because it’s a claim about the data set, not about psychology (unless, of course, your actual research question is whether people have some kind of “yes” bias!). As we’ll see shortly, this hypothesis is starting to sound more like a statistical hypothesis than a research hypothesis.</li>
</ul>
<p>As you can see, research hypotheses can be somewhat messy at times; and ultimately, they are <em>scientific</em> claims. <strong>Statistical hypotheses</strong> are neither of these two things. They must be mathematically precise and correspond to specific claims about the characteristics of the “population”. Even so, the intent is that statistical hypotheses clearly relate to the substantive research hypotheses you care about! For instance, in our ESP study, the research hypothesis is that some people are able to see through walls or whatever. We want to “map” this onto a statement about how the data were generated. So let’s think about what that statement would be. The quantity that we’d be interested in within the experiment is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">&quot;correct&quot;</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\mbox{&quot;correct&quot;})</annotation></semantics></math>, the true-but-unknown probability with which the participants in my experiment answer the question correctly. Let’s use the Greek letter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> (theta) to refer to this probability. Here are four different statistical hypotheses:</p>
<ul>
<li>If ESP doesn’t exist and if the experiment is well designed, then the participants are just guessing. So we should expect them to get it right half of the time, and so the statistical hypothesis is that the true probability of choosing correctly is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta = 0.5</annotation></semantics></math>.</li>
<li>Alternatively, suppose ESP does exist, and participants can see the card. If that’s true, people will perform better than chance. The statistical hypothesis would be that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>&gt;</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta &gt; 0.5</annotation></semantics></math>.</li>
<li>A third possibility is that ESP does exist, but the colours are all reversed, and people don’t realise it. If that’s how it works, then you’d expect people’s performance to be <em>below</em> chance. This would correspond to a statistical hypothesis that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>&lt;</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta &lt; 0.5</annotation></semantics></math>.</li>
<li>Finally, suppose ESP exists, but we have no idea whether people are seeing the right colour or the wrong one. In that case, the only claim to be made about the data would be that the probability of making the correct answer is <em>not</em> equal to 50. This corresponds to the statistical hypothesis that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>≠</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta \neq 0.5</annotation></semantics></math>.</li>
</ul>
<p>These are legitimate examples of statistical hypotheses because they are statements about a population parameter and are meaningfully related to my experiment. What this discussion hopefully makes clear is that when attempting to construct a statistical hypothesis test, the researcher has two quite distinct hypotheses to consider. They have a <em>research hypothesis</em> (a claim about psychology) corresponding to a <em>statistical hypothesis</em> (a claim about the data generating population).</p>
<table>
<caption><span id="tab:unnamed-chunk-32">Table 9.1: </span>Research and statistical hypotheses in our ESP research example.</caption>
<thead>
<tr class="header">
<th align="left">Research hypothesis</th>
<th align="left">Statistical hypothesis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ESP exists</td>
<td align="left"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>≠</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta \neq 0.5</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<p>And the critical thing to recognise is this: <em>a statistical hypothesis test is a test of the statistical hypothesis, not the research hypothesis</em>. If your study is poorly designed, the link between your research hypothesis and your statistical hypothesis is broken. To give a silly example, suppose that the ESP study was conducted in a situation where the participant can actually see the card reflected in a window; if that happens, we would be able to find robust evidence that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>≠</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta \neq 0.5</annotation></semantics></math>, but this would tell us nothing about whether “ESP exists”.</p>
</div>
<div id="null-hypotheses-and-alternative-hypotheses" class="section level3 hasAnchor" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> Null hypotheses and alternative hypotheses<a href="hypothesistesting.html#null-hypotheses-and-alternative-hypotheses" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have a research hypothesis that corresponds to the question we want to ask about the world, and we can map it onto a statistical hypothesis. Our statistical hypothesis will have a claim. This claim will become our “alternative” hypothesis, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>1</mn></msub><annotation encoding="application/x-tex">H_1</annotation></semantics></math>. In constrast, the “null” hypothesis, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math>, will correspond to the exact opposite of what the claim was. Then, we’ll focus exclusively on the null hypothesis.</p>
<p>In our ESP example, the null hypothesis is that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta = 0.5</annotation></semantics></math>, since that’s what we’d expect if ESP <em>didn’t</em> exist. The hope, of course, is that ESP is totally real, and so the <em>alternative</em> to this null hypothesis is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>≠</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta \neq 0.5</annotation></semantics></math>. In essence, what we’re doing here is dividing up the possible values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> into two groups: those values that we really hope aren’t true (the null) and those values that we’d be happy with if they turn out to be correct (the alternative). Having done so, the important thing to recognise is that the goal of a hypothesis test is <em>not</em> to show that the alternative hypothesis is (probably) true; the goal is to show that the null hypothesis is (probably) false. Most people find this pretty weird.</p>
<p>According to Danielle, the best way to think about it is to imagine that a hypothesis test is a criminal trial: <em>the trial of the null hypothesis</em>. The null hypothesis is the defendant, the researcher is the prosecutor, and the statistical test itself is the judge. Just like a criminal trial, there is a presumption of innocence: the null hypothesis is <em>deemed</em> to be true unless you, the researcher, can prove beyond a reasonable doubt that it is false. You are free to design your experiment however you like (within reason, obviously!), and your goal when doing so is to maximise the chance that the data will yield a conviction for the crime of being false. The catch is that the statistical test sets the rules of the trial, which are designed to protect the null hypothesis – specifically to ensure that if the null hypothesis is true, the chances of a false conviction are guaranteed to be low. This is pretty important: after all, the null hypothesis doesn’t get a lawyer. And given that the researcher is trying desperately to prove it to be false, <em>someone</em> has to protect it.</p>
</div>
</div>
<div id="errortypes" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Two types of errors<a href="hypothesistesting.html#errortypes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ideally, we would like to construct our test so that we never make any errors. Unfortunately, this is never possible. Sometimes you’re just really unlucky: for instance, suppose you flip a coin 10 times in a row and it comes up heads all 10 times. That feels like powerful evidence that the coin is biased (and it is!), but of course, there’s a 1 in 1024 chance that this would happen even if the coin was totally fair. In other words, in real life, we <em>always</em> have to accept that there’s a chance that we did the wrong thing. Consequently, statistical hypothesis testing aims not to <em>eliminate</em> errors but to <em>minimise</em> them.</p>
<p>At this point, we need to be more precise about what we mean by “errors”. Firstly, let’s state the obvious: it is either the case that the null hypothesis is true, or it is false. And our test will either reject the null hypothesis or retain it. So, as the table below illustrates, after we run the test and make our choice, one of four things might have happened:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Retain <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math></th>
<th align="left">Reject <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math> is true</td>
<td align="left">Correct decision</td>
<td align="left">Error (type I)</td>
</tr>
<tr class="even">
<td align="left"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math> is false</td>
<td align="left">Error (type II)</td>
<td align="left">Correct decision</td>
</tr>
</tbody>
</table>
<p>As a consequence, there are actually <em>two</em> different types of error here. If we reject a null hypothesis that is actually true, then we have made a <strong>type I error</strong>. On the other hand, if we retain the null hypothesis when it is, in fact, false, then we have made a <strong>type II error</strong>.</p>
<p>A criminal trial requires that you establish “beyond a reasonable doubt” that the defendant did it. The trial is designed to protect the rights of a defendant. In other words, a criminal trial doesn’t treat the two types of error in the same way: punishing the innocent is deemed to be much worse than letting the guilty go free. A statistical test is pretty much the same: the single most important design principle of the test is to <em>control</em> the probability of a type I error to keep it below some fixed probability. This probability, which is denoted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>, is called the <strong>significance level</strong> of the test (or sometimes, the <em>size</em> of the test). A hypothesis test is said to have a significance level <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> if the type I error rate is no larger than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.</p>
<p>So, what about the type II error rate? Well, we’d also like to keep those under control too, and we denote this probability by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>. However, it’s much more common to refer to the <strong>power</strong> of the test, which is the probability with which we reject a null hypothesis when it is false, which is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">1-\beta</annotation></semantics></math>. To help keep this straight, here’s the same table again, but with the relevant numbers added:</p>
<table>
<colgroup>
<col width="16%" />
<col width="50%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Retain <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math></th>
<th align="left">Reject <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math> is true</td>
<td align="left"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">1-\alpha</annotation></semantics></math> (probability of correct retention)</td>
<td align="left"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> (type I error rate)</td>
</tr>
<tr class="even">
<td align="left"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math> is false</td>
<td align="left"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> (type II error rate)</td>
<td align="left"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">1-\beta</annotation></semantics></math> (power of the test)</td>
</tr>
</tbody>
</table>
<p>A “powerful” hypothesis test has a small value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> while still keeping <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> fixed at some (small) desired level. By convention, scientists make use of three different <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> levels: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>.05</mn><annotation encoding="application/x-tex">.05</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>.01</mn><annotation encoding="application/x-tex">.01</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>.001</mn><annotation encoding="application/x-tex">.001</annotation></semantics></math>. Notice the asymmetry here: the tests are designed to <em>ensure</em> that the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> level is kept small, but there’s no corresponding guarantee regarding <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>. We’d certainly <em>like</em> the type II error rate to be small, and we try to design tests that keep it small, but this is very much secondary to the overwhelming need to control the type I error rate. Paraphrasing Blackstone had he been a statistician: it is “better to retain ten false null hypotheses than to reject a single true one”.</p>
</div>
<div id="teststatistics" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Test statistics and sampling distributions<a href="hypothesistesting.html#teststatistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>At this point, we need to start talking specifics about how to construct a hypothesis test. To that end, let’s return to the ESP example. Let’s ignore the actual data we obtained, for the moment, and think about the structure of the experiment. Regardless of the actual numbers, the <em>form</em> of the data is that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> out of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> people correctly identified the colour of the hidden card. Moreover, let’s suppose for the moment that the null hypothesis really is true: ESP doesn’t exist, and the true probability that anyone picks the correct colour is exactly <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta = 0.5</annotation></semantics></math>. What would we <em>expect</em> the data to look like? Well, obviously, we’d expect the proportion of people who make the correct response to be pretty close to 50%. Or, to phrase this in more mathematical terms, we’d say that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>/</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">X/N</annotation></semantics></math> is approximately <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0.5</mn><annotation encoding="application/x-tex">0.5</annotation></semantics></math>. Of course, we wouldn’t expect this fraction to be <em>exactly</em> 0.5: if, for example, we tested <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">N=100</annotation></semantics></math> people, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>53</mn></mrow><annotation encoding="application/x-tex">X = 53</annotation></semantics></math> of them got the question right, we’d probably be forced to concede that the data are quite consistent with the null hypothesis. On the other hand, if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>99</mn></mrow><annotation encoding="application/x-tex">X = 99</annotation></semantics></math> of our participants got the question right, then we’d feel pretty confident that the null hypothesis is wrong. Similarly, if only <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">X=3</annotation></semantics></math> people got the answer right, we’d be similarly confident that the null was wrong. Let’s be a little more technical about this: we have a quantity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> that we can calculate by looking at our data; after looking at the value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>, we decide whether to believe that the null hypothesis is correct or to reject the null hypothesis in favour of the alternative. The name for this thing that we calculate to guide our choices is a <strong>test statistic</strong>.</p>
<p>Having chosen a test statistic, now the next step is to state precisely which values of the test statistic would cause us to reject the null hypothesis and which values would cause us to keep it. To do so, we need to determine what the <strong>sampling distribution of the test statistic</strong> would be if the null hypothesis were true (we discussed sampling distributions earlier in Chapter <a href="estimation.html#samplingdists">8.3.1</a>). Why do we need this? Because this distribution tells us precisely what values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>, our null hypothesis would lead us to expect. And therefore, we can use this distribution to assess how closely the null hypothesis agrees with our data.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:samplingdist"></span>
<img src="lsc_files/figure-html/samplingdist-1.svg" alt="The sampling distribution for our test statistic $X$ when the null hypothesis is true. For our ESP scenario, this is a binomial distribution. Not surprisingly, since the null hypothesis says that the probability of a correct response is $\theta = .5$, the sampling distribution says that the most likely value is 50 (out of 100) correct responses. Most of the probability mass lies between 40 and 60." width="672" />
<p class="caption">
Figure 9.1: The sampling distribution for our test statistic <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> when the null hypothesis is true. For our ESP scenario, this is a binomial distribution. Not surprisingly, since the null hypothesis says that the probability of a correct response is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">\theta = .5</annotation></semantics></math>, the sampling distribution says that the most likely value is 50 (out of 100) correct responses. Most of the probability mass lies between 40 and 60.
</p>
</div>
<p>How do we determine the sampling distribution of the test statistic? Fortunately, our ESP example provides us with one of the most uncomplicated cases. Our population parameter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> is just the overall probability that people respond correctly when asked the question, and our test statistic <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> is the <em>count</em> of the number of people who did so out of a sample size of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>. We’ve seen a distribution like this in Chapter <a href="probability.html#binomial">7.4.1</a>: that’s exactly what the binomial distribution describes! So, to use the notation and terminology introduced in that section, we would say that the null hypothesis predicts that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> is binomially distributed, which is written
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>∼</mo><mtext mathvariant="normal">Binomial</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo>,</mo><mi>N</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
X \sim \mbox{Binomial}(\theta,N)
</annotation></semantics></math></p>
<p>Since the null hypothesis states that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta = 0.5</annotation></semantics></math> and our experiment has <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">N=100</annotation></semantics></math> people, we have the sampling distribution we need. This sampling distribution is plotted in Figure <a href="hypothesistesting.html#fig:samplingdist">9.1</a>. No surprises, really: the null hypothesis says that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>50</mn></mrow><annotation encoding="application/x-tex">X=50</annotation></semantics></math> is the most likely outcome, and it says that we’re almost certain to see somewhere between 40 and 60 correct responses.</p>
</div>
<div id="decisionmaking" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Making decisions<a href="hypothesistesting.html#decisionmaking" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’ve constructed a test statistic (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>), and we chose it so that we’re pretty confident that if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> is close to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">N/2</annotation></semantics></math>, then we should retain the null, and if not, we should reject it. The question remains: exactly which values of the test statistic should we associate with the null hypothesis, and which values go with the alternative hypothesis? In the ESP study, for example, we’ve observed a value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>62</mn></mrow><annotation encoding="application/x-tex">X=62</annotation></semantics></math>. What decision should we make? Should we choose to believe the null hypothesis or the alternative hypothesis?</p>
<div id="critical-regions-and-critical-values" class="section level3 hasAnchor" number="9.4.1">
<h3><span class="header-section-number">9.4.1</span> Critical regions and critical values<a href="hypothesistesting.html#critical-regions-and-critical-values" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To answer this question, we need to introduce the concept of a <strong>critical region</strong> for the test statistic <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>. The critical region of the test corresponds to those values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> that would lead us to reject the null hypothesis (which is why the critical region is also sometimes called the rejection region). How do we find this critical region? Well, let’s consider what we know:</p>
<ul>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> should be very big or very small to reject the null hypothesis.</li>
<li>If the null hypothesis is true, the sampling distribution of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> is Binomial<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.5</mn><mo>,</mo><mi>N</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0.5, N)</annotation></semantics></math>.</li>
<li>If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha =.05</annotation></semantics></math>, the critical region must cover 5% of this sampling distribution.</li>
</ul>
<p>You must understand this last point: the critical region corresponds to those values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> for which we would reject the null hypothesis, and the sampling distribution in question describes the probability that we would obtain a particular value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> if the null hypothesis were actually true.</p>
<p>Now, let’s suppose that we chose a critical region that covers 20% of the sampling distribution, and assume that the null hypothesis is actually true. What would be the probability of incorrectly rejecting the null? The answer is, of course, 20%. And therefore, we would have built a test that had an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> level of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0.2</mn><annotation encoding="application/x-tex">0.2</annotation></semantics></math>. If we want <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha = .05</annotation></semantics></math>, the critical region is only <em>allowed</em> to cover 5% of the sampling distribution of our test statistic.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:crit2"></span>
<img src="lsc_files/figure-html/crit2-1.svg" alt="The critical region associated with the hypothesis test for the ESP study, for a hypothesis test with a significance level of α=.05\alpha = .05. The plot shows the sampling distribution of XX under the null hypothesis: the grey bars correspond to those values of XX, for which we would retain the null hypothesis. The black bars show the critical region: those values of XX for which we would reject the null. Because the alternative hypothesis is two-sided (i.e. allows both θ&lt;.5\theta &lt;.5 and θ&gt;.5\theta &gt;.5), the critical region covers both tails of the distribution. To ensure an α\alpha level of .05.05, we need to ensure that each of the two regions encompasses 2.5% of the sampling distribution." width="672" />
<p class="caption">
Figure 9.2: The critical region associated with the hypothesis test for the ESP study, for a hypothesis test with a significance level of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha = .05</annotation></semantics></math>. The plot shows the sampling distribution of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> under the null hypothesis: the grey bars correspond to those values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>, for which we would retain the null hypothesis. The black bars show the critical region: those values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> for which we would reject the null. Because the alternative hypothesis is two-sided (i.e. allows both <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>&lt;</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">\theta &lt;.5</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>&gt;</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">\theta &gt;.5</annotation></semantics></math>), the critical region covers both tails of the distribution. To ensure an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> level of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>.05</mn><annotation encoding="application/x-tex">.05</annotation></semantics></math>, we need to ensure that each of the two regions encompasses 2.5% of the sampling distribution.
</p>
</div>
<p>As it turns out, those three things uniquely solve the problem: our critical region consists of the most <em>extreme values</em>, known as the <strong>tails</strong> of the distribution (illustrated in Figure <a href="hypothesistesting.html#fig:crit2">9.2</a>). As it turns out, if we want <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha = .05</annotation></semantics></math>, then our critical regions correspond to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>≤</mo><mn>40</mn></mrow><annotation encoding="application/x-tex">X \leq 40</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>≥</mo><mn>60</mn></mrow><annotation encoding="application/x-tex">X \geq 60</annotation></semantics></math>.<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a> If the number of people saying “true” is between 41 and 59, we should retain the null hypothesis. We should reject the null hypothesis if the number is between 0 to 40 or between 60 to 100. The numbers 40 and 60 are often referred to as the <strong>critical values</strong> since they define the edges of the critical region.</p>
<p>At this point, our hypothesis test is essentially complete:
- (1) we choose an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> level (e.g. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha = .05</annotation></semantics></math>),
- (2) we come up with some test statistic (e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>) that does a good job (in some meaningful sense) of comparing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>1</mn></msub><annotation encoding="application/x-tex">H_1</annotation></semantics></math>,
- (3) we figure out the sampling distribution of the test statistic on the assumption that the null hypothesis is true (in this case, binomial) and then
- (4) we calculate the critical region that produces an appropriate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> level (0-40 and 60-100).</p>
<p>Now, we have to calculate the value of the test statistic for the real data (e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>62</mn></mrow><annotation encoding="application/x-tex">X = 62</annotation></semantics></math>) and then compare it to the critical values to make our decision. Since 62 is greater than the critical value of 60, we would reject the null hypothesis. Or, to phrase it slightly differently, we say that the test has produced a <strong>significant</strong> result.</p>
</div>
<div id="a-note-on-statistical-significance" class="section level3 hasAnchor" number="9.4.2">
<h3><span class="header-section-number">9.4.2</span> A note on statistical “significance”<a href="hypothesistesting.html#a-note-on-statistical-significance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A very brief digression is in order regarding the word “significant”. It is a misnomer. The concept of statistical significance is very simple, but has a very unfortunate name. If the data allows us to reject the null hypothesis, we say that “the result is <em>statistically significant</em>”, often shortened to “the result is significant”. This terminology dates back to a time when “significant” just meant something like “indicated” rather than its modern meaning, which is much closer to “important”. As a result, many modern readers get very confused when they start learning statistics because they think a “significant result” must be an important one. It doesn’t mean that at all. All that “statistically significant” means is that the data allowed us to reject a null hypothesis. Whether or not the result is actually important in the real world is a very different question and depends on all sorts of other things.</p>
</div>
<div id="onesidedtests" class="section level3 hasAnchor" number="9.4.3">
<h3><span class="header-section-number">9.4.3</span> The difference between one sided and two sided tests<a href="hypothesistesting.html#onesidedtests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There’s one more thing to point out about the constructed hypothesis test. Let us take a moment to think about the statistical hypotheses:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>θ</mi><mo>=</mo><mn>.5</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>H</mi><mn>1</mn></msub><mo>:</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>θ</mi><mo>≠</mo><mn>.5</mn></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{array}{cc}
H_0 : &amp; \theta = .5 \\
H_1 : &amp; \theta \neq .5 
\end{array}
</annotation></semantics></math></p>
<p>We notice that the alternative hypothesis covers <em>both</em> the possibility that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>&lt;</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">\theta &lt; .5</annotation></semantics></math> and the possibility that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>&gt;</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">\theta &gt; .5</annotation></semantics></math>. This makes sense if we think ESP could produce better-than-chance performance <em>or</em> worse-than-chance performance. This is an example of a <strong>two-sided test</strong> in statistical language. It’s called this because the alternative hypothesis covers the area on both “sides” of the null hypothesis. As a consequence, the critical region of the test covers both tails of the sampling distribution (2.5% on either side provided that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha =.05</annotation></semantics></math>), as illustrated earlier in Figure <a href="hypothesistesting.html#fig:crit2">9.2</a>.</p>
<p>However, that’s not the only possibility. For example, it might be the case that we’re only willing to believe in ESP if it produces better than chance performance. If so, then the alternative hypothesis would only cover the possibility that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>&gt;</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">\theta &gt; .5</annotation></semantics></math>, and as a consequence, the null hypothesis now becomes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>≤</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">\theta \leq .5</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>θ</mi><mo>≤</mo><mn>.5</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>H</mi><mn>1</mn></msub><mo>:</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>θ</mi><mo>&gt;</mo><mn>.5</mn></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{array}{cc}
H_0 : &amp; \theta \leq .5 \\
H_1 : &amp; \theta &gt; .5 
\end{array}
</annotation></semantics></math></p>
<p>When this happens, we have what’s called a <strong>one-sided test</strong>, and when this happens, the critical region only covers one tail of the sampling distribution. This is illustrated in Figure <a href="hypothesistesting.html#fig:crit1">9.3</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:crit1"></span>
<img src="lsc_files/figure-html/crit1-1.svg" alt="The critical region for a one sided test. In this case, the alternative hypothesis is that θ&gt;.05\theta &gt; .05, so we would only reject the null hypothesis for large values of XX. As a consequence, the critical region only covers the upper tail of the sampling distribution; specifically the upper 5% of the distribution. Contrast this to the two-sided version earlier)" width="672" />
<p class="caption">
Figure 9.3: The critical region for a one sided test. In this case, the alternative hypothesis is that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>&gt;</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\theta &gt; .05</annotation></semantics></math>, so we would only reject the null hypothesis for large values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>. As a consequence, the critical region only covers the upper tail of the sampling distribution; specifically the upper 5% of the distribution. Contrast this to the two-sided version earlier)
</p>
</div>
</div>
</div>
<div id="pvalue" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value of a test<a href="hypothesistesting.html#pvalue" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In one sense, our hypothesis test is complete; we’ve constructed a test statistic, figured out its sampling distribution if the null hypothesis is true, and then constructed the critical region for the test. Nevertheless, we’ve actually omitted the most important number of all: <strong>the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value</strong>. It is to this topic that we now turn.</p>
<p>There are two somewhat different ways of interpreting a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value, one proposed by Sir Ronald Fisher and the other by Jerzy Neyman. Both versions are legitimate, though they reflect very different ways of thinking about hypothesis tests. Most introductory textbooks only give Fisher’s version, but that’s a bit of a shame. Danielle is believes Neyman’s version is cleaner and that it better reflects the logic of the null hypothesis test. You might disagree, though, so both are included. We’ll start with Neyman’s version.</p>
<div id="a-softer-view-of-decision-making" class="section level3 hasAnchor" number="9.5.1">
<h3><span class="header-section-number">9.5.1</span> A softer view of decision making<a href="hypothesistesting.html#a-softer-view-of-decision-making" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One problem with the hypothesis testing procedure is that it makes no distinction between a “barely significant” and a “highly significant” result. For instance, in the ESP study, the data only just fell inside the critical region - so we did get a significant effect, but it was a pretty near thing. In contrast, suppose we run a study in which <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>97</mn></mrow><annotation encoding="application/x-tex">X=97</annotation></semantics></math> out of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">N=100</annotation></semantics></math> participants got the answer right. This would obviously be significant, too, but by a much larger margin. There is no real ambiguity about this at all. The procedure makes no distinction between the two. If we adopt the standard convention of allowing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha = .05</annotation></semantics></math> as an acceptable Type I error rate, then both are significant results.</p>
<p>This is where the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value comes in handy. To understand how it works, let’s suppose that we ran many hypothesis tests on the same data set: but with a different value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> in each case. When we do that for our ESP data, what we’d get is something like this:</p>
<table>
<thead>
<tr class="header">
<th align="left">Value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></th>
<th align="left">Reject the null?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0.05</td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">0.04</td>
<td align="left">Yes</td>
</tr>
<tr class="odd">
<td align="left">0.03</td>
<td align="left">Yes</td>
</tr>
<tr class="even">
<td align="left">0.02</td>
<td align="left">No</td>
</tr>
<tr class="odd">
<td align="left">0.01</td>
<td align="left">No</td>
</tr>
</tbody>
</table>
<p>When we test ESP data (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>62</mn></mrow><annotation encoding="application/x-tex">X=62</annotation></semantics></math> successes out of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">N=100</annotation></semantics></math> observations) using <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> levels of .03 and above, we always reject the null hypothesis. We always retain the null hypothesis for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> levels of .02 and below. Therefore, somewhere between .02 and .03, there must be the smallest value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> that would allow us to reject the null hypothesis for this data. This is the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value; as it turns out the ESP data has <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>.021</mn></mrow><annotation encoding="application/x-tex">p = .021</annotation></semantics></math>. In short:</p>
<blockquote>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> is defined to be the smallest Type I error rate (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>) that you have to be willing to tolerate if you want to reject the null hypothesis.</p>
</blockquote>
<p>If it turns out that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> describes an error rate that you find intolerable, then you must retain the null. If you’re comfortable with an error rate equal to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, then it’s okay to reject the null hypothesis in favour of your preferred alternative.</p>
<p>In effect, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> is a summary of all the possible hypothesis tests you could have run, taken across all possible <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> values. And as a consequence, it has the effect of “softening” our decision process. For those tests in which <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>≤</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">p \leq \alpha</annotation></semantics></math>, you would have rejected the null hypothesis, whereas for those tests in which <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">p &gt; \alpha</annotation></semantics></math>, you would have retained the null.</p>
<p>In our ESP study, we obtained <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>62</mn></mrow><annotation encoding="application/x-tex">X=62</annotation></semantics></math>, and as a consequence we’ve ended up with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>.021</mn></mrow><annotation encoding="application/x-tex">p = .021</annotation></semantics></math>. So the error rate we have to tolerate is 2.1%. In contrast, suppose the experiment had yielded <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mn>97</mn></mrow><annotation encoding="application/x-tex">X=97</annotation></semantics></math>. What happens to the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value now? This time it’s shrunk to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1.36</mn><mo>×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>25</mn></mrow></msup></mrow><annotation encoding="application/x-tex">p = 1.36 \times 10^{-25}</annotation></semantics></math>, which is a tiny, tiny<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a> Type I error rate. For this second case, we would be able to reject the null hypothesis with a lot more confidence because we only have to be “willing” to tolerate a type I error rate of about 1 in 10 trillion trillion to justify our decision to reject.</p>
</div>
<div id="the-probability-of-extreme-data" class="section level3 hasAnchor" number="9.5.2">
<h3><span class="header-section-number">9.5.2</span> The probability of extreme data<a href="hypothesistesting.html#the-probability-of-extreme-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The second definition of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value comes from Sir Ronald Fisher, which you tend to see in most introductory statistics textbooks. Notice how, when we constructed the critical region, it corresponded to the <em>tails</em> (i.e. extreme values) of the sampling distribution? That’s not a coincidence: almost all “good” tests have this characteristic (good in minimising our type II error rate, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>). The reason for that is that a good critical region almost always corresponds to those values of the test statistic that are least likely to be observed if the null hypothesis is true. If this rule is true, then we can define the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value as the probability that we would have observed a test statistic that is at least as extreme as the one we actually did get.</p>
<p>In other words, if the data are extremely implausible according to the null hypothesis, then the null hypothesis is probably wrong.</p>
</div>
<div id="a-common-mistake" class="section level3 hasAnchor" number="9.5.3">
<h3><span class="header-section-number">9.5.3</span> A common mistake<a href="hypothesistesting.html#a-common-mistake" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You can see that there are two somewhat different but legitimate ways to interpret the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value, one based on Neyman’s approach to hypothesis testing and the other based on Fisher’s. Unfortunately, there is a third explanation that people sometimes give, especially when they’re first learning statistics, and it is <em>absolutely and completely wrong</em>. This mistaken approach is to refer to the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value as “the probability that the null hypothesis is true”. It’s an intuitively appealing way to think, but it’s wrong in two key respects:</p>
<ol style="list-style-type: decimal">
<li>null hypothesis testing is a frequentist tool, and the frequentist approach to probability does <em>not</em> allow you to assign probabilities to the null hypothesis. According to this view of probability, the null hypothesis is either true or not, but it cannot have a “5% chance” of being true.</li>
<li>even within the Bayesian approach, which does let you assign probabilities to hypotheses, the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value would not correspond to the probability that the null is true; this interpretation is entirely inconsistent with the mathematics of how the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value is calculated.</li>
</ol>
<p>Put bluntly, despite the intuitive appeal of thinking this way, there is <em>no</em> justification for interpreting a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value this way. Never do it.</p>
</div>
</div>
<div id="writeup" class="section level2 hasAnchor" number="9.6">
<h2><span class="header-section-number">9.6</span> Reporting the results of a hypothesis test<a href="hypothesistesting.html#writeup" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When writing up the results of a hypothesis test, there are usually several pieces of information that you need to report, but it varies a fair bit from test to test. In the chapters discussing each statistical tool, we’ll spend a little time talking about how to report the results correctly. However, regardless of what test you’re doing, the one thing that you always have to do is say something about the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value and whether or not the outcome was significant.</p>
<p>The fact that you have to do this is unsurprising: it’s the whole point of doing the test. It might be surprising, though, that there is some contention over exactly how you’re supposed to do it. Leaving aside those people who completely disagree with the entire framework underpinning null hypothesis testing, there’s a certain amount of tension that exists regarding whether or not to report the exact <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value that you obtained, or if you should state only that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">p &lt; \alpha</annotation></semantics></math> for a significance level that you chose in advance (e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">p&lt;.05</annotation></semantics></math>).</p>
<div id="the-issue" class="section level3 hasAnchor" number="9.6.1">
<h3><span class="header-section-number">9.6.1</span> The issue<a href="hypothesistesting.html#the-issue" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To see why this is an issue, the key thing to recognise is that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> values are <em>terribly</em> convenient. In practice, the fact that we can compute a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value means that we don’t have to specify any <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> level at all to run the test. Instead, what you can do is calculate your <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value and interpret it directly: if you get <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>.062</mn></mrow><annotation encoding="application/x-tex">p = .062</annotation></semantics></math>, then it means that you’d have to be willing to tolerate a Type I error rate of 6.2% to justify rejecting the null. If you personally find 6.2% intolerable, then you retain the null.</p>
<p>Therefore, the argument goes, why don’t we just report the actual <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value and let the reader make up their own minds about what an acceptable Type I error rate is? This approach has the big advantage of “softening” the decision-making process – in fact, if you accept the Neyman definition of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value, that’s the whole point of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value. We no longer have a fixed significance level of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha = .05</annotation></semantics></math> as a bright line separating “accept” from “reject” decisions; and this removes the rather pathological problem of being forced to treat <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>.051</mn></mrow><annotation encoding="application/x-tex">p = .051</annotation></semantics></math> in a fundamentally different way to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>.049</mn></mrow><annotation encoding="application/x-tex">p = .049</annotation></semantics></math>.</p>
<p>This flexibility is both the advantage and the disadvantage to the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value. Many people don’t like the idea of reporting an exact <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value because it gives the researcher a bit <em>too much</em> freedom. In particular, it lets you change your mind about what error tolerance you’re willing to put up with <em>after</em> you look at the data.</p>
<p>For instance, consider the ESP experiment. Suppose we ran the test and ended up with a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value of .09. Should we accept or reject? Now, we haven’t yet bothered to think about what level of Type I error we’re “really” willing to accept. But we <em>do</em> have an opinion about whether or not ESP exists. Regardless, we could always decide that a 9% error rate isn’t so bad, especially compared to how annoying it would be to admit to the world that the experiment has failed. So, to avoid looking like we just made it up after the fact, we now say that our <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> is .1: a 10% type I error rate isn’t too bad, and at that level, our test is significant! We win.</p>
<p>In other words, the worry here is that we might have the best of intentions and be the most honest of people, but the temptation to just “shade” things a little bit here and there is really, really strong. Anyone who has ever run an experiment can attest that it’s a long and difficult process, and you often get <em>very</em> attached to your hypotheses. It’s hard to let go and admit the experiment didn’t find what you wanted it to find. And that’s the danger here. If we use the “raw” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value, people will start interpreting the data in terms of what they <em>want</em> to believe, not what the data are actually saying. And if we allow that, well, why are we bothering to do science at all? Why not let everyone believe whatever they like about anything, regardless of what the facts are? Okay, that’s a bit extreme, but that’s where the worry comes from. According to this view, you really <em>must</em> specify your <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> value in advance and then only report whether the test was significant or not. It’s the only way to keep ourselves honest.</p>
</div>
<div id="two-proposed-solutions" class="section level3 hasAnchor" number="9.6.2">
<h3><span class="header-section-number">9.6.2</span> Two proposed solutions<a href="hypothesistesting.html#two-proposed-solutions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In practice, it’s pretty rare for a researcher to specify a single <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> level ahead of time. Instead, the convention is that scientists rely on three standard significance levels: .05, .01 and .001. When reporting your results, you indicate which (if any) of these significance levels allow you to reject the null hypothesis. This is summarised in Table <a href="hypothesistesting.html#tab:pvaltable">9.2</a>. This allows us to soften the decision rule a little bit since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>.01</mn></mrow><annotation encoding="application/x-tex">p&lt;.01</annotation></semantics></math> implies that the data meet a stronger evidentiary standard than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">p&lt;.05</annotation></semantics></math> would. Nevertheless, since these levels are fixed in advance by convention, it does prevent people from choosing their <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> level after looking at the data.</p>
<table>
<caption><span id="tab:pvaltable">Table 9.2: </span>A commonly adopted convention for reporting <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> values: in many places it is conventional to report one of four different things (e.g., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">p&lt;.05</annotation></semantics></math>) as shown below. A “significance stars” notation (i.e., a * indicates <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">p&lt;.05</annotation></semantics></math>) is sometimes produced by statistical software. It’s also worth noting that some people will write <em>n.s.</em> (not significant) rather than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">p&gt;.05</annotation></semantics></math>.</caption>
<colgroup>
<col width="11%" />
<col width="10%" />
<col width="67%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Usual notation</th>
<th align="left">Signif. stars</th>
<th align="left">Meaning</th>
<th align="left">The null is…</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">p&gt;.05</annotation></semantics></math></td>
<td align="left"></td>
<td align="left">The test wasn’t significant</td>
<td align="left">Retained</td>
</tr>
<tr class="even">
<td align="left"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">p&lt;.05</annotation></semantics></math></td>
<td align="left">*</td>
<td align="left">The test was significant at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha = .05</annotation></semantics></math> but not at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.01</mn></mrow><annotation encoding="application/x-tex">\alpha =.01</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.001</mn></mrow><annotation encoding="application/x-tex">\alpha = .001</annotation></semantics></math>.</td>
<td align="left">Rejected</td>
</tr>
<tr class="odd">
<td align="left"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>.01</mn></mrow><annotation encoding="application/x-tex">p&lt;.01</annotation></semantics></math></td>
<td align="left">**</td>
<td align="left">The test was significant at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha = .05</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.01</mn></mrow><annotation encoding="application/x-tex">\alpha = .01</annotation></semantics></math> but not at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.001</mn></mrow><annotation encoding="application/x-tex">\alpha = .001</annotation></semantics></math></td>
<td align="left">Rejected</td>
</tr>
<tr class="even">
<td align="left"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>.001</mn></mrow><annotation encoding="application/x-tex">p&lt;.001</annotation></semantics></math></td>
<td align="left">***</td>
<td align="left">The test was significant at all levels</td>
<td align="left">Rejected</td>
</tr>
</tbody>
</table>
<p>Nevertheless, many people still prefer to report exact <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> values. To many people, the advantage of allowing the reader to make up their own mind about how to interpret <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>.06</mn></mrow><annotation encoding="application/x-tex">p = .06</annotation></semantics></math> outweighs any disadvantages. In practice, however, even among those researchers who prefer exact <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> values, it is quite common to just write <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>.001</mn></mrow><annotation encoding="application/x-tex">p&lt;.001</annotation></semantics></math> instead of reporting an exact value for small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>. This is in part because a lot of software doesn’t actually print out the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value when it’s that small (e.g., SPSS just writes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>.000</mn></mrow><annotation encoding="application/x-tex">p = .000</annotation></semantics></math> whenever <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>.001</mn></mrow><annotation encoding="application/x-tex">p&lt;.001</annotation></semantics></math>), and in part because a very small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value can be kind of misleading. The human mind sees a number like .0000000001, and it’s hard to suppress the gut feeling that the evidence in favour of the alternative hypothesis is a near certainty. In practice, however, this is usually wrong. Every statistical test ever invented relies on simplifications, approximations and assumptions. As a consequence, it’s probably not reasonable to walk away from <em>any</em> statistical analysis with a feeling of confidence stronger than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>.001</mn></mrow><annotation encoding="application/x-tex">p&lt;.001</annotation></semantics></math> implies. In other words, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>.001</mn></mrow><annotation encoding="application/x-tex">p&lt;.001</annotation></semantics></math> is really code for “as far as <em>this test</em> is concerned, the evidence is overwhelming.”</p>
<p>In light of all this, you might wonder what exactly you should do. There’s a fair bit of contradictory advice on the topic, with some people arguing that you should report the exact <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value and other people arguing that you should use the tiered approach illustrated in Table <a href="hypothesistesting.html#tab:pvaltable">9.2</a>. As a result, the best advice is that you look at papers/reports written in your field and see what the convention seems to be. If there doesn’t seem to be any consistent pattern, then use whichever method you prefer.</p>
<p>For any hypothesis test that CogStat will run for you, whether that be a t-test, ANOVA, regression, etc., the output will include a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value. If you want to report the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value, you can just copy and paste it from the output.</p>
</div>
</div>
<div id="effectsize" class="section level2 hasAnchor" number="9.7">
<h2><span class="header-section-number">9.7</span> Effect size, sample size and power<a href="hypothesistesting.html#effectsize" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In previous sections, we emphasised the fact that the major design principle behind statistical hypothesis testing is that we try to control our Type I error rate. When we fix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha = .05</annotation></semantics></math>, we are attempting to ensure that only 5% of true null hypotheses are incorrectly rejected. However, this doesn’t mean that we don’t care about Type II errors. In fact, from the researcher’s perspective, the error of failing to reject the null when it is actually false is an extremely annoying one. With that in mind, a secondary goal of hypothesis testing is to try to minimise <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>, the Type II error rate, although we don’t usually <em>talk</em> in terms of minimising Type II errors. Instead, we talk about maximising the <em>power</em> of the test. Since power is defined as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">1-\beta</annotation></semantics></math>, this is the same thing.</p>
<div id="the-power-function" class="section level3 hasAnchor" number="9.7.1">
<h3><span class="header-section-number">9.7.1</span> The power function<a href="hypothesistesting.html#the-power-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:crit3"></span>
<img src="lsc_files/figure-html/crit3-1.svg" alt="Sampling distribution under the alternative hypothesis, for a population parameter value of θ=0.55θ = 0.55. A reasonable proportion of the distribution lies in the rejection region." width="672" />
<p class="caption">
Figure 9.4: Sampling distribution under the <em>alternative</em> hypothesis, for a population parameter value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0.55</mn></mrow><annotation encoding="application/x-tex">θ = 0.55</annotation></semantics></math>. A reasonable proportion of the distribution lies in the rejection region.
</p>
</div>
<p>Let’s take a moment to think about what a Type II error actually is. A Type II error occurs when the alternative hypothesis is true, but we are nevertheless unable to reject the null hypothesis. Ideally, we’d be able to calculate a single number <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math> that tells us the Type II error rate, similarly to how we can set <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha = .05</annotation></semantics></math> for the Type I error rate. Unfortunately, this is a lot trickier to do.</p>
<p>To see this, notice that in the ESP study, the alternative hypothesis actually corresponds to lots of possible values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>. In fact, the alternative hypothesis corresponds to every value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> <em>except</em> 0.5. Let’s suppose that the true probability of someone choosing the correct response is 55% (i.e., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>.55</mn></mrow><annotation encoding="application/x-tex">\theta = .55</annotation></semantics></math>). If so, then the <em>true</em> sampling distribution for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> is not the same one that the null hypothesis predicts: the most likely value for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> is now 55 out of 100. Not only that, the whole sampling distribution has now shifted, as shown in Figure <a href="hypothesistesting.html#fig:crit3">9.4</a>. The critical regions, of course, do not change: by definition, the critical regions are based on what the null hypothesis predicts. What we’re seeing in this figure is the fact that when the null hypothesis is wrong, a much larger proportion of the sampling distribution falls in the critical region. And, of course, that’s what should happen: the probability of rejecting the null hypothesis is larger when the null hypothesis is actually false! However, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>.55</mn></mrow><annotation encoding="application/x-tex">\theta = .55</annotation></semantics></math> is not the only possibility that is consistent with the alternative hypothesis.</p>
<p>Let’s instead suppose that the true value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> is 0.7. What happens to the sampling distribution when this occurs? The answer, shown in Figure <a href="hypothesistesting.html#fig:crit4">9.5</a>, is that almost the entirety of the sampling distribution has now moved into the critical region. Therefore, if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0.7</mn></mrow><annotation encoding="application/x-tex">\theta = 0.7</annotation></semantics></math>, the probability of us correctly rejecting the null hypothesis (i.e. the power of the test) is much larger than if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0.55</mn></mrow><annotation encoding="application/x-tex">\theta = 0.55</annotation></semantics></math>. In short, while <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>.55</mn></mrow><annotation encoding="application/x-tex">\theta = .55</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>.70</mn></mrow><annotation encoding="application/x-tex">\theta = .70</annotation></semantics></math> are both part of the alternative hypothesis, the Type II error rate is different.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:crit4"></span>
<img src="lsc_files/figure-html/crit4-1.svg" alt="Sampling distribution under the alternative hypothesis, for a population parameter value of θ=0.70\theta = 0.70. Almost all of the distribution lies in the rejection region." width="672" />
<p class="caption">
Figure 9.5: Sampling distribution under the <em>alternative</em> hypothesis, for a population parameter value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0.70</mn></mrow><annotation encoding="application/x-tex">\theta = 0.70</annotation></semantics></math>. Almost all of the distribution lies in the rejection region.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:powerfunction"></span>
<img src="lsc_files/figure-html/powerfunction-1.svg" alt="The probability that we will reject the null hypothesis is plotted as a function of the true value of $\theta$. Obviously, the test is more powerful (greater chance of correct rejection) if the true value of $\theta$ is very different from the value that the null hypothesis specifies (i.e., $\theta=.5$). Notice that when $\theta$ actually is equal to .5 (plotted as a black dot), the null hypothesis is, in fact, true: rejecting the null hypothesis in this instance would be a Type I error." width="672" />
<p class="caption">
Figure 9.6: The probability that we will reject the null hypothesis is plotted as a function of the true value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>. Obviously, the test is more powerful (greater chance of correct rejection) if the true value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> is very different from the value that the null hypothesis specifies (i.e., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">\theta=.5</annotation></semantics></math>). Notice that when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> actually is equal to .5 (plotted as a black dot), the null hypothesis is, in fact, true: rejecting the null hypothesis in this instance would be a Type I error.
</p>
</div>
<p>This means that the power of a test (i.e., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">1-\beta</annotation></semantics></math>) depends on the true value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>. To illustrate this, we’ve calculated the expected probability of rejecting the null hypothesis for all values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> and plotted it in Figure <a href="hypothesistesting.html#fig:powerfunction">9.6</a>. This plot describes what is usually called the <strong>power function</strong> of the test. It’s a nice summary of how good the test is because it actually tells you the power (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">1-\beta</annotation></semantics></math>) for all possible values of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>. As you can see, when the true value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> is very close to 0.5, the power of the test drops very sharply, but when it is further away, the power is large.</p>
</div>
<div id="effect-size" class="section level3 hasAnchor" number="9.7.2">
<h3><span class="header-section-number">9.7.2</span> Effect size<a href="hypothesistesting.html#effect-size" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The plot shown in Figure <a href="hypothesistesting.html#fig:powerfunction">9.6</a> captures a basic point about hypothesis testing. If the true state of the world is very different from what the null hypothesis predicts, then your power will be very high; but if the true state of the world is similar to the null (but not identical), then the power of the test is going to be very low. Therefore, it’s useful to be able to have some way of quantifying how “similar” the true state of the world is to the null hypothesis. A statistic that does this is called a measure of <strong>effect size</strong> <span class="citation">(e.g. <a href="#ref-Cohen1988">Cohen, 1988</a>; <a href="#ref-Ellis2010">Ellis, 2010</a>)</span>.</p>
<p>Effect size is defined slightly differently in different contexts, but the qualitative idea that it tries to capture is always the same: how big is the difference between the <em>true</em> population parameters and the parameter values that are assumed by the null hypothesis.</p>
<p>In our ESP example, if we let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mn>0</mn></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta_0 = 0.5</annotation></semantics></math> denote the value assumed by the null hypothesis, and let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> denote the true value, then a simple measure of effect size could be something like the difference between the true value and null (i.e., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>−</mo><msub><mi>θ</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\theta - \theta_0</annotation></semantics></math>), or possibly just the magnitude of this difference, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">abs</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>θ</mi><mo>−</mo><msub><mi>θ</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mbox{abs}(\theta - \theta_0)</annotation></semantics></math>.</p>
<table>
<colgroup>
<col width="21%" />
<col width="39%" />
<col width="39%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">Big effect size</th>
<th align="center">Small effect size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Significant result</strong></td>
<td align="center">difference is real, and of practical importance</td>
<td align="center">difference is real, but might not be interesting</td>
</tr>
<tr class="even">
<td align="left"><strong>Non-significant result</strong></td>
<td align="center">no effect observed</td>
<td align="center">no effect observed</td>
</tr>
</tbody>
</table>
<p>Why calculate effect size? Let’s assume that you’ve run your experiment, collected the data, and gotten a significant effect when you ran your hypothesis test. Isn’t it enough just to say that you’ve gotten a significant effect? Surely that’s the <em>point</em> of hypothesis testing? Well, sort of. Yes, the point of doing a hypothesis test is to try to demonstrate that the null hypothesis is wrong, but that’s hardly the only thing we’re interested in.</p>
<p>If the null hypothesis claimed that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">\theta = .5</annotation></semantics></math>, and we show that it’s wrong, we’ve only really told half of the story. Rejecting the null hypothesis implies that we believe that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>≠</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">\theta \neq .5</annotation></semantics></math>, but there’s a big difference between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>.51</mn></mrow><annotation encoding="application/x-tex">\theta = .51</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>.8</mn></mrow><annotation encoding="application/x-tex">\theta = .8</annotation></semantics></math>. If we find that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>.8</mn></mrow><annotation encoding="application/x-tex">\theta = .8</annotation></semantics></math>, then not only have we found that the null hypothesis is wrong, it appears to be <em>very</em> wrong.</p>
<p>On the other hand, suppose we’ve successfully rejected the null hypothesis, but it looks like the true value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> is only .51 (this would only be possible with a large study). Sure, the null hypothesis is wrong, but it’s not at all clear that we actually <em>care</em>, because the effect size is so small.</p>
<p>In the context of the ESP study, we might still care, since any demonstration of real psychic powers would actually be pretty cool<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a>, but in other contexts, a 1% difference isn’t very interesting, even if it is a real difference.</p>
<p>For instance, suppose we’re looking at differences in high school exam scores between males and females, and it turns out that the female scores are 1% higher on average than the males. If I’ve got data from thousands of students, then this difference will almost certainly be <em>statistically significant</em>, but regardless of how small the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value is, it’s just not very interesting. You’d hardly want to go around proclaiming a crisis in boys’ education based on such a tiny difference, would you? For this reason, it is becoming more standard (slowly but surely) to report some kind of standard measure of effect size along with the results of the hypothesis test. The hypothesis test itself tells you whether you should believe that the effect you have observed is real (i.e., not just due to chance); the effect size tells you whether or not you should care.</p>
</div>
<div id="increasing-the-power-of-your-study" class="section level3 hasAnchor" number="9.7.3">
<h3><span class="header-section-number">9.7.3</span> Increasing the power of your study<a href="hypothesistesting.html#increasing-the-power-of-your-study" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Not surprisingly, scientists are fairly obsessed with maximising the power of their experiments. We want our experiments to work, so we want to maximise the chance of rejecting the null hypothesis if it is false (and we usually want to believe it is false!).</p>
<p>As we’ve seen, one factor that influences power is the <em>effect size</em>. So the first thing you can do to increase your power is to increase the effect size. In practice, this means that you want to design your study so that the effect size gets magnified. For instance, in the ESP study, we might believe that psychic powers work best in a quiet, darkened room; with fewer distractions to cloud the mind. Therefore, we would try to conduct the experiments in such an environment. If we can strengthen people’s ESP abilities somehow, then the true value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> will go up<a href="#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a> and, therefore, our effect size will be larger. In short, clever experimental design is one way to boost power; because it can alter the effect size.</p>
<p>Unfortunately, it’s often the case that even with the best experimental designs, you may have only a small effect. Perhaps, for example, ESP really does exist, but even under the best of conditions, it’s very very weak. Under those circumstances, your best bet for increasing power is to increase the sample size. Generally, the more observations you have available, the more likely you can discriminate between two hypotheses.</p>
<p>If we had 10 participants, and 7 of them correctly guessed the colour of the hidden card, you wouldn’t be terribly impressed. But if we had 10,000 participants and 7,000 of them got the answer right, you would likely think we had discovered something. In other words, power increases with the sample size. This is illustrated in Figure <a href="hypothesistesting.html#fig:powerfunctionsample">9.7</a>, which shows the power of the test for a true parameter of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0.7</mn></mrow><annotation encoding="application/x-tex">\theta = 0.7</annotation></semantics></math>, for all sample sizes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> from 1 to 100, where I’m assuming that the null hypothesis predicts that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mn>0</mn></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta_0 = 0.5</annotation></semantics></math>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:powerfunctionsample"></span>
<img src="lsc_files/figure-html/powerfunctionsample-1.svg" alt="The power of our test, plotted as a function of the sample size $N$. In this case, the true value of $\theta$ is 0.7, but the null hypothesis is that $\theta = 0.5$. Overall, larger $N$ means greater power. (The small zig-zags in this function occur because of some odd interactions between $\theta$, $\alpha$ and the fact that the binomial distribution is discrete; it doesn't matter for any serious purpose) " width="672" />
<p class="caption">
Figure 9.7: The power of our test, plotted as a function of the sample size <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>. In this case, the true value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> is 0.7, but the null hypothesis is that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\theta = 0.5</annotation></semantics></math>. Overall, larger <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> means greater power. (The small zig-zags in this function occur because of some odd interactions between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> and the fact that the binomial distribution is discrete; it doesn’t matter for any serious purpose)
</p>
</div>
<p>Because power is important, it would be pretty useful to know how much power you’re likely to have when you’re contemplating running an experiment. It’s never possible to know for sure since you can’t possibly know what your effect size is. However, it’s sometimes possible to guess how big it should be. If so, you can guess what sample size you need!</p>
<p>This idea is called <strong>power analysis</strong>. If it’s feasible to do it, it’s very helpful since it can tell you whether you have enough time or money to run the experiment successfully. It’s increasingly common to see people arguing that power analysis should be a required part of experimental design, so it’s worth knowing about, but we won’t be discussing it in this book in details.</p>
</div>
</div>
<div id="nhstmess" class="section level2 hasAnchor" number="9.8">
<h2><span class="header-section-number">9.8</span> Some issues to consider<a href="hypothesistesting.html#nhstmess" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What we’ve discussed in this chapter is the orthodox framework for null hypothesis significance testing (NHST). Understanding how NHST works is an absolute necessity since it has been the dominant approach to inferential statistics since its prominence in the early 20th century. It’s what the vast majority of working scientists rely on for their data analysis, so even if you hate it, you need to know it. However, the approach is not without problems. There are several quirks in the framework, historical oddities in how it came to be, theoretical disputes over whether or not the framework is proper, and many practical traps for the unwary. Without going into much detail on this topic, it’s worth briefly discussing a few of these issues.</p>
<div id="neyman-versus-fisher" class="section level3 hasAnchor" number="9.8.1">
<h3><span class="header-section-number">9.8.1</span> Neyman versus Fisher<a href="hypothesistesting.html#neyman-versus-fisher" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first thing you should be aware of is that orthodox NHST is a mash-up of two somewhat different approaches to hypothesis testing, one proposed by Sir Ronald Fisher and the other proposed by Jerzy Neyman <span class="citation">(for a historical summary, see <a href="#ref-Lehmann2011">Lehmann, 2011</a>)</span>. The history is messy because Fisher and Neyman were real people whose opinions changed over time, and at no point did either of them offer “the definitive statement” of how we should interpret their work many decades later. That said, here’s a quick summary of these two approaches.</p>
<p>First, let’s talk about Fisher’s approach. Fisher assumed that you only had one hypothesis (the null), and what you want to do is find out if the null hypothesis is inconsistent with the data. From his perspective, what you should do is check to see if the data are “sufficiently unlikely” according to the null. In fact, if you remember back to our earlier discussion, that’s how Fisher defines the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value. According to Fisher, if the null hypothesis provided a very poor account of the data, you could safely reject it. But, since you don’t have any other hypotheses to compare it to, there’s no way of “accepting the alternative” because you don’t necessarily have an explicitly stated alternative. That’s more or less all that there was to it.</p>
<p>In contrast, Neyman thought that the point of hypothesis testing was as a guide to action, and his approach was somewhat more formal than Fisher’s. His view was that there are multiple things you could <em>do</em> (accept the null or accept the alternative), and the point of the test was to tell you which one the data support. From this perspective, it is critical to specify your alternative hypothesis properly. If you don’t know the alternative hypothesis, then you don’t know how powerful the test is or which action makes sense. His framework genuinely requires competition between different hypotheses. For Neyman, the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value didn’t directly measure the probability of the data (or data more extreme) under the null; it was more of an abstract description about which “possible tests” were telling you to accept the null and which “possible tests” were telling you to accept the alternative.</p>
<p>As you can see, what we have today is an odd mishmash of the two. We talk about having both a null hypothesis and an alternative (Neyman) but usually define the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value in terms of extreme data (Fisher), but we still have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> values (Neyman). Some statistical tests have explicitly specified alternatives (Neyman), but others are quite vague about it (Fisher). And, according to some people, we’re not allowed to talk about accepting the alternative (Fisher). It’s a mess.</p>
</div>
<div id="bayesians-versus-frequentists" class="section level3 hasAnchor" number="9.8.2">
<h3><span class="header-section-number">9.8.2</span> Bayesians versus frequentists<a href="hypothesistesting.html#bayesians-versus-frequentists" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Earlier in this chapter, we emphasised how you <em>cannot</em> interpret the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value as the probability that the null hypothesis is true. NHST is fundamentally a frequentist tool (see Chapter <a href="probability.html#probability">7</a>), and as such, it does not allow you to assign probabilities to hypotheses: the null hypothesis is either true or not. The Bayesian approach to statistics interprets probability as a degree of belief, so it’s totally okay to say that there is a 10% chance that the null hypothesis is true: that’s just a reflection of your degree of confidence in this hypothesis. You aren’t allowed to do this within the frequentist approach. Remember, if you’re a frequentist, a probability can only be defined in terms of what happens after a large number of independent replications (i.e. a long-run frequency). If this is your interpretation of probability, talking about the “probability” that the null hypothesis is true is complete gibberish.</p>
<p>Most importantly, this <em>isn’t</em> an ideological matter purely. If you decide that you are a Bayesian and are okay with making probability statements about hypotheses, you <em>must</em> follow the Bayesian rules for calculating those probabilities. We’ll talk more about this in Chapter <a href="bayes.html#bayes">15</a>, but for now, understand that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value is a <em>terrible</em> approximation to the probability that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>H</mi><mn>0</mn></msub><annotation encoding="application/x-tex">H_0</annotation></semantics></math> is true. If what you want to know is the probability of the null, then the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> value is not what you’re looking for!</p>
</div>
<div id="traps" class="section level3 hasAnchor" number="9.8.3">
<h3><span class="header-section-number">9.8.3</span> Traps<a href="hypothesistesting.html#traps" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As you can see, the theory behind hypothesis testing is a mess, and even now, there are arguments in statistics about how it “should” work. However, disagreements among statisticians are not our real concern here. Our real concern is practical data analysis. And while the “orthodox” approach to null hypothesis significance testing has many drawbacks, even an unrepentant Bayesian would agree that they can be useful if used responsibly. They usually give sensible answers, and you can use them to learn interesting things. Setting aside the various ideologies and historical confusions that we’ve discussed, the fact remains that the biggest danger in all of statistics is <em>thoughtlessness</em>. Not stupidity but thoughtlessness. The rush to interpret a result without thinking through what each test says about the data and checking whether that’s consistent with how you’ve interpreted it. That’s where the biggest trap lies.</p>
<p>To give an example of this, consider the following example <span class="citation">(see <a href="#ref-Gelman2006">Gelman &amp; Stern, 2006</a>)</span>. Suppose we run the ESP study, and we’ve decided to analyse the data separately for the male and female participants. Of the male participants, 33 out of 50 guessed the card’s colour correctly. This is a significant effect (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>.03</mn></mrow><annotation encoding="application/x-tex">p = .03</annotation></semantics></math>). Of the female participants, 29 out of 50 guessed correctly. This is not a significant effect (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>.32</mn></mrow><annotation encoding="application/x-tex">p = .32</annotation></semantics></math>). Upon observing this, it is extremely tempting for people to start wondering why there is a difference between males and females regarding their psychic abilities. However, this doesn’t seem right. If you think about it, we haven’t <em>actually</em> run a test that explicitly compares males to females. All we have done is compare males to chance (binomial test was significant) and compare females to chance (binomial test was non-significant). If we want to argue that there is a real difference between the males and the females, we should probably run a test of the null hypothesis that there is no difference! We can do that using a different hypothesis test, but when we do that, it turns out that we have no evidence that males and females are significantly different (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>.54</mn></mrow><annotation encoding="application/x-tex">p = .54</annotation></semantics></math>). <em>Now</em> do you think that there’s anything fundamentally different between the two groups? Of course not. What’s happened here is that the data from both groups (male and female) are pretty borderline: by pure chance, one of them happened to end up on the magic side of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">p = .05</annotation></semantics></math> line, and the other one didn’t. That doesn’t imply that males and females are different. This mistake is so common that you should always be wary of it: the difference between significant and not-significant is <em>not</em> evidence of a real difference – if you want to say that there’s a difference between two groups, then you have to test for that difference!</p>
<p>Think about <em>what</em> it is you want to test, <em>why</em> you want to test it, and whether or not the answers that your test gives could possibly make any sense in the real world.</p>
</div>
</div>
<div id="summary-5" class="section level2 hasAnchor" number="9.9">
<h2><span class="header-section-number">9.9</span> Summary<a href="hypothesistesting.html#summary-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Null hypothesis testing is one of the most ubiquitous elements of statistical theory. The vast majority of scientific papers report the results of some hypothesis test or another. As a consequence, it is almost impossible to get by in science without having at least a cursory understanding of what a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value means, making this one of the most important chapters in the book. Here is a quick recap of the key ideas that we’ve talked about:</p>
<ul>
<li>Research hypotheses and statistical hypotheses. Null and alternative hypotheses. (Section <a href="hypothesistesting.html#hypotheses">9.1</a>).</li>
<li>Type 1 and Type 2 errors (Section <a href="hypothesistesting.html#errortypes">9.2</a>)</li>
<li>Test statistics and sampling distributions (Section <a href="hypothesistesting.html#teststatistics">9.3</a>)</li>
<li>Hypothesis testing as a decision making process (Section <a href="hypothesistesting.html#decisionmaking">9.4</a>)</li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values as “soft” decisions (Section <a href="hypothesistesting.html#pvalue">9.5</a>)</li>
<li>Writing up the results of a hypothesis test (Section <a href="hypothesistesting.html#writeup">9.6</a>)</li>
<li>Effect size and power (Section <a href="hypothesistesting.html#effectsize">9.7</a>)</li>
<li>A few issues to consider regarding hypothesis testing (Section <a href="hypothesistesting.html#nhstmess">9.8</a>)</li>
</ul>
<p>Later in the book, in Chapter <a href="bayes.html#bayes">15</a>, we’ll revisit the theory of null hypothesis tests from a Bayesian perspective and introduce a number of new tools that you can use if you aren’t particularly fond of the orthodox approach. But for now, though, we’re done with the abstract statistical theory, and we can start discussing specific data analysis tools.</p>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-Cohen1988" class="csl-entry">
Cohen, J. (1988). <em>Statistical power analysis for the behavioral sciences</em> (2nd ed.). Lawrence Erlbaum.
</div>
<div id="ref-Ellis2010" class="csl-entry">
Ellis, P. D. (2010). <em>The essential guide to effect sizes: Statistical power, meta-analysis, and the interpretation of research results</em>. Cambridge University Press.
</div>
<div id="ref-Gelman2006" class="csl-entry">
Gelman, A., &amp; Stern, H. (2006). The difference between <span>“significant”</span> and <span>“not significant”</span> is not itself statistically significant. <em>The American Statistician</em>, <em>60</em>, 328–331.
</div>
<div id="ref-Lehmann2011" class="csl-entry">
Lehmann, E. L. (2011). <em>Fisher, <span>N</span>eyman, and the creation of classical statistics</em>. Springer.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="35">
<li id="fn35"><p>My apologies to anyone who actually believes in this stuff, but on my reading of the literature on ESP, it’s just not reasonable to think this is real. To be fair, though, some of the studies are rigorously designed; so it’s actually an interesting area for thinking about psychological research design. And of course, it’s a free country, so you can spend your own time and effort proving me wrong if you like, but I wouldn’t think that’s a terribly practical use of your intellect. – Danielle<a href="hypothesistesting.html#fnref35" class="footnote-back">↩︎</a></p></li>
<li id="fn36"><p>Strictly speaking, the test has <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.057</mn></mrow><annotation encoding="application/x-tex">\alpha = .057</annotation></semantics></math>, which is a bit too generous. However, if we’d chosen 39 and 61 as the boundaries for the critical region, then the critical region only covers 3.5% of the distribution. For the sake of the example, we’re willing to tolerate a 5.7% type I error rate since that’s as close as we can get to a value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>.05</mn></mrow><annotation encoding="application/x-tex">\alpha = .05</annotation></semantics></math>.<a href="hypothesistesting.html#fnref36" class="footnote-back">↩︎</a></p></li>
<li id="fn37"><p>That’s <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>.000000000000000000000000136</mn></mrow><annotation encoding="application/x-tex">p = .000000000000000000000000136</annotation></semantics></math> for folks that don’t like or know scientific notation!<a href="hypothesistesting.html#fnref37" class="footnote-back">↩︎</a></p></li>
<li id="fn38"><p>Although, in practice, a very small effect size is worrying because even very minor methodological flaws might be responsible for the effect. And in practice, no experiment is perfect, so there are always methodological issues to worry about.<a href="hypothesistesting.html#fnref38" class="footnote-back">↩︎</a></p></li>
<li id="fn39"><p>Notice that the true population parameter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> doesn’t necessarily correspond to an immutable fact of nature. In this context, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> is just the true probability that people would correctly guess the colour of the card in the other room. As such, the population parameter can be influenced by all sorts of things. Of course, this is all on the assumption that ESP actually exists!<a href="hypothesistesting.html#fnref39" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chisquare.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"fig_caption": true,
"number_sections": true
},
"toc_depth": 3,
"tof": true,
"tot": true,
"toolbar": {
"position": "static"
}
});
});
</script>

</body>

</html>
