<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Exploring a variable pair | Learning Statistics with CogStat</title>
  <meta name="description" content="Chapter 5 Exploring a variable pair | Learning Statistics with CogStat covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students based on Danielle Navarro’s original Learning Statistics with R with a focus on automatic statistical analysis performed in CogStat." />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Exploring a variable pair | Learning Statistics with CogStat" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 5 Exploring a variable pair | Learning Statistics with CogStat covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students based on Danielle Navarro’s original Learning Statistics with R with a focus on automatic statistical analysis performed in CogStat." />
  <meta name="github-repo" content="https://github.com/robertfodor/lsc" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Exploring a variable pair | Learning Statistics with CogStat" />
  
  <meta name="twitter:description" content="Chapter 5 Exploring a variable pair | Learning Statistics with CogStat covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students based on Danielle Navarro’s original Learning Statistics with R with a focus on automatic statistical analysis performed in CogStat." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="exploringavariable.html"/>
<link rel="next" href="probability.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Learning Statistics with CogStat</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this book</a></li>
<li class="part"><span><b>INTRODUCTIONS</b></span></li>
<li class="chapter" data-level="1" data-path="whywhywhy.html"><a href="whywhywhy.html"><i class="fa fa-check"></i><b>1</b> Why do we learn statistics?</a></li>
<li class="chapter" data-level="2" data-path="researchdesign.html"><a href="researchdesign.html"><i class="fa fa-check"></i><b>2</b> A brief introduction to research design</a>
<ul>
<li class="chapter" data-level="2.1" data-path="researchdesign.html"><a href="researchdesign.html#measurement"><i class="fa fa-check"></i><b>2.1</b> Introduction to psychological measurement</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="researchdesign.html"><a href="researchdesign.html#some-thoughts-about-psychological-measurement"><i class="fa fa-check"></i><b>2.1.1</b> Some thoughts about psychological measurement</a></li>
<li class="chapter" data-level="2.1.2" data-path="researchdesign.html"><a href="researchdesign.html#operationalisation-defining-your-measurement"><i class="fa fa-check"></i><b>2.1.2</b> Operationalisation: defining your measurement</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="researchdesign.html"><a href="researchdesign.html#scales"><i class="fa fa-check"></i><b>2.2</b> Scales of measurement</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="researchdesign.html"><a href="researchdesign.html#nominalscale"><i class="fa fa-check"></i><b>2.2.1</b> Nominal scale</a></li>
<li class="chapter" data-level="2.2.2" data-path="researchdesign.html"><a href="researchdesign.html#ordinalscale"><i class="fa fa-check"></i><b>2.2.2</b> Ordinal scale</a></li>
<li class="chapter" data-level="2.2.3" data-path="researchdesign.html"><a href="researchdesign.html#intervalscale"><i class="fa fa-check"></i><b>2.2.3</b> Interval scale</a></li>
<li class="chapter" data-level="2.2.4" data-path="researchdesign.html"><a href="researchdesign.html#ratioscale"><i class="fa fa-check"></i><b>2.2.4</b> Ratio scale</a></li>
<li class="chapter" data-level="2.2.5" data-path="researchdesign.html"><a href="researchdesign.html#continuousdiscrete"><i class="fa fa-check"></i><b>2.2.5</b> Continuous versus discrete variables</a></li>
<li class="chapter" data-level="2.2.6" data-path="researchdesign.html"><a href="researchdesign.html#likertscale"><i class="fa fa-check"></i><b>2.2.6</b> Some complexities: the Likert scale</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="researchdesign.html"><a href="researchdesign.html#reliability"><i class="fa fa-check"></i><b>2.3</b> Assessing the reliability of a measurement</a></li>
<li class="chapter" data-level="2.4" data-path="researchdesign.html"><a href="researchdesign.html#ivdv"><i class="fa fa-check"></i><b>2.4</b> The “role” of variables: predictors and outcomes</a></li>
<li class="chapter" data-level="2.5" data-path="researchdesign.html"><a href="researchdesign.html#researchdesigns"><i class="fa fa-check"></i><b>2.5</b> Experimental and non-experimental research</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="researchdesign.html"><a href="researchdesign.html#experimental-research"><i class="fa fa-check"></i><b>2.5.1</b> Experimental research</a></li>
<li class="chapter" data-level="2.5.2" data-path="researchdesign.html"><a href="researchdesign.html#non-experimental-research"><i class="fa fa-check"></i><b>2.5.2</b> Non-experimental research</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="researchdesign.html"><a href="researchdesign.html#validity"><i class="fa fa-check"></i><b>2.6</b> Assessing the validity of a study</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="researchdesign.html"><a href="researchdesign.html#internal-validity"><i class="fa fa-check"></i><b>2.6.1</b> Internal validity</a></li>
<li class="chapter" data-level="2.6.2" data-path="researchdesign.html"><a href="researchdesign.html#external-validity"><i class="fa fa-check"></i><b>2.6.2</b> External validity</a></li>
<li class="chapter" data-level="2.6.3" data-path="researchdesign.html"><a href="researchdesign.html#construct-validity"><i class="fa fa-check"></i><b>2.6.3</b> Construct validity</a></li>
<li class="chapter" data-level="2.6.4" data-path="researchdesign.html"><a href="researchdesign.html#face-validity"><i class="fa fa-check"></i><b>2.6.4</b> Face validity</a></li>
<li class="chapter" data-level="2.6.5" data-path="researchdesign.html"><a href="researchdesign.html#ecological-validity"><i class="fa fa-check"></i><b>2.6.5</b> Ecological validity</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="researchdesign.html"><a href="researchdesign.html#confounds-artefacts-and-other-threats-to-validity"><i class="fa fa-check"></i><b>2.7</b> Confounds, artefacts and other threats to validity</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="researchdesign.html"><a href="researchdesign.html#history-effects"><i class="fa fa-check"></i><b>2.7.1</b> History effects</a></li>
<li class="chapter" data-level="2.7.2" data-path="researchdesign.html"><a href="researchdesign.html#maturation-effects"><i class="fa fa-check"></i><b>2.7.2</b> Maturation effects</a></li>
<li class="chapter" data-level="2.7.3" data-path="researchdesign.html"><a href="researchdesign.html#repeated-testing-effects"><i class="fa fa-check"></i><b>2.7.3</b> Repeated testing effects</a></li>
<li class="chapter" data-level="2.7.4" data-path="researchdesign.html"><a href="researchdesign.html#selection-bias"><i class="fa fa-check"></i><b>2.7.4</b> Selection bias</a></li>
<li class="chapter" data-level="2.7.5" data-path="researchdesign.html"><a href="researchdesign.html#differentialattrition"><i class="fa fa-check"></i><b>2.7.5</b> Differential attrition</a></li>
<li class="chapter" data-level="2.7.6" data-path="researchdesign.html"><a href="researchdesign.html#non-response-bias"><i class="fa fa-check"></i><b>2.7.6</b> Non-response bias</a></li>
<li class="chapter" data-level="2.7.7" data-path="researchdesign.html"><a href="researchdesign.html#regression-to-the-mean"><i class="fa fa-check"></i><b>2.7.7</b> Regression to the mean</a></li>
<li class="chapter" data-level="2.7.8" data-path="researchdesign.html"><a href="researchdesign.html#experimenter-bias"><i class="fa fa-check"></i><b>2.7.8</b> Experimenter bias</a></li>
<li class="chapter" data-level="2.7.9" data-path="researchdesign.html"><a href="researchdesign.html#demand-effects-and-reactivity"><i class="fa fa-check"></i><b>2.7.9</b> Demand effects and reactivity</a></li>
<li class="chapter" data-level="2.7.10" data-path="researchdesign.html"><a href="researchdesign.html#placebo-effects"><i class="fa fa-check"></i><b>2.7.10</b> Placebo effects</a></li>
<li class="chapter" data-level="2.7.11" data-path="researchdesign.html"><a href="researchdesign.html#situation-measurement-and-subpopulation-effects"><i class="fa fa-check"></i><b>2.7.11</b> Situation, measurement and subpopulation effects</a></li>
<li class="chapter" data-level="2.7.12" data-path="researchdesign.html"><a href="researchdesign.html#fraud-deception-and-self-deception"><i class="fa fa-check"></i><b>2.7.12</b> Fraud, deception and self-deception</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="researchdesign.html"><a href="researchdesign.html#summary"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="cogstat_intro.html"><a href="cogstat_intro.html"><i class="fa fa-check"></i><b>3</b> An Introduction to CogStat</a>
<ul>
<li class="chapter" data-level="3.1" data-path="cogstat_intro.html"><a href="cogstat_intro.html#foreword_cogstat"><i class="fa fa-check"></i><b>3.1</b> How CogStat came about</a></li>
<li class="chapter" data-level="3.2" data-path="cogstat_intro.html"><a href="cogstat_intro.html#autostat"><i class="fa fa-check"></i><b>3.2</b> An introduction to automatic statistical analysis</a></li>
<li class="chapter" data-level="3.3" data-path="cogstat_intro.html"><a href="cogstat_intro.html#gettingstarted"><i class="fa fa-check"></i><b>3.3</b> Getting started with CogStat</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="cogstat_intro.html"><a href="cogstat_intro.html#installing-cogstat"><i class="fa fa-check"></i><b>3.3.1</b> Installing CogStat</a></li>
<li class="chapter" data-level="3.3.2" data-path="cogstat_intro.html"><a href="cogstat_intro.html#loading-data"><i class="fa fa-check"></i><b>3.3.2</b> Loading data</a></li>
<li class="chapter" data-level="3.3.3" data-path="cogstat_intro.html"><a href="cogstat_intro.html#saving-and-exporting-your-results"><i class="fa fa-check"></i><b>3.3.3</b> Saving and exporting your results</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>DESCRIPTIVE STATISTICS</b></span></li>
<li class="chapter" data-level="4" data-path="exploringavariable.html"><a href="exploringavariable.html"><i class="fa fa-check"></i><b>4</b> Exploring a single variable</a>
<ul>
<li class="chapter" data-level="4.1" data-path="exploringavariable.html"><a href="exploringavariable.html#centraltendency"><i class="fa fa-check"></i><b>4.1</b> Measures of central tendency</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="exploringavariable.html"><a href="exploringavariable.html#mean"><i class="fa fa-check"></i><b>4.1.1</b> The mean</a></li>
<li class="chapter" data-level="4.1.2" data-path="exploringavariable.html"><a href="exploringavariable.html#median"><i class="fa fa-check"></i><b>4.1.2</b> The median</a></li>
<li class="chapter" data-level="4.1.3" data-path="exploringavariable.html"><a href="exploringavariable.html#mean-or-median-whats-the-difference"><i class="fa fa-check"></i><b>4.1.3</b> Mean or median? What’s the difference?</a></li>
<li class="chapter" data-level="4.1.4" data-path="exploringavariable.html"><a href="exploringavariable.html#trimmedmean"><i class="fa fa-check"></i><b>4.1.4</b> Trimmed mean</a></li>
<li class="chapter" data-level="4.1.5" data-path="exploringavariable.html"><a href="exploringavariable.html#mode"><i class="fa fa-check"></i><b>4.1.5</b> Mode</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="exploringavariable.html"><a href="exploringavariable.html#var"><i class="fa fa-check"></i><b>4.2</b> Measures of variability</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="exploringavariable.html"><a href="exploringavariable.html#range"><i class="fa fa-check"></i><b>4.2.1</b> Range</a></li>
<li class="chapter" data-level="4.2.2" data-path="exploringavariable.html"><a href="exploringavariable.html#IQR"><i class="fa fa-check"></i><b>4.2.2</b> Interquartile range</a></li>
<li class="chapter" data-level="4.2.3" data-path="exploringavariable.html"><a href="exploringavariable.html#aad"><i class="fa fa-check"></i><b>4.2.3</b> Mean absolute deviation</a></li>
<li class="chapter" data-level="4.2.4" data-path="exploringavariable.html"><a href="exploringavariable.html#variance"><i class="fa fa-check"></i><b>4.2.4</b> Variance</a></li>
<li class="chapter" data-level="4.2.5" data-path="exploringavariable.html"><a href="exploringavariable.html#sd"><i class="fa fa-check"></i><b>4.2.5</b> Standard deviation</a></li>
<li class="chapter" data-level="4.2.6" data-path="exploringavariable.html"><a href="exploringavariable.html#mad"><i class="fa fa-check"></i><b>4.2.6</b> Median absolute deviation</a></li>
<li class="chapter" data-level="4.2.7" data-path="exploringavariable.html"><a href="exploringavariable.html#which-measure-to-use"><i class="fa fa-check"></i><b>4.2.7</b> Which measure to use?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="exploringavariable.html"><a href="exploringavariable.html#skewnesskurtosis"><i class="fa fa-check"></i><b>4.3</b> Skewness and kurtosis</a></li>
<li class="chapter" data-level="4.4" data-path="exploringavariable.html"><a href="exploringavariable.html#summary-descriptives"><i class="fa fa-check"></i><b>4.4</b> Summary: descriptives</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="correl.html"><a href="correl.html"><i class="fa fa-check"></i><b>5</b> Exploring a variable pair</a>
<ul>
<li class="chapter" data-level="5.1" data-path="correl.html"><a href="correl.html#correlationincogstat"><i class="fa fa-check"></i><b>5.1</b> Correlations in Cogstat</a></li>
<li class="chapter" data-level="5.2" data-path="correl.html"><a href="correl.html#the-strength-and-direction-of-a-relationship"><i class="fa fa-check"></i><b>5.2</b> The strength and direction of a relationship</a></li>
<li class="chapter" data-level="5.3" data-path="correl.html"><a href="correl.html#the-correlation-coefficient"><i class="fa fa-check"></i><b>5.3</b> The correlation coefficient</a></li>
<li class="chapter" data-level="5.4" data-path="correl.html"><a href="correl.html#interpretingcorrelations"><i class="fa fa-check"></i><b>5.4</b> Interpreting a correlation</a></li>
<li class="chapter" data-level="5.5" data-path="correl.html"><a href="correl.html#spearmans-rank-correlations"><i class="fa fa-check"></i><b>5.5</b> Spearman’s rank correlations</a></li>
<li class="chapter" data-level="5.6" data-path="correl.html"><a href="correl.html#missing-values-in-pairwise-calculations"><i class="fa fa-check"></i><b>5.6</b> Missing values in pairwise calculations</a></li>
</ul></li>
<li class="part"><span><b>INFERENTIAL STATISTICS</b></span></li>
<li class="chapter" data-level="6" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>6</b> Probability and distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="probability.html"><a href="probability.html#probabilitystats"><i class="fa fa-check"></i><b>6.1</b> How are probability and statistics different?</a></li>
<li class="chapter" data-level="6.2" data-path="probability.html"><a href="probability.html#probabilitymeaning"><i class="fa fa-check"></i><b>6.2</b> What does probability mean?</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="probability.html"><a href="probability.html#the-frequentist-view"><i class="fa fa-check"></i><b>6.2.1</b> The frequentist view</a></li>
<li class="chapter" data-level="6.2.2" data-path="probability.html"><a href="probability.html#the-bayesian-view"><i class="fa fa-check"></i><b>6.2.2</b> The Bayesian view</a></li>
<li class="chapter" data-level="6.2.3" data-path="probability.html"><a href="probability.html#whats-the-difference-and-who-is-right"><i class="fa fa-check"></i><b>6.2.3</b> What’s the difference? And who is right?</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="probability.html"><a href="probability.html#basicprobability"><i class="fa fa-check"></i><b>6.3</b> Basic probability theory</a></li>
<li class="chapter" data-level="6.4" data-path="probability.html"><a href="probability.html#distributions"><i class="fa fa-check"></i><b>6.4</b> Distributions</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="probability.html"><a href="probability.html#binomial"><i class="fa fa-check"></i><b>6.4.1</b> The binomial distribution</a></li>
<li class="chapter" data-level="6.4.2" data-path="probability.html"><a href="probability.html#normal"><i class="fa fa-check"></i><b>6.4.2</b> The normal distribution</a></li>
<li class="chapter" data-level="6.4.3" data-path="probability.html"><a href="probability.html#density"><i class="fa fa-check"></i><b>6.4.3</b> Probability density</a></li>
<li class="chapter" data-level="6.4.4" data-path="probability.html"><a href="probability.html#otherdists"><i class="fa fa-check"></i><b>6.4.4</b> Other useful distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="probability.html"><a href="probability.html#summary-1"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>7</b> Hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Learning Statistics with CogStat</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="correl" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Exploring a variable pair<a href="correl.html#correl" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Up to this point we have focused entirely on how to construct descriptive statistics for a single variable. What we haven’t done is talked about how to describe the relationships <em>between</em> variables in the data. To do that, we want to talk mostly about the <strong>correlation</strong> between variables. But first, we need some data.</p>
<p>After spending so much time looking at the AFL data, let’s turn to a topic close to every parent’s heart: sleep. The following data set is fictitious, but based on real events. Suppose we’re curious to find out how much a baby’s sleeping habits affect the parent’s mood. Let’s say that we can rate parent grumpiness very precisely on a scale from 0 (not at all grumpy) to 100 (very, very grumpy). And, lets also assume that we’ve been measuring parent grumpiness, parent sleeping patterns and the baby’s sleeping patterns for 100 days.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:loadparenthood"></span>
<img src="resources/cogstatparenthoodload.png" alt="This is what you would see after loading the [parenthood.csv](resources/data/parenthood.csv) dataset." width="100%" />
<p class="caption">
Figure 5.1: This is what you would see after loading the <a href="resources/data/parenthood.csv">parenthood.csv</a> dataset.
</p>
</div>
<p>As described in Chapter <a href="exploringavariable.html#exploringavariable">4</a>, we can get all the necessary descriptive statistics for all the variables: <code>parentsleep</code>, <code>babysleep</code> and <code>grumpiness</code>. Let’s summarise all these into a neat little table.</p>
<table>
<caption>
<span id="tab:parenthoodtab">Table 5.1: </span>Descriptive statistics for the parenthood data.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:center;">
Parent grumpiness
</th>
<th style="text-align:center;">
Parent’s hours slept
</th>
<th style="text-align:center;">
Baby’s hours slept
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:center;">
<code>parentgrump</code>
</td>
<td style="text-align:center;">
<code>parentsleep</code>
</td>
<td style="text-align:center;">
<code>babysleep</code>
</td>
</tr>
<tr>
<td style="text-align:left;">
Mean
</td>
<td style="text-align:center;">
63.7
</td>
<td style="text-align:center;">
6.965
</td>
<td style="text-align:center;">
8.049
</td>
</tr>
<tr>
<td style="text-align:left;">
Standard deviation
</td>
<td style="text-align:center;">
10
</td>
<td style="text-align:center;">
1.011
</td>
<td style="text-align:center;">
2.064
</td>
</tr>
<tr>
<td style="text-align:left;">
Skewness
</td>
<td style="text-align:center;">
0.4
</td>
<td style="text-align:center;">
-0.296
</td>
<td style="text-align:center;">
-0.024
</td>
</tr>
<tr>
<td style="text-align:left;">
Kurtosis
</td>
<td style="text-align:center;">
0
</td>
<td style="text-align:center;">
-0.649
</td>
<td style="text-align:center;">
-0.613
</td>
</tr>
<tr>
<td style="text-align:left;">
Range
</td>
<td style="text-align:center;">
50
</td>
<td style="text-align:center;">
4.16
</td>
<td style="text-align:center;">
8.82
</td>
</tr>
<tr>
<td style="text-align:left;">
Maximum
</td>
<td style="text-align:center;">
91
</td>
<td style="text-align:center;">
9
</td>
<td style="text-align:center;">
12.07
</td>
</tr>
<tr>
<td style="text-align:left;">
Upper quartile
</td>
<td style="text-align:center;">
71
</td>
<td style="text-align:center;">
7.74
</td>
<td style="text-align:center;">
9.635
</td>
</tr>
<tr>
<td style="text-align:left;">
Median
</td>
<td style="text-align:center;">
62
</td>
<td style="text-align:center;">
7.03
</td>
<td style="text-align:center;">
7.95
</td>
</tr>
<tr>
<td style="text-align:left;">
Lower quartile
</td>
<td style="text-align:center;">
57
</td>
<td style="text-align:center;">
6.292
</td>
<td style="text-align:center;">
6.425
</td>
</tr>
<tr>
<td style="text-align:left;">
Minimum
</td>
<td style="text-align:center;">
41
</td>
<td style="text-align:center;">
4.84
</td>
<td style="text-align:center;">
3.25
</td>
</tr>
</tbody>
</table>
<div id="correlationincogstat" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Correlations in Cogstat<a href="correl.html#correlationincogstat" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To start understanding the relationship between a pair of variables, select <code>Explore relation of variable pair</code> so a pop-up appears. Move the name of the two variables you wish to analyse (from <code>Available variables</code> to <code>Selected variables</code>, then click <code>OK</code>.</p>
</div>
<div id="the-strength-and-direction-of-a-relationship" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> The strength and direction of a relationship<a href="correl.html#the-strength-and-direction-of-a-relationship" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterparent1a"></span>
<img src="resources/parentsleepgrumpplot.png" alt="Scatterplot drawn by CogStat showing the relationship between `parentsleep` and `parentgrump`" width="100%" />
<p class="caption">
Figure 5.2: Scatterplot drawn by CogStat showing the relationship between <code>parentsleep</code> and <code>parentgrump</code>
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterparent1b"></span>
<img src="resources/babysleepgrumpplot.png" alt="Scatterplot drawn by CogStat showing the relationship between `babysleep` and `parentgrump`" width="100%" />
<p class="caption">
Figure 5.3: Scatterplot drawn by CogStat showing the relationship between <code>babysleep</code> and <code>parentgrump</code>
</p>
</div>
<p>We can draw scatterplots to give us a general sense of how closely related two variables are. Ideally though, we might want to say a bit more about it than that. For instance, let’s compare the relationship between <code>parentsleep</code> and <code>parentgrump</code> (Figure <a href="correl.html#fig:scatterparent1a">5.2</a> with that between <code>babysleep</code> and <code>parentgrump</code> (Figure <a href="correl.html#fig:scatterparent1b">5.3</a>. When looking at these two plots side by side, it’s clear that the relationship is <em>qualitatively</em> the same in both cases: more sleep equals less grump! However, it’s also pretty obvious that the relationship between <code>parentsleep</code> and <code>parentgrump</code> is <em>stronger</em> than the relationship between <code>babysleep</code> and <code>parentgrump</code>. The plot on the left is “neater” than the one on the right. What it feels like is that if you want to predict what the parent’s mood is, it’d help you a little bit to know how many hours the baby slept, but it’d be <em>more</em> helpful to know how many hours the parent slept.</p>
<p>In contrast, let’s consider Figure <a href="correl.html#fig:scatterparent1b">5.3</a> vs. Figure <a href="correl.html#fig:scatterparent2">5.4</a>. If we compare the scatterplot of “<code>babysleep</code> v <code>parentgrump</code>” to the scatterplot of “`<code>babysleep</code> v <code>parentsleep</code>”, the overall strength of the relationship is the same, but the direction is different. That is, if the baby sleeps more, the parent gets <em>more</em> sleep (positive relationship, but if the baby sleeps more then the parent gets <em>less</em> grumpy (negative relationship).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterparent2"></span>
<img src="resources/parentsleepbabysleepplot.png" alt="Scatterplot drawn by CogStat showing the relationship between `babysleep` and `parentsleep`" width="100%" />
<p class="caption">
Figure 5.4: Scatterplot drawn by CogStat showing the relationship between <code>babysleep</code> and <code>parentsleep</code>
</p>
</div>
</div>
<div id="the-correlation-coefficient" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> The correlation coefficient<a href="correl.html#the-correlation-coefficient" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can make these ideas a bit more explicit by introducing the idea of a <strong>correlation coefficient</strong> (or, more specifically, <strong>Pearson’s correlation coefficient</strong>), which is traditionally denoted by <span class="math inline">\(r\)</span>. The correlation coefficient between two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (sometimes denoted <span class="math inline">\(r_{XY}\)</span>), which we’ll define more precisely in the next section, is a measure that varies from <span class="math inline">\(-1\)</span> to <span class="math inline">\(1\)</span>. When <span class="math inline">\(r = -1\)</span> it means that we have a perfect negative relationship, and when <span class="math inline">\(r = 1\)</span> it means we have a perfect positive relationship. When <span class="math inline">\(r = 0\)</span>, there’s no relationship at all. If you look at Figure <a href="correl.html#fig:corr">5.5</a>, you can see several plots showing what different correlations look like.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:corr"></span>
<img src="lsc_files/figure-html/corr-1.svg" alt="Illustration of the effect of varying the strength and direction of a correlation" width="100%" />
<p class="caption">
Figure 5.5: Illustration of the effect of varying the strength and direction of a correlation
</p>
</div>
<p>The formula for the Pearson’s correlation coefficient can be written in several different ways. The simplest way to write down the formula is to break it into two steps. Firstly, let’s introduce the idea of a <strong>covariance</strong>. The covariance between two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is a generalisation of the notion of the variance; it’s a mathematically simple way of describing the relationship between two variables that isn’t terribly informative to humans:
<span class="math display">\[
\mbox{Cov}(X,Y) = \frac{1}{N-1} \sum_{i=1}^N \left( X_i - \bar{X} \right) \left( Y_i - \bar{Y} \right)
\]</span>
Because we’re multiplying (i.e., taking the “product” of) a quantity that depends on <span class="math inline">\(X\)</span> by a quantity that depends on <span class="math inline">\(Y\)</span> and then averaging<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a>, you can think of the formula for the covariance as an “average cross product” between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The covariance has the nice property that, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are entirely unrelated, then the covariance is exactly zero. If the relationship between them is positive (in the sense shown in Figure <a href="correl.html#fig:corr">5.5</a>) then the covariance is also positive; and if the relationship is negative then the covariance is also negative. In other words, the covariance captures the basic qualitative idea of correlation. Unfortunately, the raw magnitude of the covariance isn’t easy to interpret: it depends on the units in which <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are expressed, and worse yet, the actual units that the covariance itself is expressed in are really weird. For instance, if <span class="math inline">\(X\)</span> refers to the <code>parentsleep</code> variable (units: hours) and <span class="math inline">\(Y\)</span> refers to the <code>parentgrump</code> variable (units: grumps), then the units for their covariance are “hours <span class="math inline">\(\times\)</span> grumps”. And I have no freaking idea what that would even mean.</p>
<p>The Pearson correlation coefficient <span class="math inline">\(r\)</span> fixes this interpretation problem by standardising the covariance, in pretty much the exact same way that the <span class="math inline">\(z\)</span>-score standardises a raw score: by dividing by the standard deviation. However, because we have two variables that contribute to the covariance, the standardisation only works if we divide by both standard deviations.<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> In other words, the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> can be written as follows:
<span class="math display">\[
r_{XY}  = \frac{\mbox{Cov}(X,Y)}{ \hat{\sigma}_X \ \hat{\sigma}_Y}
\]</span>
By doing this standardisation, not only do we keep all of the nice properties of the covariance discussed earlier, but the actual values of <span class="math inline">\(r\)</span> are on a meaningful scale: <span class="math inline">\(r= 1\)</span> implies a perfect positive relationship, and <span class="math inline">\(r = -1\)</span> implies a perfect negative relationship.</p>
</div>
<div id="interpretingcorrelations" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Interpreting a correlation<a href="correl.html#interpretingcorrelations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Naturally, in real life you don’t see many correlations of 1. So how should you interpret a correlation of, say <span class="math inline">\(r= .4\)</span>? The honest answer is that it really depends on what you want to use the data for, and on how strong the correlations in your field tend to be. A friend of mine in engineering once argued that any correlation less than <span class="math inline">\(.95\)</span> is completely useless (I think he was exaggerating, even for engineering). On the other hand there are real cases – even in psychology – where you should really expect correlations that strong. For instance, one of the benchmark data sets used to test theories of how people judge similarities is so clean that any theory that can’t achieve a correlation of at least <span class="math inline">\(.9\)</span> really isn’t deemed to be successful. However, when looking for (say) elementary correlates of intelligence (e.g., inspection time, response time), if you get a correlation above <span class="math inline">\(.3\)</span> you’re doing very very well. In short, the interpretation of a correlation depends a lot on the context. That said, the rough guide in Table <a href="correl.html#tab:interpretingcorrelations">5.2</a> is pretty typical.</p>
<table>
<caption>
<span id="tab:interpretingcorrelations">Table 5.2: </span>Rough guide to interpreting correlations
</caption>
<thead>
<tr>
<th style="text-align:left;">
Correlation
</th>
<th style="text-align:center;">
Strength
</th>
<th style="text-align:center;">
Direction
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
-1.0 to -0.9
</td>
<td style="text-align:center;">
Very strong
</td>
<td style="text-align:center;">
Negative
</td>
</tr>
<tr>
<td style="text-align:left;">
-0.9 to -0.7
</td>
<td style="text-align:center;">
Strong
</td>
<td style="text-align:center;">
Negative
</td>
</tr>
<tr>
<td style="text-align:left;">
-0.7 to -0.4
</td>
<td style="text-align:center;">
Moderate
</td>
<td style="text-align:center;">
Negative
</td>
</tr>
<tr>
<td style="text-align:left;">
-0.4 to -0.2
</td>
<td style="text-align:center;">
Weak
</td>
<td style="text-align:center;">
Negative
</td>
</tr>
<tr>
<td style="text-align:left;">
-0.2 to 0
</td>
<td style="text-align:center;">
Negligible
</td>
<td style="text-align:center;">
Negative
</td>
</tr>
<tr>
<td style="text-align:left;">
0 to 0.2
</td>
<td style="text-align:center;">
Negligible
</td>
<td style="text-align:center;">
Positive
</td>
</tr>
<tr>
<td style="text-align:left;">
0.2 to 0.4
</td>
<td style="text-align:center;">
Weak
</td>
<td style="text-align:center;">
Positive
</td>
</tr>
<tr>
<td style="text-align:left;">
0.4 to 0.7
</td>
<td style="text-align:center;">
Moderate
</td>
<td style="text-align:center;">
Positive
</td>
</tr>
<tr>
<td style="text-align:left;">
0.7 to 0.9
</td>
<td style="text-align:center;">
Strong
</td>
<td style="text-align:center;">
Positive
</td>
</tr>
<tr>
<td style="text-align:left;">
0.9 to 1.0
</td>
<td style="text-align:center;">
Very strong
</td>
<td style="text-align:center;">
Positive
</td>
</tr>
</tbody>
</table>
<p>However, something that can never be stressed enough is that you should <em>always</em> look at the scatterplot before attaching any interpretation to the data. A correlation might not mean what you think it means. The classic illustration of this is “Anscombe’s Quartet” <span class="citation">(Anscombe, 1973)</span>, which is a collection of four data sets. Each data set has two variables, an <span class="math inline">\(X\)</span> and a <span class="math inline">\(Y\)</span>. For all four data sets the mean value for <span class="math inline">\(X\)</span> is 9 and the mean for <span class="math inline">\(Y\)</span> is 7.5. The, standard deviations for all <span class="math inline">\(X\)</span> variables are almost identical, as are those for the the <span class="math inline">\(Y\)</span> variables. And in each case the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is <span class="math inline">\(r = 0.816\)</span>.</p>
<p>You’d think that these four data sets would look pretty similar to one another. They do not. If we draw scatterplots of <span class="math inline">\(X\)</span> against <span class="math inline">\(Y\)</span> for all four variables, as shown in Figure <a href="correl.html#fig:anscombe">5.6</a> we see that all four of these are <em>spectacularly</em> different to each other.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:anscombe"></span>
<img src="lsc_files/figure-html/anscombe-1.svg" alt="Anscombe's quartet. All four of these data sets have a Pearson correlation of $r = .816$, but they are qualitatively different from one another." width="100%" />
<p class="caption">
Figure 5.6: Anscombe’s quartet. All four of these data sets have a Pearson correlation of <span class="math inline">\(r = .816\)</span>, but they are qualitatively different from one another.
</p>
</div>
<p>The lesson here, which so very many people seem to forget in real life is “<em>always graph your raw data</em>”.</p>
</div>
<div id="spearmans-rank-correlations" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Spearman’s rank correlations<a href="correl.html#spearmans-rank-correlations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rankcorrpic"></span>
<img src="lsc_files/figure-html/rankcorrpic-1.svg" alt="The relationship between hours worked and grade received, for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of $r = .91$. However, the interesting thing to note here is that there's actually a perfect monotonic relationship between the two variables: in this toy example at least, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of $rho = 1$. With such a small data set, however, it's an open question as to which version better describes the actual relationship involved." width="672" />
<p class="caption">
Figure 5.7: The relationship between hours worked and grade received, for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of <span class="math inline">\(r = .91\)</span>. However, the interesting thing to note here is that there’s actually a perfect monotonic relationship between the two variables: in this toy example at least, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of <span class="math inline">\(rho = 1\)</span>. With such a small data set, however, it’s an open question as to which version better describes the actual relationship involved.
</p>
</div>
<p>The Pearson correlation coefficient is useful for a lot of things, but it does have shortcomings. One issue in particular stands out: what it actually measures is the strength of the <em>linear</em> relationship between two variables. In other words, what it gives you is a measure of the extent to which the data all tend to fall on a single, perfectly straight line. Often, this is a pretty good approximation to what we mean when we say “relationship”, and so the Pearson correlation is a good thing to calculation. Sometimes, it isn’t.</p>
<p>One very common situation where the Pearson correlation isn’t quite the right thing to use arises when an increase in one variable <span class="math inline">\(X\)</span> really is reflected in an increase in another variable <span class="math inline">\(Y\)</span>, but the nature of the relationship isn’t necessarily linear. An example of this might be the relationship between effort and reward when studying for an exam. If you put in zero effort (<span class="math inline">\(X\)</span>) into learning a subject, then you should expect a grade of 0% (<span class="math inline">\(Y\)</span>). However, a little bit of effort will cause a <em>massive</em> improvement: just turning up to lectures means that you learn a fair bit, and if you just turn up to classes, and scribble a few things down so your grade might rise to 35%, all without a lot of effort. However, you just don’t get the same effect at the other end of the scale. As everyone knows, it takes <em>a lot</em> more effort to get a grade of 90% than it takes to get a grade of 55%. What this means is that, if I’ve got data looking at study effort and grades, there’s a pretty good chance that Pearson correlations will be misleading.</p>
<p>To illustrate, consider the data plotted in Figure <a href="correl.html#fig:rankcorrpic">5.7</a>, showing the relationship between hours worked and grade received for 10 students taking some class. The curious thing about this – highly fictitious – data set is that increasing your effort <em>always</em> increases your grade. It might be by a lot or it might be by a little, but increasing effort will never decrease your grade. The data are stored in <a href="resources/data/effort.csv"><code>effort.csv</code></a>.</p>
<p>CogStat will calculate a standard Pearson correlation first[^Unless of course you set your measurement level in the second row of the csv as “ord” (<em>ordinal</em>), because then you already tell the software that Pearson’s <span class="math inline">\(r\)</span> does not make sense to look at.]. It shows a strong relationship between hours worked and grade received: <span class="math inline">\(r = 0.909\)</span>. But this doesn’t actually capture the observation that increasing hours worked <em>always</em> increases the grade. There’s a sense here in which we want to be able to say that the correlation is <em>perfect</em> but for a somewhat different notion of what a “relationship” is. What we’re looking for is something that captures the fact that there is a perfect <strong>ordinal relationship</strong> here. That is, if student 1 works more hours than student 2, then we can guarantee that student 1 will get the better grade. That’s not what a correlation of <span class="math inline">\(r = 0.91\)</span> says at all.</p>
<p>How should we address this? Actually, it’s really easy: if we’re looking for ordinal relationships, all we have to do is treat the data as if it were ordinal scale! So, instead of measuring effort in terms of “hours worked”, lets rank all 10 of our students in order of hours worked. That is, student 1 did the least work out of anyone (2 hours) so they get the lowest rank (rank = 1). Student 4 was the next laziest, putting in only 6 hours of work in over the whole semester, so they get the next lowest rank (rank = 2). Notice that we’re using “rank = 1” to mean “low rank”. Sometimes in everyday language we talk about “rank = 1” to mean “top rank” rather than “bottom rank”. So be careful: you can rank “from smallest value to largest value” (i.e., small equals rank 1) or you can rank “from largest value to smallest value” (i.e., large equals rank 1). In this case, we’re ranking from smallest to largest. But in real life, it’s really easy to forget which way you set things up, so you have to put a bit of effort into remembering!</p>
<p>Okay, so let’s have a look at our students when we rank them from worst to best in terms of effort and reward:</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
Student
</th>
<th style="text-align:center;">
Rank (hours worked)
</th>
<th style="text-align:center;">
Rank (grade received)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
student 1
</td>
<td style="text-align:center;">
1
</td>
<td style="text-align:center;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
student 2
</td>
<td style="text-align:center;">
10
</td>
<td style="text-align:center;">
10
</td>
</tr>
<tr>
<td style="text-align:left;">
student 3
</td>
<td style="text-align:center;">
6
</td>
<td style="text-align:center;">
6
</td>
</tr>
<tr>
<td style="text-align:left;">
student 4
</td>
<td style="text-align:center;">
2
</td>
<td style="text-align:center;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
student 5
</td>
<td style="text-align:center;">
3
</td>
<td style="text-align:center;">
3
</td>
</tr>
<tr>
<td style="text-align:left;">
student 6
</td>
<td style="text-align:center;">
5
</td>
<td style="text-align:center;">
5
</td>
</tr>
<tr>
<td style="text-align:left;">
student 7
</td>
<td style="text-align:center;">
4
</td>
<td style="text-align:center;">
4
</td>
</tr>
<tr>
<td style="text-align:left;">
student 8
</td>
<td style="text-align:center;">
8
</td>
<td style="text-align:center;">
8
</td>
</tr>
<tr>
<td style="text-align:left;">
student 9
</td>
<td style="text-align:center;">
7
</td>
<td style="text-align:center;">
7
</td>
</tr>
<tr>
<td style="text-align:left;">
student 10
</td>
<td style="text-align:center;">
9
</td>
<td style="text-align:center;">
9
</td>
</tr>
</tbody>
</table>
<p>Hm. These are <em>identical</em>. The student who put in the most effort got the best grade, the student with the least effort got the worst grade, etc. We can rank students by hours worked, then rank students by grade received, and these two rankings would be identical. So if we now correlate them we get a perfect relationship: <span class="math inline">\(1\)</span>.</p>
<p>What we’ve just re-invented is <strong>Spearman’s rank order correlation</strong>, usually denoted <span class="math inline">\(\rho\)</span> (pronounced: rho) to distinguish it from the Pearson correlation <span class="math inline">\(r\)</span>. CogStat will use <span class="math inline">\(r_{S}\)</span> to denote rank order correlation.</p>
<p>CogStat will calculate both Pearson’s correlation and Spearman’s rank order correlation for you automatically if your measurement is not set in your source data (Figure <a href="correl.html#fig:cogstatcorrelationresult">5.8</a>). If you set the measurement type to “ordinal” in your source file, it will omit calculating Pearson’s correlation due to the reasons discussed above.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cogstatcorrelationresult"></span>
<img src="resources/cogstatcorrelationresult.png" alt="This is what you would see in CogStat after loading the `effort.csv` dataset." width="100%" />
<p class="caption">
Figure 5.8: This is what you would see in CogStat after loading the <code>effort.csv</code> dataset.
</p>
</div>
</div>
<div id="missing-values-in-pairwise-calculations" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Missing values in pairwise calculations<a href="correl.html#missing-values-in-pairwise-calculations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>I mentioned earlier that the <code>cor()</code> function is a special case. It doesn’t have an <code>na.rm</code> argument, because the story becomes a lot more complicated when more than one variable is involved. What it does have is an argument called <code>use</code> which does roughly the same thing, but you need to think little more carefully about what you want this time. To illustrate the issues, let’s open up a data set that has missing values, <code>parenthood2.Rdata</code>. This file contains the same data as the original parenthood data, but with some values deleted. It contains a single data frame, <code>parenthood2</code>:</p>
<pre><code>&gt; load( &quot;parenthood2.Rdata&quot; )
&gt; print( parenthood2 )
  parentsleep baby.sleep parentgrump day
1      7.59         NA        56   1
2      7.91      11.66        60   2
3      5.14       7.92        82   3
4      7.71       9.61        55   4
5      6.68       9.75        NA   5
6      5.99       5.04        72   6
BLAH BLAH BLAH</code></pre>
<p>If I calculate my descriptive statistics using the <code>describe()</code> function</p>
<pre><code>&gt; describe( parenthood2 )
           var   n  mean    sd median trimmed   mad   min    max    BLAH
parentsleep    1  91  6.98  1.02   7.03    7.02  1.13  4.84   9.00    BLAH
baby.sleep   2  89  8.11  2.05   8.20    8.13  2.28  3.25  12.07    BLAH
parentgrump    3  92 63.15  9.85  61.00   62.66 10.38 41.00  89.00    BLAH
day          4 100 50.50 29.01  50.50   50.50 37.06  1.00 100.00    BLAH</code></pre>
<p>we can see from the <code>n</code> column that there are 9 missing values for <code>parentsleep</code>, 11 missing values for <code>baby.sleep</code> and 8 missing values for <code>parentgrump</code>.<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a> Suppose what I would like is a correlation matrix. And let’s also suppose that I don’t bother to tell R how to handle those missing values. Here’s what happens:</p>
<pre><code>&gt; cor( parenthood2 )
           parentsleep baby.sleep parentgrump day
parentsleep          1         NA        NA  NA
baby.sleep        NA          1        NA  NA
parentgrump         NA         NA         1  NA
day               NA         NA        NA   1</code></pre>
<p>Annoying, but it kind of makes sense. If I don’t <em>know</em> what some of the values of <code>parentsleep</code> and <code>baby.sleep</code> actually are, then I can’t possibly <em>know</em> what the correlation between these two variables is either, since the formula for the correlation coefficient makes use of every single observation in the data set. Once again, it makes sense: it’s just not particularly <em>helpful</em>.</p>
<p>To make R behave more sensibly in this situation, you need to specify the <code>use</code> argument to the <code>cor()</code> function. There are several different values that you can specify for this, but the two that we care most about in practice tend to be <code>"complete.obs"</code> and <code>"pairwise.complete.obs"</code>. If we specify <code>use = "complete.obs"</code>, R will completely ignore all cases (i.e., all rows in our <code>parenthood2</code> data frame) that have any missing values at all. So, for instance, if you look back at the extract earlier when I used the <code>head()</code> function, notice that observation 1 (i.e., day 1) of the <code>parenthood2</code> data set is missing the value for <code>baby.sleep</code>, but is otherwise complete? Well, if you choose <code>use = "complete.obs"</code> R will ignore that row completely: that is, even when it’s trying to calculate the correlation between <code>parentsleep</code> and <code>parentgrump</code>, observation 1 will be ignored, because the value of <code>baby.sleep</code> is missing for that observation. Here’s what we get:</p>
<pre><code>&gt; cor(parenthood2, use = &quot;complete.obs&quot;)
             parentsleep baby.sleep   parentgrump         day
parentsleep   1.00000000  0.6394985 -0.89951468  0.06132891
baby.sleep  0.63949845  1.0000000 -0.58656066  0.14555814
parentgrump  -0.89951468 -0.5865607  1.00000000 -0.06816586
day         0.06132891  0.1455581 -0.06816586  1.00000000</code></pre>
<p>The other possibility that we care about, and the one that tends to get used more often in practice, is to set <code>use = "pairwise.complete.obs"</code>. When we do that, R only looks at the variables that it’s trying to correlate when determining what to drop. So, for instance, since the only missing value for observation 1 of <code>parenthood2</code> is for <code>baby.sleep</code> R will only drop observation 1 when <code>baby.sleep</code> is one of the variables involved: and so R keeps observation 1 when trying to correlate <code>parentsleep</code> and <code>parentgrump</code>. When we do it this way, here’s what we get:</p>
<pre><code>&gt; cor(parenthood2, use = &quot;pairwise.complete.obs&quot;) 
             parentsleep  baby.sleep    parentgrump          day
parentsleep   1.00000000  0.61472303 -0.903442442 -0.076796665
baby.sleep  0.61472303  1.00000000 -0.567802669  0.058309485
parentgrump  -0.90344244 -0.56780267  1.000000000  0.005833399
day        -0.07679667  0.05830949  0.005833399  1.000000000</code></pre>
<p>Similar, but not quite the same. It’s also worth noting that the <code>correlate()</code> function (in the <code>lsr</code> package) automatically uses the “pairwise complete” method:</p>
<pre><code>&gt; correlate(parenthood2)

CORRELATIONS
============
- correlation type:  pearson 
- correlations shown only when both variables are numeric

           parentsleep baby.sleep parentgrump    day
parentsleep          .      0.615    -0.903 -0.077
baby.sleep     0.615          .    -0.568  0.058
parentgrump     -0.903     -0.568         .  0.006
day           -0.077      0.058     0.006      .</code></pre>
<p>The two approaches have different strengths and weaknesses. The “pairwise complete” approach has the advantage that it keeps more observations, so you’re making use of more of your data and (as we’ll discuss in tedious detail in Chapter <a href="#estimation"><strong>??</strong></a> and it improves the reliability of your estimated correlation. On the other hand, it means that every correlation in your correlation matrix is being computed from a slightly different set of observations, which can be awkward when you want to compare the different correlations that you’ve got.</p>
<p>So which method should you use? It depends a lot on <em>why</em> you think your values are missing, and probably depends a little on how paranoid you are. For instance, if you think that the missing values were “chosen” completely randomly<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a> then you’ll probably want to use the pairwise method. If you think that missing data are a cue to thinking that the whole observation might be rubbish (e.g., someone just selecting arbitrary responses in your questionnaire), but that there’s no pattern to which observations are “rubbish” then it’s probably safer to keep only those observations that are complete. If you think there’s something systematic going on, in that some observations are more likely to be missing than others, then you have a much trickier problem to solve, and one that is beyond the scope of this book.</p>

</div>
</div>



<div class="footnotes">
<hr />
<ol start="16">
<li id="fn16"><p>Just like we saw with the variance and the standard deviation, in practice we divide by <span class="math inline">\(N-1\)</span> rather than <span class="math inline">\(N\)</span>.<a href="correl.html#fnref16" class="footnote-back">↩︎</a></p></li>
<li id="fn17"><p>This is an oversimplification, but it’ll do for our purposes.<a href="correl.html#fnref17" class="footnote-back">↩︎</a></p></li>
<li id="fn18"><p>It’s worth noting that, even though we have missing data for each of these variables, the output doesn’t contain any <code>NA</code> values. This is because, while <code>describe()</code> also has an <code>na.rm</code> argument, the default value for this function is <code>na.rm = TRUE</code>.<a href="correl.html#fnref18" class="footnote-back">↩︎</a></p></li>
<li id="fn19"><p>The technical term here is “missing completely at random” (often written MCAR for short). Makes sense, I suppose, but it does sound ungrammatical to me.<a href="correl.html#fnref19" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="exploringavariable.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="probability.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"number_sections": true,
"fig_caption": true
},
"toc_depth": 2,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
