<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Introduction to probability | Learning Statistics with CogStat</title>
  <meta name="description" content="Chapter 5 Introduction to probability | Learning Statistics with CogStat covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students based on Danielle Navarro’s original Learning Statistics with R with a focus on automatic statistical analysis performed in CogStat." />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Introduction to probability | Learning Statistics with CogStat" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 5 Introduction to probability | Learning Statistics with CogStat covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students based on Danielle Navarro’s original Learning Statistics with R with a focus on automatic statistical analysis performed in CogStat." />
  <meta name="github-repo" content="https://github.com/robertfodor/lsc" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Introduction to probability | Learning Statistics with CogStat" />
  
  <meta name="twitter:description" content="Chapter 5 Introduction to probability | Learning Statistics with CogStat covers the contents of an introductory statistics class, as typically taught to undergraduate psychology students based on Danielle Navarro’s original Learning Statistics with R with a focus on automatic statistical analysis performed in CogStat." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="exploring-a-variable.html"/>
<link rel="next" href="distributions.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Learning Statistics with CogStat</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this book</a></li>
<li class="part"><span><b>INTRODUCTIONS</b></span></li>
<li class="chapter" data-level="1" data-path="whywhywhy.html"><a href="whywhywhy.html"><i class="fa fa-check"></i><b>1</b> Why do we learn statistics?</a></li>
<li class="chapter" data-level="2" data-path="researchdesign.html"><a href="researchdesign.html"><i class="fa fa-check"></i><b>2</b> A brief introduction to research design</a>
<ul>
<li class="chapter" data-level="2.1" data-path="researchdesign.html"><a href="researchdesign.html#measurement"><i class="fa fa-check"></i><b>2.1</b> Introduction to psychological measurement</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="researchdesign.html"><a href="researchdesign.html#some-thoughts-about-psychological-measurement"><i class="fa fa-check"></i><b>2.1.1</b> Some thoughts about psychological measurement</a></li>
<li class="chapter" data-level="2.1.2" data-path="researchdesign.html"><a href="researchdesign.html#operationalisation-defining-your-measurement"><i class="fa fa-check"></i><b>2.1.2</b> Operationalisation: defining your measurement</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="researchdesign.html"><a href="researchdesign.html#scales"><i class="fa fa-check"></i><b>2.2</b> Scales of measurement</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="researchdesign.html"><a href="researchdesign.html#nominalscale"><i class="fa fa-check"></i><b>2.2.1</b> Nominal scale</a></li>
<li class="chapter" data-level="2.2.2" data-path="researchdesign.html"><a href="researchdesign.html#ordinalscale"><i class="fa fa-check"></i><b>2.2.2</b> Ordinal scale</a></li>
<li class="chapter" data-level="2.2.3" data-path="researchdesign.html"><a href="researchdesign.html#intervalscale"><i class="fa fa-check"></i><b>2.2.3</b> Interval scale</a></li>
<li class="chapter" data-level="2.2.4" data-path="researchdesign.html"><a href="researchdesign.html#ratioscale"><i class="fa fa-check"></i><b>2.2.4</b> Ratio scale</a></li>
<li class="chapter" data-level="2.2.5" data-path="researchdesign.html"><a href="researchdesign.html#continuousdiscrete"><i class="fa fa-check"></i><b>2.2.5</b> Continuous versus discrete variables</a></li>
<li class="chapter" data-level="2.2.6" data-path="researchdesign.html"><a href="researchdesign.html#likertscale"><i class="fa fa-check"></i><b>2.2.6</b> Some complexities: the Likert scale</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="researchdesign.html"><a href="researchdesign.html#reliability"><i class="fa fa-check"></i><b>2.3</b> Assessing the reliability of a measurement</a></li>
<li class="chapter" data-level="2.4" data-path="researchdesign.html"><a href="researchdesign.html#ivdv"><i class="fa fa-check"></i><b>2.4</b> The “role” of variables: predictors and outcomes</a></li>
<li class="chapter" data-level="2.5" data-path="researchdesign.html"><a href="researchdesign.html#researchdesigns"><i class="fa fa-check"></i><b>2.5</b> Experimental and non-experimental research</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="researchdesign.html"><a href="researchdesign.html#experimental-research"><i class="fa fa-check"></i><b>2.5.1</b> Experimental research</a></li>
<li class="chapter" data-level="2.5.2" data-path="researchdesign.html"><a href="researchdesign.html#non-experimental-research"><i class="fa fa-check"></i><b>2.5.2</b> Non-experimental research</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="researchdesign.html"><a href="researchdesign.html#validity"><i class="fa fa-check"></i><b>2.6</b> Assessing the validity of a study</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="researchdesign.html"><a href="researchdesign.html#internal-validity"><i class="fa fa-check"></i><b>2.6.1</b> Internal validity</a></li>
<li class="chapter" data-level="2.6.2" data-path="researchdesign.html"><a href="researchdesign.html#external-validity"><i class="fa fa-check"></i><b>2.6.2</b> External validity</a></li>
<li class="chapter" data-level="2.6.3" data-path="researchdesign.html"><a href="researchdesign.html#construct-validity"><i class="fa fa-check"></i><b>2.6.3</b> Construct validity</a></li>
<li class="chapter" data-level="2.6.4" data-path="researchdesign.html"><a href="researchdesign.html#face-validity"><i class="fa fa-check"></i><b>2.6.4</b> Face validity</a></li>
<li class="chapter" data-level="2.6.5" data-path="researchdesign.html"><a href="researchdesign.html#ecological-validity"><i class="fa fa-check"></i><b>2.6.5</b> Ecological validity</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="researchdesign.html"><a href="researchdesign.html#confounds-artefacts-and-other-threats-to-validity"><i class="fa fa-check"></i><b>2.7</b> Confounds, artefacts and other threats to validity</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="researchdesign.html"><a href="researchdesign.html#history-effects"><i class="fa fa-check"></i><b>2.7.1</b> History effects</a></li>
<li class="chapter" data-level="2.7.2" data-path="researchdesign.html"><a href="researchdesign.html#maturation-effects"><i class="fa fa-check"></i><b>2.7.2</b> Maturation effects</a></li>
<li class="chapter" data-level="2.7.3" data-path="researchdesign.html"><a href="researchdesign.html#repeated-testing-effects"><i class="fa fa-check"></i><b>2.7.3</b> Repeated testing effects</a></li>
<li class="chapter" data-level="2.7.4" data-path="researchdesign.html"><a href="researchdesign.html#selection-bias"><i class="fa fa-check"></i><b>2.7.4</b> Selection bias</a></li>
<li class="chapter" data-level="2.7.5" data-path="researchdesign.html"><a href="researchdesign.html#differentialattrition"><i class="fa fa-check"></i><b>2.7.5</b> Differential attrition</a></li>
<li class="chapter" data-level="2.7.6" data-path="researchdesign.html"><a href="researchdesign.html#non-response-bias"><i class="fa fa-check"></i><b>2.7.6</b> Non-response bias</a></li>
<li class="chapter" data-level="2.7.7" data-path="researchdesign.html"><a href="researchdesign.html#regression-to-the-mean"><i class="fa fa-check"></i><b>2.7.7</b> Regression to the mean</a></li>
<li class="chapter" data-level="2.7.8" data-path="researchdesign.html"><a href="researchdesign.html#experimenter-bias"><i class="fa fa-check"></i><b>2.7.8</b> Experimenter bias</a></li>
<li class="chapter" data-level="2.7.9" data-path="researchdesign.html"><a href="researchdesign.html#demand-effects-and-reactivity"><i class="fa fa-check"></i><b>2.7.9</b> Demand effects and reactivity</a></li>
<li class="chapter" data-level="2.7.10" data-path="researchdesign.html"><a href="researchdesign.html#placebo-effects"><i class="fa fa-check"></i><b>2.7.10</b> Placebo effects</a></li>
<li class="chapter" data-level="2.7.11" data-path="researchdesign.html"><a href="researchdesign.html#situation-measurement-and-subpopulation-effects"><i class="fa fa-check"></i><b>2.7.11</b> Situation, measurement and subpopulation effects</a></li>
<li class="chapter" data-level="2.7.12" data-path="researchdesign.html"><a href="researchdesign.html#fraud-deception-and-self-deception"><i class="fa fa-check"></i><b>2.7.12</b> Fraud, deception and self-deception</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="researchdesign.html"><a href="researchdesign.html#summary"><i class="fa fa-check"></i><b>2.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="cogstat_intro.html"><a href="cogstat_intro.html"><i class="fa fa-check"></i><b>3</b> An Introduction to CogStat</a>
<ul>
<li class="chapter" data-level="3.1" data-path="cogstat_intro.html"><a href="cogstat_intro.html#foreword_cogstat"><i class="fa fa-check"></i><b>3.1</b> How CogStat came about</a></li>
<li class="chapter" data-level="3.2" data-path="cogstat_intro.html"><a href="cogstat_intro.html#autostat"><i class="fa fa-check"></i><b>3.2</b> An introduction to automatic statistical analysis</a></li>
<li class="chapter" data-level="3.3" data-path="cogstat_intro.html"><a href="cogstat_intro.html#gettingstarted"><i class="fa fa-check"></i><b>3.3</b> Getting started with CogStat</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="cogstat_intro.html"><a href="cogstat_intro.html#installing-cogstat"><i class="fa fa-check"></i><b>3.3.1</b> Installing CogStat</a></li>
<li class="chapter" data-level="3.3.2" data-path="cogstat_intro.html"><a href="cogstat_intro.html#loading-data"><i class="fa fa-check"></i><b>3.3.2</b> Loading data</a></li>
<li class="chapter" data-level="3.3.3" data-path="cogstat_intro.html"><a href="cogstat_intro.html#saving-and-exporting-your-results"><i class="fa fa-check"></i><b>3.3.3</b> Saving and exporting your results</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>DESCRIPTIVE STATISTICS</b></span></li>
<li class="chapter" data-level="4" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html"><i class="fa fa-check"></i><b>4</b> Exploring a variable</a>
<ul>
<li class="chapter" data-level="4.1" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#centraltendency"><i class="fa fa-check"></i><b>4.1</b> Measures of central tendency</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#mean"><i class="fa fa-check"></i><b>4.1.1</b> The mean</a></li>
<li class="chapter" data-level="4.1.2" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#median"><i class="fa fa-check"></i><b>4.1.2</b> The median</a></li>
<li class="chapter" data-level="4.1.3" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#mean-or-median-whats-the-difference"><i class="fa fa-check"></i><b>4.1.3</b> Mean or median? What’s the difference?</a></li>
<li class="chapter" data-level="4.1.4" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#trimmedmean"><i class="fa fa-check"></i><b>4.1.4</b> Trimmed mean</a></li>
<li class="chapter" data-level="4.1.5" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#mode"><i class="fa fa-check"></i><b>4.1.5</b> Mode</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#var"><i class="fa fa-check"></i><b>4.2</b> Measures of variability</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#range"><i class="fa fa-check"></i><b>4.2.1</b> Range</a></li>
<li class="chapter" data-level="4.2.2" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#IQR"><i class="fa fa-check"></i><b>4.2.2</b> Interquartile range</a></li>
<li class="chapter" data-level="4.2.3" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#aad"><i class="fa fa-check"></i><b>4.2.3</b> Mean absolute deviation</a></li>
<li class="chapter" data-level="4.2.4" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#variance"><i class="fa fa-check"></i><b>4.2.4</b> Variance</a></li>
<li class="chapter" data-level="4.2.5" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#sd"><i class="fa fa-check"></i><b>4.2.5</b> Standard deviation</a></li>
<li class="chapter" data-level="4.2.6" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#mad"><i class="fa fa-check"></i><b>4.2.6</b> Median absolute deviation</a></li>
<li class="chapter" data-level="4.2.7" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#which-measure-to-use"><i class="fa fa-check"></i><b>4.2.7</b> Which measure to use?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#skewnesskurtosis"><i class="fa fa-check"></i><b>4.3</b> Skewness and kurtosis</a></li>
<li class="chapter" data-level="4.4" data-path="exploring-a-variable.html"><a href="exploring-a-variable.html#summary-descriptives"><i class="fa fa-check"></i><b>4.4</b> Summary: descriptives</a></li>
</ul></li>
<li class="part"><span><b>INFERENTIAL STATISTICS</b></span></li>
<li class="chapter" data-level="5" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>5</b> Introduction to probability</a>
<ul>
<li class="chapter" data-level="5.1" data-path="probability.html"><a href="probability.html#probabilitystats"><i class="fa fa-check"></i><b>5.1</b> How are probability and statistics different?</a></li>
<li class="chapter" data-level="5.2" data-path="probability.html"><a href="probability.html#probabilitymeaning"><i class="fa fa-check"></i><b>5.2</b> What does probability mean?</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="probability.html"><a href="probability.html#the-frequentist-view"><i class="fa fa-check"></i><b>5.2.1</b> The frequentist view</a></li>
<li class="chapter" data-level="5.2.2" data-path="probability.html"><a href="probability.html#the-bayesian-view"><i class="fa fa-check"></i><b>5.2.2</b> The Bayesian view</a></li>
<li class="chapter" data-level="5.2.3" data-path="probability.html"><a href="probability.html#whats-the-difference-and-who-is-right"><i class="fa fa-check"></i><b>5.2.3</b> What’s the difference? And who is right?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="probability.html"><a href="probability.html#basicprobability"><i class="fa fa-check"></i><b>5.3</b> Basic probability theory</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="probability.html"><a href="probability.html#introducing-probability-distributions"><i class="fa fa-check"></i><b>5.3.1</b> Introducing probability distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>6</b> Distributions</a>
<ul>
<li class="chapter" data-level="6.1" data-path="distributions.html"><a href="distributions.html#binomial"><i class="fa fa-check"></i><b>6.1</b> The binomial distribution</a></li>
<li class="chapter" data-level="6.2" data-path="distributions.html"><a href="distributions.html#normal"><i class="fa fa-check"></i><b>6.2</b> The normal distribution</a></li>
<li class="chapter" data-level="6.3" data-path="distributions.html"><a href="distributions.html#density"><i class="fa fa-check"></i><b>6.3</b> Probability density</a></li>
<li class="chapter" data-level="6.4" data-path="distributions.html"><a href="distributions.html#otherdists"><i class="fa fa-check"></i><b>6.4</b> Other useful distributions</a></li>
<li class="chapter" data-level="6.5" data-path="distributions.html"><a href="distributions.html#summary-1"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="correl.html"><a href="correl.html"><i class="fa fa-check"></i><b>7</b> Correlations</a>
<ul>
<li class="chapter" data-level="7.1" data-path="correl.html"><a href="correl.html#the-strength-and-direction-of-a-relationship"><i class="fa fa-check"></i><b>7.1</b> The strength and direction of a relationship</a></li>
<li class="chapter" data-level="7.2" data-path="correl.html"><a href="correl.html#pearsons-correlation-coefficient"><i class="fa fa-check"></i><b>7.2</b> Pearson’s correlation coefficient</a></li>
<li class="chapter" data-level="7.3" data-path="correl.html"><a href="correl.html#spearmans-rank-correlations"><i class="fa fa-check"></i><b>7.3</b> Spearman’s rank correlations</a></li>
<li class="chapter" data-level="7.4" data-path="correl.html"><a href="correl.html#interpretingcorrelations"><i class="fa fa-check"></i><b>7.4</b> Calculating and interpreting correlations in CogStat</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>8</b> Hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Learning Statistics with CogStat</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Introduction to probability<a href="probability.html#probability" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Up to this point in the book, we’ve discussed some critical ideas in experimental design and talked a little about how you can summarise a data set. To many people, this is all there is to statistics: it’s about calculating averages, collecting all the numbers, drawing pictures, and putting them all in a report somewhere. Kind of like stamp collecting, but with numbers. However, statistics cover much more than that. Descriptive statistics is one of the less significant parts of statistics and one of the least powerful. The more valuable part of statistics is that it provides tools that let you make <em>inferences</em> about data.</p>
<p>Once you start thinking about statistics in these terms – that statistics are there to help us draw inferences from data – you start seeing examples everywhere. For instance, here’s a small extract from a newspaper article in the Sydney Morning Herald (30 Oct 2010):</p>
<blockquote>
<p>“I have a tough job,” the Premier said in response to a poll which found her government is now the most unpopular Labor administration in polling history, with a primary vote of just 23 per cent.</p>
</blockquote>
<p>This kind of remark is unremarkable in the papers or in everyday life, but let’s consider what it entails. A polling company has conducted an extensive survey because they can afford it. Let’s imagine that they called 1000 NSW voters at random, and 230 (23%) of those claimed that they intended to vote for the ALP. For the 2010 Federal election, the Australian Electoral Commission reported 4,610,795 enrolled voters in NSW, so the opinions of the remaining 4,609,795 voters (about 99.98% of voters) remain unknown to us. Even assuming that no one lied to the polling company, we can only say with 100% confidence that the actual ALP primary vote is somewhere between 230/4610795 (about 0.005%) and 4610025/4610795 (about 99.83%). So, on what basis is it legitimate for the polling company, the newspaper, and the readership to conclude that the ALP primary vote is only about 23%?</p>
<p>The answer to the question is pretty obvious: if we call 1000 people at random, and 230 of them say they intend to vote for the ALP, then it seems very unlikely that these are the <em>only</em> 230 people out of the entire voting public who actually intend to do so. In other words, we assume that the data collected by the polling company is pretty representative of the population at large. But how representative? Would we be surprised to discover that the true ALP primary vote is actually 24%? 29%? 37%? At this point, everyday intuition starts to break down a bit. No one would be surprised by 24%, and everybody would be surprised by 37%, but it’s hard to say whether 29% is plausible. We need more powerful tools than just looking at the numbers and guessing.</p>
<p><strong>Inferential statistics</strong> provides the tools we need to answer these questions. Since these questions lie at the heart of the scientific enterprise, they take up the lion’s share of every introductory course on statistics and research methods. However, the theory of statistical inference is built on top of <strong>probability theory</strong>. And it is to probability theory that we must now turn. This discussion of probability theory is basically background: there’s not a lot of statistics per se in this chapter, and you don’t need to understand this material in as much depth as in the other chapters in this part of the book. Nevertheless, because probability theory does underpin so much of statistics, it’s worth covering some of the basics.</p>
<div id="probabilitystats" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> How are probability and statistics different?<a href="probability.html#probabilitystats" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before discussing probability theory, it’s helpful to consider the relationship between probability and statistics. The two disciplines are closely related, but they’re not identical. Probability theory is “the doctrine of chances”. It’s a branch of mathematics that tells you how often different events will happen. For example, all of these questions are things you can answer using probability theory:</p>
<ul>
<li>What are the chances of a fair coin coming up heads 10 times in a row?</li>
<li>If we roll two six-sided dice, how likely is it that we’ll roll two sixes?</li>
<li>How likely is it that five cards drawn from a perfectly shuffled deck will all be hearts?</li>
<li>What are the chances that we’ll win the lottery?</li>
</ul>
<p>Notice that all of these questions have something in common. In each case the “truth of the world” is known, and my question relates to the “what kind of events” will happen. In the first question, we <em>know</em> that the coin is fair, so there’s a 50% chance that any individual coin flip will come up heads. In the second question, we <em>know</em> that the chance of rolling a 6 on a single die is 1 in 6. In the third question, we <em>know</em> that the deck is shuffled properly. And in the fourth question, we <em>know</em> that the lottery follows specific rules. You get the idea. The critical point is that probabilistic questions start with a known <strong><em>model</em></strong> of the world, and we use that model to do some calculations. The underlying model can be quite simple. For instance, in the coin flipping example, we can write down the model like this:
<span class="math display">\[
P(\mbox{heads}) = 0.5
\]</span>
which you can read as “the probability of heads is 0.5”. As we’ll see later, in the same way, percentages are numbers that range from 0% to 100%, and probabilities are just numbers that range from 0 to 1. When using this probability model to answer the first question, we don’t know exactly what will happen. Maybe we’ll get 10 heads like the question says. But perhaps we’ll get three heads. That’s the key thing: in probability theory, the <em>model</em> is known, but the <em>data</em> are not.</p>
<p>So that’s probability. What about statistics? Statistical questions work the other way around. In statistics, we <em>do not</em> know the truth about the world. All we have is the data from which we want to <em>learn</em> the truth about the world. Statistical questions tend to look more like these:</p>
<ul>
<li>If a friend flips a coin 10 times and gets 10 heads, are they playing a trick?</li>
<li>If five cards off the top of the deck are all hearts, how likely is it that the deck was shuffled?</li>
<li>If the lottery commissioner’s spouse wins the lottery, how likely is it that the lottery was rigged?</li>
</ul>
<p>This time around, the only thing we have are data. What we <em>know</em> is that we saw a friend flip the coin 10 times, and it came up heads every time. And what we want to <strong>infer</strong> is whether or not we should conclude that what we just saw was actually a fair coin being flipped 10 times in a row, or whether we should suspect that the friend is playing a trick on us. The data we have look like this:</p>
<pre><code>H H H H H H H H H H H</code></pre>
<p>and what we’re trying to do is work out which “model of the world” we should put my trust in. If the coin is fair, then the model we should adopt is one that says that the probability of heads is 0.5; that is, <span class="math inline">\(P(\mbox{heads}) = 0.5\)</span>. If the coin is not fair, then we should conclude that the probability of heads is <em>not</em> 0.5, which we would write as <span class="math inline">\(P(\mbox{heads}) \neq 0.5\)</span>. In other words, the statistical inference problem is to figure out which of these probability models is right. Clearly, the statistical question isn’t the same as the probability question, but they’re deeply connected to one another. Because of this, a good introduction to statistical theory will start with a discussion of what probability is and how it works.</p>
</div>
<div id="probabilitymeaning" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> What does probability mean?<a href="probability.html#probabilitymeaning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s start with the first of these questions. What is “probability”? It might seem surprising to you, but while statisticians and mathematicians (mostly) agree on what the <em>rules</em> of probability are, there’s much less of a consensus on what the word really <em>means</em>. It seems weird because we’re all very comfortable using words like “chance”, “likely”, “possible”, and “probable”, and it doesn’t seem like it should be a complicated question to answer. If you had to explain “probability” to a five-year-old, you could do a pretty good job. But if you’ve ever had that experience in real life, you might walk away from the conversation feeling like you didn’t quite get it right and that (like many everyday concepts) it turns out that you don’t <em>really</em> know what it’s all about.</p>
<p>Let’s suppose we want to bet on a soccer game between two teams of robots, <em>Arduino Arsenal</em> and <em>C Milan</em>. After thinking about it, we decide that there is an 80% probability that <em>Arduino Arsenal</em> winning. What do we mean by that? Here are three possibilities:</p>
<ul>
<li>They’re robot teams, so we can make them play over and over again, and if we did that, <em>Arduino Arsenal</em> would win 8 out of every 10 games on average.</li>
<li>For any given game, we would only agree that betting on this game is only “fair” if a $1 bet on <em>C Milan</em> gives a $5 payoff (i.e. we get $1 back plus a $4 reward for being correct), as would a $4 bet on <em>Arduino Arsenal</em> (i.e., $4 bet plus a $1 reward).</li>
<li>Our subjective “belief” or “confidence” in an <em>Arduino Arsenal</em> victory is four times as strong as our belief in a <em>C Milan</em> victory.</li>
</ul>
<p>Each of these seems sensible. However, they’re not identical, and not every statistician would endorse all of them. The reason is that there are different statistical ideologies (yes, really!) and depending on which one you subscribe to, you might say that some of those statements are meaningless or irrelevant. In this section, we give a brief introduction to the two main approaches that exist in the literature. These are by no means the only approaches, but they’re the two big ones.</p>
<div id="the-frequentist-view" class="section level3 hasAnchor" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> The frequentist view<a href="probability.html#the-frequentist-view" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first of the two major approaches to probability, and the more dominant one in statistics, is referred to as the <strong><em>frequentist view</em></strong>, which defines probability as a <strong><em>long-run frequency</em></strong>. Suppose we were to try flipping a fair coin over and over again. By definition, this is a coin that has <span class="math inline">\(P(H) = 0.5\)</span>. What might we observe? One possibility is that the first 20 flips might look like this:</p>
<pre><code>T,H,H,H,H,T,T,H,H,H,H,T,H,H,T,T,T,T,T,H</code></pre>
<p>In this case, 11 of these 20 coin flips (55%) came up heads. Now suppose that we’d been keeping a running tally of the number of heads (which we’ll call <span class="math inline">\(N_H\)</span>) that we’ve seen, across the first <span class="math inline">\(N\)</span> flips, and calculate the proportion of heads <span class="math inline">\(N_H / N\)</span> every time. Here’s what we’d get:</p>
<table>
<thead>
<tr>
<th style="text-align:right;">
number.of.flips
</th>
<th style="text-align:right;">
number.of.heads
</th>
<th style="text-align:right;">
proportion
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.00
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.50
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0.67
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.75
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.80
</td>
</tr>
<tr>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.67
</td>
</tr>
<tr>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.57
</td>
</tr>
<tr>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.63
</td>
</tr>
<tr>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
0.67
</td>
</tr>
<tr>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
0.70
</td>
</tr>
<tr>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
0.73
</td>
</tr>
<tr>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
0.67
</td>
</tr>
<tr>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
0.69
</td>
</tr>
<tr>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.71
</td>
</tr>
<tr>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.67
</td>
</tr>
<tr>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.63
</td>
</tr>
<tr>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.59
</td>
</tr>
<tr>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.56
</td>
</tr>
<tr>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.53
</td>
</tr>
<tr>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
0.55
</td>
</tr>
</tbody>
</table>
<p>Notice that at the start of the sequence, the <em>proportion</em> of heads fluctuates wildly, starting at .00 and rising as high as .80. Later on, one gets the impression that it dampens out a bit, with more and more of the values actually being pretty close to the “right” answer of .50. This is the frequentist definition of probability in a nutshell: flip a fair coin over and over again, and as <span class="math inline">\(N\)</span> grows large (approaches infinity, denoted <span class="math inline">\(N\rightarrow \infty\)</span>), the proportion of heads will converge to 50%. There are some subtle technicalities that mathematicians care about, but qualitatively speaking, that’s how the frequentists define probability. Unfortunately, we don’t have an infinite number of coins, or the infinite patience required to flip a coin an infinite number of times. However, we do have a computer, and computers excel at mindless repetitive tasks. So let’s ask a computer to simulate flipping a coin 1000 times, and then draw a picture of what happens to the proportion <span class="math inline">\(N_H / N\)</span> as <span class="math inline">\(N\)</span> increases. The results are shown in Figure <a href="probability.html#fig:frequentistprobability">5.1</a>. As you can see, the <em>proportion of observed heads</em> eventually stops fluctuating, and settles down; when it does, the number at which it finally settles is the true probability of heads.</p>
<div class="figure"><span style="display:block;" id="fig:frequentistprobability"></span>
<img src="lsc_files/figure-html/frequentistprobability-1.svg" alt="An illustration of how frequentist probability works. If you flip a fair coin over and over again, the proportion of heads that you've seen eventually settles down, and converges to the true probability of 0.5. Each panel shows four different simulated experiments: in each case, we pretend we flipped a coin 1000 times, and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of .5, if we'd extended the experiment for an infinite number of coin flips they would have." width="672" />
<p class="caption">
Figure 5.1: An illustration of how frequentist probability works. If you flip a fair coin over and over again, the proportion of heads that you’ve seen eventually settles down, and converges to the true probability of 0.5. Each panel shows four different simulated experiments: in each case, we pretend we flipped a coin 1000 times, and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of .5, if we’d extended the experiment for an infinite number of coin flips they would have.
</p>
</div>
<p>The frequentist definition of probability has some desirable characteristics. Firstly, it is objective: the probability of an event is <em>necessarily</em> grounded in the world. The only way that probability statements can make sense is if they refer to (a sequence of) events that occur in the physical universe.<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a> Secondly, it is unambiguous: any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer. However, it also has undesirable characteristics. Firstly, infinite sequences don’t exist in the physical world. Suppose you picked up a coin from your pocket and started to flip it. Every time it lands, it impacts the ground. Each impact wears the coin down a bit; eventually, the coin will be destroyed. So, one might ask whether it really makes sense to pretend that an “infinite” sequence of coin flips is even a meaningful concept or an objective one. We can’t say that an “infinite sequence” of events is a real thing in the physical universe because the physical universe doesn’t allow infinite anything. More seriously, the frequentist definition has a narrow scope. There are many things out there that human beings are happy to assign a probability to in everyday language but cannot (even in theory) be mapped onto a hypothetical sequence of events. For instance, if a meteorologist comes on TV and says, “the probability of rain in Adelaide on 2 November 2048 is 60%”, we humans are happy to accept this. But it’s not clear how to define this in frequentist terms. There’s only one city of Adelaide, and only 2 November 2048. There’s no infinite sequence of events here, just a once-off thing. Frequentist probability genuinely <em>forbids</em> us from making probability statements about a single event. From the frequentist perspective, it will either rain tomorrow or not; there is no “probability” that attaches to a single non-repeatable event. Now, it should be said that there are some very clever tricks that frequentists can use to get around this. One possibility is that what the meteorologist means is something like this: “There is a category of days for which we predict a 60% chance of rain; if we look only across those days for which we make this prediction, then on 60% of those days it will actually rain”. It’s very weird and counterintuitive to think of it this way, but you do see frequentists do this sometimes. And it <em>will</em> come up later in this book (see Section <a href="#ci"><strong>??</strong></a>).</p>
</div>
<div id="the-bayesian-view" class="section level3 hasAnchor" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> The Bayesian view<a href="probability.html#the-bayesian-view" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Bayesian view</strong> of probability is often called the <em>subjectivist view</em>, and it is a minority view among statisticians but one that has been steadily gaining traction for the last several decades. There are many flavours of Bayesianism, making it hard to say what “the” Bayesian view is. The most common way of thinking about subjective probability is to define the probability of an event as the <strong>degree of belief</strong> that an intelligent and rational agent assigns to the truth of that event. From that perspective, probabilities don’t exist in the world but rather in the thoughts and assumptions of people and other intelligent beings.
However, for this approach to work, we need some way of operationalising the “degree of belief”. One way you can do this is to formalise it in terms of “rational gambling”, though there are many other ways. Suppose that we believe that there’s a 60% probability of rain tomorrow. If someone offers us a bet: if it rains tomorrow, then we win $5, but if it doesn’t rain, then we lose $5. Clearly, from our perspective, this is a pretty good bet. On the other hand, if we think the probability of rain is only 40%, then it’s a bad bet. Thus, we can operationalise the notion of a “subjective probability” in terms of what bets we’re willing to accept.</p>
<p>What are the advantages and disadvantages of the Bayesian approach? The main advantage is that it allows you to assign probabilities to any event you want to. You don’t need to be limited to those events that are repeatable. The main disadvantage (to many people) is that we can’t be purely objective – specifying a probability requires us to specify an entity with the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician, but there has to be an intelligent agent out there that believes in things. To many people, this is uncomfortable: it seems to make probability arbitrary. While the Bayesian approach does require that the agent in question be rational (i.e., obey the rules of probability), it does allow everyone to have their own beliefs; I can believe the coin is fair, and you don’t have to, even though we’re both rational. The frequentist view doesn’t allow any two observers to attribute different probabilities to the same event: when that happens, then at least one of them must be wrong. The Bayesian view does not prevent this from occurring. Two observers with different background knowledge can legitimately hold different beliefs about the same event. In short, where the frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to), the Bayesian view is sometimes thought to be too broad (allows too many differences between observers).</p>
</div>
<div id="whats-the-difference-and-who-is-right" class="section level3 hasAnchor" number="5.2.3">
<h3><span class="header-section-number">5.2.3</span> What’s the difference? And who is right?<a href="probability.html#whats-the-difference-and-who-is-right" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that you’ve seen these two views independently, it’s useful to make sure you can compare them. Go back to the hypothetical robot soccer game at the start of the section. What would a frequentist and a Bayesian say about these three statements? Which statement would a frequentist say is the correct definition of probability? Which one would a Bayesian do? Would some of these statements be meaningless to a frequentist or a Bayesian? If you’ve understood the two perspectives, you should have some sense of how to answer those questions.</p>
<p>Okay, assuming you understand the difference, you might be wondering which of them is <em>right</em>? Honestly, we don’t know that there is a right answer. As far as we can tell, there’s nothing mathematically incorrect about the way frequentists think about sequences of events, and there’s nothing mathematically incorrect about the way that Bayesians define the beliefs of a rational agent. In fact, when you dig down into the details, Bayesians and frequentists actually agree about a lot of things. Many frequentist methods lead to decisions that Bayesians agree a rational agent would make. Many Bayesian methods have very good frequentist properties.</p>
<p>Consider Sir Ronald Fisher, one of the towering figures of 20th-century statistics and a vehement opponent of all things Bayesian, whose paper on the mathematical foundations of statistics referred to Bayesian probability as “an impenetrable jungle [that] arrests progress towards precision of statistical concepts” <span class="citation">Fisher (1922)</span>. Or the psychologist Paul Meehl, who suggests that relying on frequentist methods could turn you into “a potent but sterile intellectual rake who leaves in his merry path a long train of ravished maidens but no viable scientific offspring” <span class="citation">Meehl (1967)</span>. The history of statistics, as you might gather, is not devoid of entertainment.</p>
<p>In any case, while Danielle personally prefers the Bayesian view, the majority of statistical analyses are based on the frequentist approach. My reasoning is pragmatic: the goal of this book is to cover roughly the same territory as a typical undergraduate stats class in psychology, and if you want to understand the statistical tools used by most psychologists, you’ll need a good grasp of frequentist methods. We promise you that this isn’t a wasted effort. Even if you end up wanting to switch to the Bayesian perspective, you really should read through at least one book on the “orthodox” frequentist view. Every now and then, we’ll add some commentary from a Bayesian point of view, and we’ll revisit the topic in more depth in Chapter <a href="#bayes"><strong>??</strong></a>.</p>
</div>
</div>
<div id="basicprobability" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Basic probability theory<a href="probability.html#basicprobability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Although there are ideological arguments between Bayesians and frequentists, it turns out that people mostly agree on the rules that probabilities should obey. There are lots of different ways of arriving at these rules. The most commonly used approach is based on the work of Andrey Kolmogorov, one of the great Soviet mathematicians of the 20th century. Without going into a lot of detail, we’ll try to give you a sense of how it works.</p>
<div id="introducing-probability-distributions" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Introducing probability distributions<a href="probability.html#introducing-probability-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s assume we own five pairs of pants: three pairs of jeans, the bottom half of a suit, and a pair of tracksuit pants. Let’s call them <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_3\)</span>, <span class="math inline">\(X_4\)</span> and <span class="math inline">\(X_5\)</span>. Now, on any given day, we pick out exactly one pair of pants. If we were to describe this situation using the language of probability theory, we would refer to each pair of pants (i.e., each <span class="math inline">\(X\)</span>) as an <strong>elementary event</strong>. The key characteristic of elementary events is that every time we make an observation (e.g., every time we put on a pair of pants), then the outcome will be one and only one of these events. As said, we only wear exactly one pair of pants, so it satisfies this constraint. Similarly, the set of all possible events is called a <strong>sample space</strong>.</p>
<p>Okay, now that we have a sample space (a wardrobe) built from lots of possible elementary events (pants), we want to assign a <strong>probability</strong> of one of these elementary events. For an event <span class="math inline">\(X\)</span>, the probability of that event <span class="math inline">\(P(X)\)</span> is a number that lies between 0 and 1. The bigger the value of <span class="math inline">\(P(X)\)</span>, the more likely the event will occur. So, for example, if <span class="math inline">\(P(X) = 0\)</span>, it means the event <span class="math inline">\(X\)</span> is impossible (i.e., we never wear those pants). On the other hand, if <span class="math inline">\(P(X) = 1\)</span> it means that event <span class="math inline">\(X\)</span> is certain to occur (i.e., we always wear those pants). For probability values in the middle, it means that we sometimes wear those pants. For instance, if <span class="math inline">\(P(X) = 0.5\)</span>, it means that we wear those pants half of the time.</p>
<p>At this point, we’re almost done. The last thing we need to recognise is that “something always happens”. Every time we put on pants, we really do end up wearing pants (crazy, right?). What this somewhat trite statement means, in probabilistic terms, is that the probabilities of the elementary events need to add up to 1. This is known as the <strong>law of total probability</strong>. More importantly, if these requirements are satisfied, then we have a <strong>probability distribution</strong>. For example, this is an example of a probability distribution</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
Which.pants
</th>
<th style="text-align:left;">
Blue.jeans
</th>
<th style="text-align:left;">
Grey.jeans
</th>
<th style="text-align:left;">
Black.jeans
</th>
<th style="text-align:left;">
Black.suit
</th>
<th style="text-align:left;">
Blue.tracksuit
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Label
</td>
<td style="text-align:left;">
<span class="math inline">\(X_1\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(X_2\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(X_3\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(X_4\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(X_5\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
Probability
</td>
<td style="text-align:left;">
<span class="math inline">\(P(X_1) = .5\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(P(X_2) = .3\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(P(X_3) = .1\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(P(X_4) = 0\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(P(X_5) = .1\)</span>
</td>
</tr>
</tbody>
</table>
<p>Each of the events has a probability that lies between 0 and 1, and if we add up the probability of all events, they sum to 1. Awesome. We can even draw a nice bar graph (see Section <a href="#bargraph"><strong>??</strong></a>) to visualise this distribution, as shown in Figure <a href="probability.html#fig:pantsprob">5.2</a>. And at this point, we’ve all achieved something. You’ve learned what probability distribution is.</p>
<div class="figure"><span style="display:block;" id="fig:pantsprob"></span>
<img src="lsc_files/figure-html/pantsprob-1.svg" alt="A visual depiction of the &quot;pants&quot; probability distribution. There are five &quot;elementary events&quot;, corresponding to the five pairs of pants. Each event has some probability of occurring: this probability is a number between 0 to 1. The sum of these probabilities is 1." width="672" />
<p class="caption">
Figure 5.2: A visual depiction of the “pants” probability distribution. There are five “elementary events”, corresponding to the five pairs of pants. Each event has some probability of occurring: this probability is a number between 0 to 1. The sum of these probabilities is 1.
</p>
</div>
<p>The only other thing that needs pointing out is that probability theory allows you to talk about <strong>non-elementary events</strong> as well as elementary ones. The easiest way to illustrate the concept is with an example. In the pants example, it’s legitimate to refer to the probability that we wear jeans. In this scenario, the “We wear jeans” event is said to have happened as long as the elementary event that actually did occur is one of the appropriate ones; in this case “blue jeans”, “black jeans”, or “grey jeans”. In mathematical terms, we defined the “jeans” event <span class="math inline">\(E\)</span> to correspond to the set of elementary events <span class="math inline">\((X_1, X_2, X_3)\)</span>. If any of these elementary events occurs, then <span class="math inline">\(E\)</span> is also said to have occurred. Having decided to write down the definition of the <span class="math inline">\(E\)</span> this way, it’s pretty straightforward to state what the probability <span class="math inline">\(P(E)\)</span> is: we just add everything up. In this particular case
<span class="math display">\[
P(E) = P(X_1) + P(X_2) + P(X_3)
\]</span>
and, since the probabilities of blue, grey and black jeans respectively are .5, .3 and .1, the probability that we wear jeans is equal to .9.</p>
<p>At this point, you might be thinking that this is all terribly obvious and simple, and you’d be right. All we’ve done is wrap some basic mathematics around a few common sense intuitions. However, from these simple beginnings, it’s possible to construct some extremely powerful mathematical tools. Without going into the details in this book, we list – in Table <a href="probability.html#tab:probrules">5.1</a> – some of the other rules that probabilities satisfy. These rules can be derived from the simple assumptions that we’ve outlined above.</p>
<table>
<caption>
<span id="tab:probrules">Table 5.1: </span>Some basic rules that probabilities must satisfy. You don’t really need to know these rules in order to understand the analyses that we’ll talk about later in the book, but they are important if you want to understand probability theory a bit more deeply.
</caption>
<thead>
<tr>
<th style="text-align:left;">
English
</th>
<th style="text-align:left;">
Notation
</th>
<th style="text-align:left;">
NANA
</th>
<th style="text-align:left;">
Formula
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Not <span class="math inline">\(A\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(P(\neg A)\)</span>
</td>
<td style="text-align:left;">
=
</td>
<td style="text-align:left;">
<span class="math inline">\(1-P(A)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(P(A \cup B)\)</span>
</td>
<td style="text-align:left;">
=
</td>
<td style="text-align:left;">
<span class="math inline">\(P(A) + P(B) - P(A \cap B)\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(P(A \cap B)\)</span>
</td>
<td style="text-align:left;">
=
</td>
<td style="text-align:left;">
<span class="math inline">\(P(A|B) P(B)\)</span>
</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="16">
<li id="fn16"><p>This doesn’t mean that frequentists can’t make hypothetical statements, of course; it’s just that if you want to make a statement about probability, then it must be possible to redescribe that statement in terms of a sequence of potentially observable events, and the relative frequencies of different outcomes that appear within that sequence.<a href="probability.html#fnref16" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="exploring-a-variable.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="distributions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"number_sections": true,
"fig_caption": true
},
"toc_depth": 2,
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
