# (PART\*) UNDERSTANDING YOUR DATA{-}

# Exploring a variable

Any time you get a new data set to look at, one of the first things you might want to do is find ways of summarising the data in a compact, easily understood fashion. This is what **_descriptive statistics_** (as opposed to *inferential statistics*) is all about. In fact, to many people, the term "statistics" is synonymous with descriptive statistics.

The first dataset we'll be looking at is real data relating to the Australian Football League (AFL)^[Note for non-Australians: the AFL is an Australian rules football competition. You don't need to know anything about Australian rules in order to follow this section.]. To do this, let us load the [aflsmall.Rdata](resources/data/aflsmall.Rdata) file.

```{r loadaflsmall, out.width='70%', fig.align='center',echo=FALSE, fig.cap="This is what you would see after loading the dataset."}
knitr::include_graphics("resources/load_aflsmall.png")
```

CogStat will help you get familiar with some essential aspects of your variable, like:

- measures of central tendency (mean, median)
- measures of variability (range, minimum, maximum, standard deviation, quartiles)
- measures of "distortion" (skewness, kurtosis).

These will help you contextualise the results so the conclusions drawn from the variable will be valid.

To start understanding a variable, select
`Explore variable` so a pop-up appears. Move the name of the data you wish to analyse (in this case: `afl.margins`) from `Available variables` to `Selected variables`, then click `OK`.

```{r rawaflsmall, out.width='70%', fig.align='center',echo=FALSE, fig.cap="This is the first chart you will see exploring the raw shape of the data."}
knitr::include_graphics("resources/raw_aflsmall.png")
```

The first piece of information here is $N$, which we will use to refer to the number of observations we're analysing. CogStat (or any other software for that matter) will only use valid data for calculations. Sometimes, mainly when working with survey data, you will have missing data points, the number of which you might have to mention in your written analysis. CogStat will quote these for you:

> `N of valid cases: 176`\
> `N of missing cases: 0`

```{r histogramaflsmall, out.width='70%', fig.align='center',echo=FALSE, fig.cap="Scrolling down, you'll see CogStat reporting all the descriptive measures while showing you a histogram to understand the shape of your data better. Drawing pictures of the data is an excellent way to convey the gist of what the data is trying to tell you; it's often instrumental to try to condense the data into a few simple summary statistics."}
knitr::include_graphics("resources/histogram_aflsmall.png")
```

In the rest of this chapter, we will explore what these measures mean and what they indicate.

## Measures of central tendency{#centraltendency}

In most situations, the first thing that you'll want to calculate is a measure of **_central tendency_**. That is, you'd like to know something about the "average" or "middle" of your data lies.

### The mean{#mean}

The **mean** of a set of observations is just a normal, old-fashioned (arithmetic) average: add all of the values up and then divide by the total number of values. The first five AFL margins were 56, 31, 56, 8 and 32 (which CogStat will quote to you when loading the data, see Figure \@ref(fig:loadaflsmall)), so the mean of these observations is just:
$$
\frac{56 + 31 + 56 + 8 + 32}{5} = \frac{183}{5} = 36.60
$$
Of course, this definition of the mean isn't news to anyone: averages (i.e. means) are used so often in everyday life that this is quite familiar.

We used $N$ to denote the number of observations. Now let's attach a label to the observations themselves. It's traditional to use $X$ for this, and to use subscripts to indicate which observation we're actually talking about. That is, we'll use $X_1$ to refer to the first observation, $X_2$ to refer to the second observation, and so on, all the way up to $X_N$ for the last one. The following table lists the 5 observations in the `afl.margins` variable, along with the mathematical symbol used to refer to it, and the actual value that the observation corresponds to:

```{r echo=FALSE}
knitr::kable(rbind(
              c("winning margin, game 1", "$X_1$", "56 points"),
              c("winning margin, game 2", "$X_2$", "31 points"),
              c("winning margin, game 3", "$X_3$", "56 points"),
              c("winning margin, game 4", "$X_4$", "8 points"),
              c("winning margin, game 5", "$X_5$", "32 points")),
col.names = c("the observation", "its symbol", "the observed value"),
  booktabs = TRUE)

```

Okay, now let's try to write a formula for the mean. By tradition, we use $\bar{X}$ as the notation for the mean. So the calculation for the mean could be expressed using the following formula:
$$
\bar{X} = \frac{X_1 + X_2 + ... + X_{N-1} + X_N}{N}
$$
This formula is entirely correct, but it's terribly long, so we make use of the **_summation symbol_** $\scriptstyle\sum$ to shorten it.^[The choice to use $\Sigma$ to denote summation isn't arbitrary: it's the Greek upper case letter sigma, which is the analogue of the letter S in that alphabet. Similarly, there's an equivalent symbol used to denote the multiplication of lots of numbers: because multiplications are also called "products", we use the $\Pi$ symbol for this; the Greek upper case pi, which is the analogue of the letter P.] If I want to add up the first five observations, I could write out the sum the long way, $X_1 + X_2 + X_3 + X_4 +X_5$ or I could use the summation symbol to shorten it to this:
$$
\sum_{i=1}^5 X_i
$$
Taken literally, this could be read as "the sum, taken over all $i$ values from 1 to 5, of the value $X_i$". But basically, what it means is "add up the first five observations". In any case, we can use this notation to write out the formula for the mean, which looks like this:
$$
\bar{X} = \frac{1}{N} \sum_{i=1}^N X_i 
$$

In all honesty, all this mathematical notation is just a fancy way of laying out the same things said in words: *add all the values up, and then divide by the total number of items*. The goal here was to try to make sure that everyone reading this book is clear on the notation that we'll be using throughout the book: $\bar{X}$ for the mean, $\scriptstyle\sum$ for the idea of summation, $X_i$ for the $i$th observation, and $N$ for the total number of observations. We're going to be re-using these symbols a fair bit, so you must understand them well enough to be able to "read" the equations and to be able to see what they're really saying.

CogStat calculates the mean automatically when exploring a variable using all valid data points, not just the first five. It will be part of the `Descriptives for the variable` section, as seen in Figure \@ref(fig:histogramaflsmall). The result for our variable is:

> `afl.margins`\
> `Mean 35.3`

### The median{#median}

The second measure of central tendency people use a lot is the **median**, and it's even easier to describe than the mean. The median of a set of observations is just the middle value^[*medianus* is Latin for "the one in the middle", originating from the word *medius*, meaning "the middle".]. As before, let's imagine we were interested only in the first 5 AFL winning margins: 56, 31, 56, 8 and 32. To figure out the median, we sort these numbers into ascending order:

$$
8, 31, \mathbf{32}, 56, 56
$$

From inspection, it's evident that the median value of these five observations is 32 since that's the middle one in the sorted list. Easy stuff. But what should we do if we were interested in the first six games rather than the first 5? Since the sixth game in the season had a winning margin of 14 points, our sorted list is now

$$
8, 14, \mathbf{31}, \mathbf{32}, 56, 56
$$

and there are *two* middle numbers, 31 and 32. The median is defined as the average of those two numbers, which is 31.5.

In the data set we loaded to CogStat, there were 176 valid cases, so we ought to have two middle numbers. The result in this case is (as seen in Figure \@ref(fig:histogramaflsmall)):

> `afl.margins`\
> `Median 30.5`

### Mean or median? What's the difference?

```{r meanmedian, out.width='70%', fig.align='center', echo=FALSE, fig.cap="An illustration of the difference between how the mean and the median should be interpreted. The mean is basically the \"centre of gravity\" of the data set: if you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation. Half of the observations are smaller, and half of the observations are larger."}
knitr::include_graphics("./resources/meanmedian.png")
```

Knowing how to calculate means and medians is only a part of the story. You also need to understand what each one is saying about the data, what that implies, and which one to choose. This is illustrated in Figure \@ref(fig:meanmedian); the mean is kind of like the "centre of gravity" of the data set, whereas the median is the "middle value" in the data. What this implies about which one you should use depends a little on what type of data you've got and what you're trying to achieve. As a rough guide:
 
- If your data are *[nominal scale](#nominalscale)*, you probably shouldn't be using either the mean or the median. Both the mean and the median rely on the idea that the numbers assigned to values are meaningful. If the numbering scheme is arbitrary, then it's probably best to use the mode (Section \@ref(mode)) instead.
- If your data are *[ordinal scale](#ordinalscale)*, you're more likely to want to use the median than the mean. The median only uses the order information in your data (i.e., which numbers are larger) but doesn't depend on the precise numbers involved. That's precisely the situation that applies when your data are ordinal scale. Conversely, the mean makes use of the precise numeric values assigned to the observations, so it's not appropriate for ordinal data.
- For *[interval](#intervalscale)* and *[ratio scale](#ratioscale)* data, either one is generally acceptable. Which one you pick depends a bit on what you're trying to achieve. The mean has the advantage of using all the information in the data (which is useful when you don't have a lot of data), but it's very susceptible to extreme values, as we'll see in Section \@ref(trimmedmean).

Let's expand on that last part a little. One consequence is that there are systematic differences between the mean and the median when the histogram is asymmetric (skewed; see Section \@ref(skewandkurtosis)). This is illustrated in Figure \@ref(fig:meanmedian) notice that the median (right hand side) is located closer to the "body" of the histogram, whereas the mean (left hand side) gets dragged towards the "tail" (where the extreme values are). To give a concrete example, suppose Bob (income \$50,000), Kate (income \$60,000) and Jane (income \$65,000) are sitting at a table: the average income at the table is \$58,333 and the median income is \$60,000. Then Bill sits down with them (income \$100,000,000). The average income has now jumped to \$25,043,750 but the median rises only to \$62,500. If you're interested in looking at the overall income at the table, the mean might be the right answer; but if you're interested in what counts as a typical income at the table, the median would be a better choice here.

### Trimmed mean{#trimmedmean}

One of the fundamental rules of applied statistics is that the data are messy. Real life is never simple, so the data sets you obtain are never as straightforward as the statistical theory says.^[Or at least, the basic statistical theory. These days, a whole subfield of statistics called *robust statistics* tries to grapple with the messiness of real data and develop the theory that can cope with it.] This can have awkward consequences. To illustrate, consider this rather strange-looking data set:
$$
-100,2,3,4,5,6,7,8,9,10
$$
If you were to observe this in a real-life data set, you'd probably suspect that there is something odd about the $-100$ value. It's probably an **outlier**, a value that doesn't belong with the others. You might consider removing it from the data set entirely. In this particular case, it might be the right call. In real life, however, you don't always get such cut-and-dried examples. For instance, you might get this instead:
$$
-15,2,3,4,5,6,7,8,9,12
$$
The $-15$ looks suspicious, but not as much as that $-100$ did. In this case, it's a little trickier. It *might* be a legitimate observation; it might not.

When faced with a situation where some of the most extreme-valued observations might not be quite trustworthy, the mean is not necessarily a good measure of central tendency. It is highly sensitive to one or two extreme values and thus is not considered a *robust* measure. One remedy that we've seen is to use the median. A more general solution is to use a "trimmed mean". To calculate a trimmed mean, what you do is "discard" the most extreme examples on both ends (i.e., the largest and the smallest) and then take the mean of everything else. The goal is to preserve the best characteristics of the mean and the median: just like a median, you aren't highly influenced by extreme outliers, but like the mean, you "use" more than one of the observations.

Generally, we describe a trimmed mean in terms of the percentage of observations on either side that are discarded. So, for instance, a 10% trimmed mean discards the largest 10% of the observations *and* the smallest 10% of the observations and then takes the mean of the remaining 80% of the observations. Not surprisingly, the 0% trimmed mean is just the regular mean, and the 50% trimmed mean is the median. In that sense, trimmed means provide a whole family of central tendency measures that span the range from the mean to the median.

For our toy example above, we have 10 observations. So a 10% trimmed mean is calculated by ignoring the largest value (i.e. `12`) and the smallest value (i.e. `-15`) and taking the mean of the remaining values. 

> `Mean: 4.1`\
> `Median: 5.5`

That's a fairly substantial difference. But the mean is being influenced too much by the extreme values at either end of the data set, especially the $-15$ one.  If we take a 10% trimmed mean, we'll drop the extreme values on either side and take the mean of the rest: 

> `Mean: 5.5`

Which, in this case, gives exactly the same answer as the median.

Currently, there is no direct way for you to do that in CogStat, but you can certainly trim those outlying data points in your source file and re-load the data.

### Mode{#mode}

The **mode** of a sample is straightforward: it is the value that occurs most frequently.

In CogStat, you will see a **frequency table** (Figure \@ref(fig:freqaflsmall)) of the values in your data if you have `Frequencies` ticked in the `Explore variable` dialogue.

```{r freqaflsmall, out.width='70%', fig.align='center',echo=FALSE, fig.cap="The frequency table sorts non-nominal values from lowest to highest."}
knitr::include_graphics("resources/frequency_aflsmall.png")
```

While it's generally true that the mode is most often calculated when you have [nominal scale](#nominalscale) data -- because means and medians are useless for those sorts of variables --, there are some situations in which you do want to know the mode of an [ordinal](#ordinalscale), [interval](#intervalscale) or [ratio scale](#ratioscale) variable.

For instance, let's look at our `afl.margins` variable we loaded into CogStat. This variable is clearly ratio scale, and so in most situations the mean or the median is the measure of central tendency that you want. But consider this scenario... a friend of yours is offering a bet. They pick a football game at random, and (without knowing who is playing) you have to guess the *exact* margin. If you guess correctly, you win \$50. If you don't, you lose \$1. There are no consolation prizes for "almost" getting the right answer. You have to guess exactly the right margin^[This is called a "0-1 loss function", meaning that you either win (1) or you lose (0), with no middle ground.] For this bet, the mean and the median are completely useless to you. It is the mode that you should bet on. So, we look at the frequency table offered by the result set: the data suggest you should bet on a `3.0` point margin, and since this was observed in 8 of the 176 games (4.5% of games -- the *relative frequency*), the odds are firmly in your favour.

## Measures of variability{#var}

### Range{#range}

### Interquartile range

### Mean absolute deviation{#aad}

### Variance

### Standard deviation{#sd}

### Median absolute deviation{#mad}

### Which measure to use?

## Skew and kurtosis{#skewandkurtosis}

## Summary: descriptives

As you've seen in this chapter...

```{r echo=FALSE}
knitr::kable(rbind(
              c("Mean", "35.3"),
              c("Standard deviation", ""),
              c("Skewness", ""),
              c("Kurtosis", ""),
              c("Range", ""),
              c("Maximum", ""),
              c("Upper quartile", ""),
              c("Median", "30.5"),
              c("Lower quartile", ""),
              c("Minimum", "")
              ),
  col.names = c("", "afl.margins"),
  booktabs = TRUE,
  caption = 'Descriptives for the variable'
  )

```
