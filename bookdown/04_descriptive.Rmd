# (PART\*) UNDERSTANDING YOUR DATA{-}

# Exploring a variable

Any time you get a new data set to look at, one of the first things you might want to do is find ways of summarising the data in a compact, easily understood fashion. This is what **_descriptive statistics_** (as opposed to *inferential statistics*) is all about. In fact, to many people, the term "statistics" is synonymous with descriptive statistics.

The first dataset we'll be looking at is real data relating to the Australian Football League (AFL)^[Note for non-Australians: the AFL is an Australian rules football competition. You don't need to know anything about Australian rules in order to follow this section.]. To do this, let us load the [aflsmall.Rdata](resources/data/aflsmall.Rdata) file.

```{r loadaflsmall, out.width='70%', fig.align='center',echo=FALSE, fig.cap="This is what you would see after loading the dataset."}
knitr::include_graphics("resources/load_aflsmall.png")
```

CogStat will help you get familiar with some essential aspects of your variable, like:

- measures of central tendency (mean, median)
- measures of variability (range, minimum, maximum, standard deviation, quartiles)
- measures of "distortion" (skewness, kurtosis).

These will help you contextualise the results so the conclusions drawn from the variable will be valid.

To start understanding a variable, select
`Explore variable` so a pop-up appears. Move the name of the data you wish to analyse (in this case: `afl.margins`) from `Available variables` to `Selected variables`, then click `OK`.

```{r rawaflsmall, out.width='70%', fig.align='center',echo=FALSE, fig.cap="This is the first chart you will see exploring the raw shape of the data."}
knitr::include_graphics("resources/raw_aflsmall.png")
```

The first piece of information here is $N$, which we will use to refer to the number of observations we're analysing. CogStat (or any other software for that matter) will only use valid data for calculations. Sometimes, mainly when working with survey data, you will have missing data points, the number of which you might have to mention in your written analysis. CogStat will quote these for you:

> `N of valid cases: 176`\
> `N of missing cases: 0`

```{r histogramaflsmall, out.width='70%', fig.align='center',echo=FALSE, fig.cap="Scrolling down, you'll see CogStat reporting all the descriptive measures while showing you a histogram to understand the shape of your data better. Drawing pictures of the data is an excellent way to convey the gist of what the data is trying to tell you; it's often instrumental to try to condense the data into a few simple summary statistics."}
knitr::include_graphics("resources/histogram_aflsmall.png")
```

In the rest of this chapter, we will explore what these measures mean and what they indicate.

## Measures of central tendency{#centraltendency}

In most situations, the first thing that you'll want to calculate is a measure of **_central tendency_**. That is, you'd like to know something about the "average" or "middle" of your data lies.

### The mean{#mean}

The **mean** of a set of observations is just a normal, old-fashioned (arithmetic) average: add all of the values up and then divide by the total number of values. The first five AFL margins were 56, 31, 56, 8 and 32 (which CogStat will quote to you when loading the data, see Figure \@ref(fig:loadaflsmall)), so the mean of these observations is just:
$$
\frac{56 + 31 + 56 + 8 + 32}{5} = \frac{183}{5} = 36.60
$$
Of course, this definition of the mean isn't news to anyone: averages (i.e. means) are used so often in everyday life that this is quite familiar.

We used $N$ to denote the number of observations. Now let's attach a label to the observations themselves. It's traditional to use $X$ for this, and to use subscripts to indicate which observation we're actually talking about. That is, we'll use $X_1$ to refer to the first observation, $X_2$ to refer to the second observation, and so on, all the way up to $X_N$ for the last one. The following table lists the 5 observations in the `afl.margins` variable, along with the mathematical symbol used to refer to it, and the actual value that the observation corresponds to:

```{r echo=FALSE}
knitr::kable(rbind(
              c("winning margin, game 1", "$X_1$", "56 points"),
              c("winning margin, game 2", "$X_2$", "31 points"),
              c("winning margin, game 3", "$X_3$", "56 points"),
              c("winning margin, game 4", "$X_4$", "8 points"),
              c("winning margin, game 5", "$X_5$", "32 points")),
col.names = c("the observation", "its symbol", "the observed value"),
  booktabs = TRUE)

```

Okay, now let's try to write a formula for the mean. By tradition, we use $\bar{X}$ as the notation for the mean. So the calculation for the mean could be expressed using the following formula:
$$
\bar{X} = \frac{X_1 + X_2 + ... + X_{N-1} + X_N}{N}
$$
This formula is entirely correct, but it's terribly long, so we make use of the **_summation symbol_** $\scriptstyle\sum$ to shorten it.^[The choice to use $\Sigma$ to denote summation isn't arbitrary: it's the Greek upper case letter sigma, which is the analogue of the letter S in that alphabet. Similarly, there's an equivalent symbol used to denote the multiplication of lots of numbers: because multiplications are also called "products", we use the $\Pi$ symbol for this; the Greek upper case pi, which is the analogue of the letter P.] If I want to add up the first five observations, I could write out the sum the long way, $X_1 + X_2 + X_3 + X_4 +X_5$ or I could use the summation symbol to shorten it to this:
$$
\sum_{i=1}^5 X_i
$$
Taken literally, this could be read as "the sum, taken over all $i$ values from 1 to 5, of the value $X_i$". But basically, what it means is "add up the first five observations". In any case, we can use this notation to write out the formula for the mean, which looks like this:
$$
\bar{X} = \frac{1}{N} \sum_{i=1}^N X_i 
$$

In all honesty, all this mathematical notation is just a fancy way of laying out the same things said in words: *add all the values up, and then divide by the total number of items*. The goal here was to try to make sure that everyone reading this book is clear on the notation that we'll be using throughout the book: $\bar{X}$ for the mean, $\scriptstyle\sum$ for the idea of summation, $X_i$ for the $i$th observation, and $N$ for the total number of observations. We're going to be re-using these symbols a fair bit, so you must understand them well enough to be able to "read" the equations and to be able to see what they're really saying.

CogStat calculates the mean automatically when exploring a variable using all valid data points, not just the first five. It will be part of the `Descriptives for the variable` section, as seen in Figure \@ref(fig:histogramaflsmall). The result for our variable is:

> `afl.margins`\
> `Mean 35.3`

### The median{#median}

The second measure of central tendency people use a lot is the **median**, and it's even easier to describe than the mean. The median of a set of observations is just the middle value^[*medianus* is Latin for "the one in the middle", originating from the word *medius*, meaning "the middle".]. As before, let's imagine we were interested only in the first 5 AFL winning margins: 56, 31, 56, 8 and 32. To figure out the median, we sort these numbers into ascending order:

$$
8, 31, \mathbf{32}, 56, 56
$$

From inspection, it's evident that the median value of these five observations is 32 since that's the middle one in the sorted list. Easy stuff. But what should we do if we were interested in the first six games rather than the first 5? Since the sixth game in the season had a winning margin of 14 points, our sorted list is now

$$
8, 14, \mathbf{31}, \mathbf{32}, 56, 56
$$

and there are *two* middle numbers, 31 and 32. The median is defined as the average of those two numbers, which is 31.5.

In the data set we loaded to CogStat, there were 176 valid cases, so we ought to have two middle numbers. The result in this case is (as seen in Figure \@ref(fig:histogramaflsmall)):

> `afl.margins`\
> `Median 30.5`

### Mean or median? What's the difference?

```{r meanmedian, out.width='70%', fig.align='center', echo=FALSE, fig.cap="An illustration of the difference between how the mean and the median should be interpreted. The mean is basically the \"centre of gravity\" of the data set: if you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation. Half of the observations are smaller, and half of the observations are larger."}
knitr::include_graphics("./resources/meanmedian.png")
```

Knowing how to calculate means and medians is only a part of the story. You also need to understand what each one is saying about the data, what that implies, and which one to choose. This is illustrated in Figure \@ref(fig:meanmedian); the mean is kind of like the "centre of gravity" of the data set, whereas the median is the "middle value" in the data. What this implies about which one you should use depends a little on what type of data you've got and what you're trying to achieve. As a rough guide:
 
- If your data are *[nominal scale](#nominalscale)*, you probably shouldn't be using either the mean or the median. Both the mean and the median rely on the idea that the numbers assigned to values are meaningful. If the numbering scheme is arbitrary, then it's probably best to use the mode (Section \@ref(mode)) instead.
- If your data are *[ordinal scale](#ordinalscale)*, you're more likely to want to use the median than the mean. The median only uses the order information in your data (i.e., which numbers are larger) but doesn't depend on the precise numbers involved. That's precisely the situation that applies when your data are ordinal scale. Conversely, the mean makes use of the precise numeric values assigned to the observations, so it's not appropriate for ordinal data.
- For *[interval](#intervalscale)* and *[ratio scale](#ratioscale)* data, either one is generally acceptable. Which one you pick depends a bit on what you're trying to achieve. The mean has the advantage of using all the information in the data (which is useful when you don't have a lot of data), but it's very susceptible to extreme values, as we'll see in Section \@ref(trimmedmean).

Let's expand on that last part a little. One consequence is that there are systematic differences between the mean and the median when the histogram is asymmetric (skewed; see Section \@ref(skewnesskurtosis)). This is illustrated in Figure \@ref(fig:meanmedian) notice that the median (right hand side) is located closer to the "body" of the histogram, whereas the mean (left hand side) gets dragged towards the "tail" (where the extreme values are). To give a concrete example, suppose Bob (income \$50,000), Kate (income \$60,000) and Jane (income \$65,000) are sitting at a table: the average income at the table is \$58,333 and the median income is \$60,000. Then Bill sits down with them (income \$100,000,000). The average income has now jumped to \$25,043,750 but the median rises only to \$62,500. If you're interested in looking at the overall income at the table, the mean might be the right answer; but if you're interested in what counts as a typical income at the table, the median would be a better choice here.

### Trimmed mean{#trimmedmean}

One of the fundamental rules of applied statistics is that the data are messy. Real life is never simple, so the data sets you obtain are never as straightforward as the statistical theory says.^[Or at least, the basic statistical theory. These days, a whole subfield of statistics called *robust statistics* tries to grapple with the messiness of real data and develop the theory that can cope with it.] This can have awkward consequences. To illustrate, consider this rather strange-looking data set:
$$
-100,2,3,4,5,6,7,8,9,10
$$
If you were to observe this in a real-life data set, you'd probably suspect that there is something odd about the $-100$ value. It's probably an **outlier**, a value that doesn't belong with the others. You might consider removing it from the data set entirely. In this particular case, it might be the right call. In real life, however, you don't always get such cut-and-dried examples. For instance, you might get this instead:
$$
-15,2,3,4,5,6,7,8,9,12
$$
The $-15$ looks suspicious, but not as much as that $-100$ did. In this case, it's a little trickier. It *might* be a legitimate observation; it might not.

When faced with a situation where some of the most extreme-valued observations might not be quite trustworthy, the mean is not necessarily a good measure of central tendency. It is highly sensitive to one or two extreme values and thus is not considered a *robust* measure. One remedy that we've seen is to use the median. A more general solution is to use a "trimmed mean". To calculate a trimmed mean, what you do is "discard" the most extreme examples on both ends (i.e., the largest and the smallest) and then take the mean of everything else. The goal is to preserve the best characteristics of the mean and the median: just like a median, you aren't highly influenced by extreme outliers, but like the mean, you "use" more than one of the observations.

Generally, we describe a trimmed mean in terms of the percentage of observations on either side that are discarded. So, for instance, a 10% trimmed mean discards the largest 10% of the observations *and* the smallest 10% of the observations and then takes the mean of the remaining 80% of the observations. Not surprisingly, the 0% trimmed mean is just the regular mean, and the 50% trimmed mean is the median. In that sense, trimmed means provide a whole family of central tendency measures that span the range from the mean to the median.

For our toy example above, we have 10 observations. So a 10% trimmed mean is calculated by ignoring the largest value (i.e. $12$) and the smallest value (i.e. $-15$) and taking the mean of the remaining values. 

> `Mean: 4.1`\
> `Median: 5.5`

That's a fairly substantial difference. But the mean is being influenced too much by the extreme values at either end of the data set, especially the $-15$ one.  If we take a 10% trimmed mean, we'll drop the extreme values on either side and take the mean of the rest: 

> `Mean: 5.5`

Which, in this case, gives exactly the same answer as the median.

Currently, there is no direct way for you to do that in CogStat, but you can certainly trim those outlying data points in your source file and re-load the data.

### Mode{#mode}

The **mode** of a sample is straightforward: it is the value that occurs most frequently.

In CogStat, you will see a **frequency table** (Figure \@ref(fig:freqaflsmall)) of the values in your data if you have `Frequencies` ticked in the `Explore variable` dialogue.

```{r freqaflsmall, out.width='70%', fig.align='center',echo=FALSE, fig.cap="The frequency table sorts non-nominal values from lowest to highest."}
knitr::include_graphics("resources/frequency_aflsmall.png")
```

While it's generally true that the mode is most often calculated when you have [nominal scale](#nominalscale) data -- because means and medians are useless for those sorts of variables --, there are some situations in which you do want to know the mode of an [ordinal](#ordinalscale), [interval](#intervalscale) or [ratio scale](#ratioscale) variable.

For instance, let's look at our `afl.margins` variable we loaded into CogStat. This variable is clearly ratio scale, and so in most situations the mean or the median is the measure of central tendency that you want. But consider this scenario... a friend of yours is offering a bet. They pick a football game at random, and (without knowing who is playing) you have to guess the *exact* margin. If you guess correctly, you win \$50. If you don't, you lose \$1. There are no consolation prizes for "almost" getting the right answer. You have to guess exactly the right margin^[This is called a "0-1 loss function", meaning that you either win (1) or you lose (0), with no middle ground.] For this bet, the mean and the median are completely useless to you. It is the mode that you should bet on. So, we look at the frequency table offered by the result set: the data suggest you should bet on a $3.0$ point margin, and since this was observed in 8 of the 176 games (4.5% of games -- the *relative frequency*), the odds are firmly in your favour.

## Measures of variability{#var}

The statistics that we've discussed so far all relate to *central tendency*. They all talk about which values are "in the middle" or "popular" in the data. However, central tendency is not the only type of summary statistic that we want to calculate. The second thing that we really want is a measure of the **variability** of the data. That is, how "spread out" are the data? How "far" away from the mean or median do the observed values tend to be? For now, let's assume that the data are interval or ratio scale, so we'll continue to use the `afl.margins` data. We'll use this data to discuss several different measures of spread, each with different strengths and weaknesses. 

### Range{#range}

The **range** of a variable is very simple: it's the largest value minus the smallest value. For the AFL winning margins data, the maximum value is $116$, and the minimum is $0$, so the range is:

$$116-0=\mathbf{116}$$

CogStat automatically calculates all these values (see Figure \@ref(fig:histogramaflsmall)), so there is nothing we need to do about this, only to understand what it implies.

Although the range is the simplest way to quantify the notion of "variability", it's one of the worst. Recall from our discussion of the mean that we want our summary measure to be robust. If the data set has one or two extremely bad values in it, we'd like our statistics not to be unduly influenced by these cases. Let us look once again at our toy example of a data set containing very extreme outliers:
$$
-100,2,3,4,5,6,7,8,9,10
$$

It is clear that the range is not robust since this has a range of $110$, but if the outlier was removed, we would have a range of only $8$.

### Interquartile range{#IQR}

The **interquartile range** (IQR) is like the range, but instead of calculating the difference between the largest and smallest value, it calculates the difference between the 25th and 75th quantile.

Probably you already know what a **quantile** is (or more commonly called **percentile**), but if not: the 10th percentile of a data set is the smallest number $x$ such that 10% of the data is less than $x$. In fact, we've already come across the idea: the median of a data set is its 50th quantile/percentile! **Quartile** is another specific term used for the 25th (lower), 50th (median), and 75th (upper) percentile, as it is splitting the data set into four (hence '*quart-*') equal parts.^[It'd be remiss if we didn't mention *quintiles*, which is splitting the data set into five equal parts, with the split points being the 20th, 40th, 60th, and 80th percentiles. And there are *tertiles* and *deciles* -- but now we're just getting sidetracked.]

CogStat provides you with both the 25th (`Lower quartile`) and 75th quantiles (`Upper quartile`):

> `Upper quartile: 50.5`\
> `Lower quartile: 12.8`

We can see that the interquartile range for the 2010 AFL winning margins data is:
$$
50.5 - 12.8 = \mathbf{37.7}
$$

While it's obvious how to interpret the range, it's a little less obvious how to interpret the IQR. The simplest way to think about it is like this: the interquartile range is the range spanned by the "middle half" of the data. That is, one quarter of the data falls below the 25th percentile, one quarter of the data is above the 75th percentile, leaving the "middle half" of the data lying in between the two. And the IQR is the range covered by that middle half.

### Mean absolute deviation (currently unsupported){#aad}

The two measures we've looked at so far, the range and the interquartile range, both rely on the idea that we can measure the spread of the data by looking at quantiles. However, this isn't the only way to think about the problem. A different approach is to select some meaningful reference point (usually the mean or the median) and then report the "typical" deviations from that. What do we mean by "typical" deviation? Usually, the mean or median value of these deviations. In practice, this leads to two different measures, the "mean absolute deviation" (from the mean) and the "median absolute deviation" (from the median). The measure based on the median seems to be used in statistics and might be the better of the two, but it's less frequently used in psychology, per Danielle's observation. The measure based on the mean occasionally shows up in psychology; hence we will cover this measure in this chapter.

One useful thing about **mean absolute deviation** is that the name tells you exactly how to calculate it. Let's think about our AFL winning margins data, and once again we'll start by pretending that there's only 5 games in total, with winning margins of 56, 31, 56, 8 and 32. Since our calculations rely on an examination of the deviation from some reference point (in this case, the mean), the first thing we need to look up is the mean, $\bar{X}$. For these five observations, our mean is $\bar{X} = 36.6$. The next step is to convert each of our observations $X_i$ into a deviation score. We do this by calculating the difference between the observation $X_i$ and the mean $\bar{X}$. That is, the deviation score is defined to be $X_i - \bar{X}$. For the first observation in our sample, this is equal to $56 - 36.6 = 19.4$. The next step in the process is to convert these deviations to absolute deviations. Mathematically, we would denote the absolute value of $-3$ as $|-3|$, and so we say that $|-3| = 3$. We use the absolute value function here because we don't care whether the value is higher than the mean or lower than the mean; we're just interested in how *close* it is to the mean. To help make this process as obvious as possible, the table below shows these calculations for all five observations:

```{r echo=FALSE}
knitr::kable(rbind(
 c("winning margin, game 1", "$X_1$", "56 points"),
 c("winning margin, game 2", "$X_2$", "31 points"),
 c("winning margin, game 3", "$X_3$", "56 points"),
 c("winning margin, game 4", "$X_4$", "8 points"),
 c("winning margin, game 5", "$X_5$", "32 points")),
col.names = c("the observation", "its symbol", "the observed value"),
 booktabs = TRUE)

```

Now that we have calculated the absolute deviation score for every observation in the data set, we only have to calculate the mean of these scores. Let's do that:
$$
\frac{19.4 + 5.6 + 19.4 + 28.6 + 4.6}{5} = 15.52
$$
And we're done. The mean absolute deviation for these five scores is 15.52. 

However, while our calculations for this little example are at an end, we do have a couple of things left to talk about. Firstly, we should really try to write down a proper mathematical formula. But in order to do this, we need some mathematical notation to refer to the mean absolute deviation. Irritatingly, "mean absolute deviation" and "median absolute deviation" have the same acronym (MAD), which leads to a certain amount of ambiguity. What we'll do is use AAD instead, short for *average* absolute deviation. Now that we have some unambiguous notation, here's the formula that describes what we just calculated:
$$
\mbox{}(X) = \frac{1}{N} \sum_{i = 1}^N |X_i - \bar{X}|
$$

The last thing we must discuss is how to calculate AAD in CogStat. Currently, the software does not automatically compute this, and no manual interventions exist. This calculation might get added in a future release if needed.

### Variance



### Standard deviation{#sd}

### Median absolute deviation (currently unsupported){#mad}

### Which measure to use?

## Skewness and kurtosis{#skewnesskurtosis}

## Summary: descriptives

As you've seen in this chapter...

```{r echo=FALSE}
library(kableExtra)
knitr::kable(rbind(
              c("[Mean](#mean)", "35.3", "Average -- the \"centre of gravity\" of the data"),
              c("[Standard deviation](#sd)", "26.0", ""),
              c("[Skewness](#skewnesskurtosis)", "0.8", ""),
              c("[Kurtosis](#skewnesskurtosis)", "0.1", ""),
              c("[Range](#range)", "116.0", "The spread of the data set between the maximum and minimum values"),
              c("[Maximum](#range)", "116.0", "The highest value in the data set"),
              c("[Upper quartile](#IQR)", "50.5", "25% of the data points reside at and above this value"),
              c("[Median](#median)", "30.5", "This is the value of the data point in the middle (or the average of the two middle points in case of even number of data points). 50-50% of data points reside at above and below this value"),
              c("[Lower quartile](#IQR)", "12.8", "25% of the data points reside at and below this value"),
              c("[Minimum](#range)", "0.0", "The lowest value in the data set")
              ),
  col.names = c("", "afl.margins", "Meaning"),
  booktabs = TRUE,
  caption = 'Descriptives for the variable',
  align = c("l", "c", "l")
  ) %>%
  column_spec(1, width_min = '1.5in')

```
