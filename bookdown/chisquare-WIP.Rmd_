## The Fisher exact test{#fisherexacttest}

What should you do if your cell counts are too small, but you'd still like to test the null hypothesis that the two variables are independent? One answer would be "collect more data", but that's far too glib: there are a lot of situations in which it would be either infeasible or unethical do that. If so, statisticians have a kind of moral obligation to provide scientists with better tests. In this instance, Fisher (1922) kindly provided the right answer to the question. To illustrate the basic idea, let's suppose that we're analysing data from a field experiment, looking at the emotional status of people who have been accused of witchcraft; some of whom are currently being burned at the stake.^[This example is based on a joke article published in the *Journal of Irreproducible Results*.] Unfortunately for the scientist (but rather fortunately for the general populace), it's actually quite hard to find people in the process of being set on fire, so the cell counts are awfully small in some cases. The `salem.Rdata` file illustrates the point:

```{r cogstatsalemload, echo=FALSE, fig.align="center", fig.cap="The `salem` data set", out.width="100%"}
knitr::include_graphics("resources/cogstatsalemload.png")
```

Looking at this data, you'd be hard pressed not to suspect that people not on fire are more likely to be happy than people on fire. However, the chi-square test makes this very hard to test because of the small sample size. If I try to do so, R gives me a warning message:
```{r}
chisq.test( salem.tabs )
```
Speaking as someone who doesn't want to be set on fire, I'd *really* like to be able to get a better answer than this. This is where **_Fisher's exact test_** \cite{Fisher1922} comes in very handy. 

The Fisher exact test works somewhat differently to the chi-square test (or in fact any of the other hypothesis tests that I talk about in this book) insofar as it doesn't have a test statistic; it calculates the $p$-value "directly". I'll explain the basics of how the test works for a $2 \times 2$ contingency table, though the test works fine for larger tables. As before, let's have some notation: 

```{r echo=FALSE}
knitr::kable(tibble::tribble(
                 ~NANA,          ~Happy,          ~Sad,         ~Total,
    "Set on fire   ", " $O_{11}$ ", " $O_{12}$ ", " $R_{1}$ ",
  "Not set on fire ", " $O_{21}$ ", " $O_{22}$ ", " $R_{2}$ ",
            "Total ",  " $C_{1}$ ",  " $C_{2}$ ",      " $N$"
  ), col.names = c("", "Happy", "Sad", "Total"))
```

In order to construct the test Fisher treats both the row and column totals ($R_1$, $R_2$, $C_1$ and $C_2$) are known, fixed quantities; and then calculates the probability that we would have obtained the observed frequencies that we did ($O_{11}$, $O_{12}$, $O_{21}$ and $O_{22}$) given those totals. In the notation that we developed in Chapter \@ref(probability) this is written:
$$
P(O_{11}, O_{12}, O_{21}, O_{22} \ | \ R_1, R_2, C_1, C_2) 
$$
and as you might imagine, it's a slightly tricky exercise to figure out what this probability is, but it turns out that this probability is described by a distribution known as the *hypergeometric distribution* ^[The R functions for this distribution are `dhyper()`, `phyper()`, `qhyper()` and `rhyper()`, though you don't need them for this book, and I haven't given you enough information to use these to perform the Fisher exact test the long way.]. Now that we know this, what we have to do to calculate our $p$-value is calculate the probability of observing this particular table *or a table that is "more extreme"*.^[Not surprisingly, the Fisher exact test is motivated by Fisher's interpretation of a $p$-value, not Neyman's!] Back in the 1920s, computing this sum was daunting even in the simplest of situations, but these days it's pretty easy as long as the tables aren't too big and the sample size isn't too large. The conceptually tricky issue is to figure out what it means to say that one contingency table is more "extreme" than another. The easiest solution is to say that the table with the lowest probability is the most extreme. This then gives us the $p$-value. 

The implementation of the test in R is via the `fisher.test()` function. Here's how it is used:
```{r}
fisher.test( salem.tabs )
```
This is a bit more output than we got from some of our earlier tests. The main thing we're interested in here is the $p$-value, which in this case is small enough ($p=.036$) to justify rejecting the null hypothesis that people on fire are just as happy as people not on fire. 


## The McNemar test{#mcnemar}

Suppose you've been hired to work for the *Australian Generic Political Party* (AGPP), and part of your job is to find out how effective the AGPP political advertisements are. So, what you do, is you put together a sample of $N=100$ people, and ask them to watch the AGPP ads. Before they see anything, you ask them if they intend to vote for the AGPP; and then after showing the ads, you ask them again, to see if anyone has changed their minds. Obviously, if you're any good at your job, you'd also do a whole lot of other things too, but let's consider just this one simple experiment. One way to describe your data is via the following contingency table:

```{r echo=FALSE}
knitr::kable(tibble::tribble(
        ~"NANA", ~"Before", ~"After", ~"Total",
  "Yes",     "30",    "10",    "40",
  "No",     "70",    "90",   "160",
  "Total",    "100 ",   "100",    "200"
  ), col.names = c("", "Before", "After", "Total"), align = 'lccc')
```

At first pass, you might think that this situation lends itself to the Pearson $\chi^2$ test of independence (as per Section \@ref(chisqindependence)). However, a little bit of thought reveals that we've got a problem: we have 100 participants, but 200 observations. This is because each person has provided us with an answer in *both* the before column and the after column. What this means is that the 200 observations aren't independent of each other: if voter A says "yes" the first time and voter B says "no", then you'd expect that voter A is more likely to say "yes" the second time than voter B! The consequence of this is that the usual $\chi^2$ test won't give trustworthy answers due to the violation of the independence assumption. Now, if this were a really uncommon situation, I wouldn't be bothering to waste your time talking about it. But it's not uncommon at all: this is a *standard* repeated measures design, and none of the tests we've considered so far can handle it. Eek. 

The solution to the problem was published by @McNemar1947. The trick is to start by tabulating your data in a slightly different way:

```{r echo=FALSE}
knitr::kable(tibble::tribble(
               ~"NANA", ~"Before: Yes", ~"Before: No", ~"Total",
  "After: Yes",           "5",          "5",    "10",
  "After: No",          "25",         "65",    "90",
  "Total",          "30",         "70",    "100"
  ), col.names = c("", "Before: Yes", "Before: No", "Total"), align = 'lccc')
```

This is exactly the same data, but it's been rewritten so that each of our 100 participants appears in only one cell. Because we've written our data this way, the independence assumption is now satisfied, and this is a contingency table that we *can* use to construct an $X^2$ goodness of fit statistic. However, as we'll see, we need to do it in a slightly nonstandard way. To see what's going on, it helps to label the entries in our table a little differently:

```{r echo=FALSE}
knitr::kable(tibble::tribble(
               ~"NANA", ~"Before: Yes", ~"Before: No", ~"Total",
  "After: Yes",         "$a$",        "$b$", "$a+b$",
     "After: No",         "$c$",        "$d$", "$c+d$",
         "Total",       "$a+c$",      "$b+d$",    "$n$"
  ), col.names = c("", "Before: Yes", "Before: No", "Total"), align = 'lccc')
```

Next, let's think about what our null hypothesis is: it's that the "before" test and the "after" test have the same proportion of people saying "Yes, I will vote for AGPP". Because of the way that we have rewritten the data, it means that we're now testing the hypothesis that the *row totals* and *column totals* come from the same distribution. Thus, the null hypothesis in McNemar's test is that we have "marginal homogeneity". That is, the row totals and column totals have the same distribution: $P_a + P_b = P_a + P_c$, and similarly that $P_c + P_d = P_b + P_d$. Notice that this means that the null hypothesis actually simplifies to $P_b = P_c$. In other words, as far as the McNemar test is concerned, it's only the off-diagonal entries in this table (i.e. $b$ and $c$) that matter! After noticing this, the **_McNemar test of marginal homogeneity_** is no different to a usual $\chi^2$ test. After applying the Yates correction, our test statistic becomes:
$$
X^2 = \frac{(|b-c| - 0.5)^2}{b+c}
$$
or, to revert to the notation that we used earlier in this chapter:
$$
X^2 = \frac{(|O_{12}-O_{21}| - 0.5)^2}{O_{12} + O_{21}}
$$
and this statistic has an (approximately) $\chi^2$ distribution with $df=1$. However, remember that -- just like the other $\chi^2$ tests -- it's only an approximation, so you need to have reasonably large expected cell counts for it to work.


### Doing the McNemar test in R

Now that you know what the McNemar test is all about, lets actually run one. The `agpp.Rdata` file contains the raw data that I discussed previously, so let's have a look at it:
```{r}
load("resources/data/agpp.Rdata")
str(agpp)
```
The `agpp` data frame contains three variables, an `id` variable that labels each participant in the data set (we'll see why that's useful in a moment), a `response_before` variable that records the person's answer when they were asked the question the first time, and a `response_after` variable that shows the answer that they gave when asked the same question a second time. As usual, here's the first 6 entries:
```{r}
head(agpp)
```
and here's a summary:
```{r}
summary(agpp)     
```
Notice that each participant appears only once in this data frame. When we tabulate this data frame using `xtabs()`, we get the appropriate table:
```{r}
right.table <- xtabs( ~ response_before + response_after, data = agpp)
print( right.table )
```
and from there, we can run the McNemar test by using the `mcnemar.test()` function:
```{r}
mcnemar.test( right.table )
```
And we're done. We've just run a McNemar's test to determine if people were just as likely to vote AGPP after the ads as they were before hand. The test was significant ($\chi^2(1) = 12.04, p<.001$), suggesting that they were not. And in fact, it looks like the ads had a negative effect: people were less likely to vote AGPP after seeing the ads. Which makes a lot of sense when you consider the quality of a typical political advertisement.


## What's the difference between McNemar and independence?

Let's go all the way back to the beginning of the chapter, and look at the `cards` data set again. If you recall, the actual experimental design that I described involved people making *two* choices. Because we have information about the first choice and the second choice that everyone made, we can construct the following contingency table that cross-tabulates the first choice against the second choice.
```{r}
#cardChoices <- xtabs( ~ choice_1 + choice_2, data = cards )
#cardChoices
```
Suppose I wanted to know whether the choice you make the second time is dependent on the choice you made the first time. This is where a test of independence is useful, and what we're trying to do is see if there's some relationship between the rows and columns of this table. Here's the result:
```{r}
#chisq.test( cardChoices )
```

Alternatively, suppose I wanted to know if *on average*, the frequencies of suit choices were different the second time than the first time. In that situation, what I'm really trying to see if the row totals in `cardChoices` (i.e. the frequencies for `choice_1`) are different from the column totals (i.e. the frequencies for `choice_2`). That's when you use the McNemar test:
```{r}
#mcnemar.test( cardChoices )
```
Notice that the results are different! These aren't the same test. 



## Summary

The key ideas discussed in this chapter are:


- The chi-square goodness of fit test (Section \@ref(goftest)) is used when you have a table of observed frequencies of different categories; and the null hypothesis gives you a set of "known" probabilities to compare them to. You can either use the `goodnessOfFitTest()` function in the `lsr` package to run this test, or the `chisq.test()` function. 
- The chi-square test of independence (Section \@ref(chisqindependence)) is used when you have a contingency table (cross-tabulation) of two categorical variables. The null hypothesis is that there is no relationship/association between the variables. You can either use the `associationTest()` function in the `lsr` package, or you can use   `chisq.test()`. 
- Effect size for a contingency table can be measured in several ways (Section \@ref(chisqeffectsize)). In particular we noted the Cramer's $V$ statistic, which can be calculated using `cramersV()`. This is also part of the output produced by  `associationTest()`.
- Both versions of the Pearson test rely on two assumptions: that the expected frequencies are sufficiently large, and that the observations are independent (Section \@ref(chisqassumptions)). The Fisher exact test (Section \@ref(fisherexacttest)) can be used when the expected frequencies are small, `fisher.test(x = contingency.table)`. The McNemar test (Section \@ref(mcnemar)) can be used for some kinds of violations of independence, `mcnemar.test(x = contingency.table)`. 



If you're interested in learning more about categorical data analysis, a good first choice would be @Agresti1996 which, as the title suggests, provides an *Introduction to Categorical Data Analysis*. If the introductory book isn't enough for you (or can't solve the problem you're working on) you could consider @Agresti2002, *Categorical Data Analysis*. The latter is a more advanced text, so it's probably not wise to jump straight from this book to that one. 

